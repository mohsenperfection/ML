{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenperfection/ML/blob/main/Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f42a3ce",
      "metadata": {
        "id": "5f42a3ce"
      },
      "source": [
        "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
        "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
        "\n",
        "\n",
        "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
        "\n",
        "\n",
        "**Student Name**: **Mohsen Kamalabadi Farahani**\n",
        "\n",
        "**Student ID**: **Mohsen Kamalabadi Farahani**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01c559e",
      "metadata": {
        "id": "a01c559e"
      },
      "source": [
        "## Importing Libraries\n",
        "\n",
        "First we import libraries that we need for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0423187e",
      "metadata": {
        "id": "0423187e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import any other libraries needed below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f795731",
      "metadata": {
        "id": "2f795731"
      },
      "source": [
        "## Reading Data and Preprocessing\n",
        "\n",
        "In this section, we want to read data from a CSV file and then preprocess it to make it ready for the rest of the problem.\n",
        "\n",
        "First, we read the data in the cell below and extract an $m \\times n$ matrix, $X$, and an $m \\times 1$ vector, $Y$, from it, which represent our knowledge about the features of the data (`X1`, `X2`, `X3`) and the class (`Y`), respectively. Note that by $m$, we mean the number of data points and by $n$, we mean the number of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410e750d",
      "metadata": {
        "id": "410e750d",
        "outputId": "fe475492-3d9a-46bf-8bba-576cc88a831e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 3)\n",
            "(10000, 1)\n"
          ]
        }
      ],
      "source": [
        "X, Y = None, None\n",
        "\n",
        "### START CODE HERE ###\n",
        "\n",
        "# Load the datase\n",
        "data = pd.read_csv('data_logistic.csv')\n",
        "\n",
        "# Feature columns\n",
        "X = data[['X1', 'X2', 'X3']].values\n",
        "Y = data['Y'].values.reshape(-1, 1)  # Reshaping to make Y a column vector\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "866734e2",
      "metadata": {
        "id": "866734e2"
      },
      "source": [
        "Next, we should normalize our data. For normalizing a vector $\\mathbf{x}$, a very common method is to use this formula:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_{norm} = \\dfrac{\\mathbf{x} - \\overline{\\mathbf{x}}}{\\sigma_\\mathbf{x}}\n",
        "$$\n",
        "\n",
        "Here, $\\overline{x}$ and $\\sigma_\\mathbf{x}$ denote the mean and standard deviation of vector $\\mathbf{x}$, respectively. Use this formula and store the new $X$ and $Y$ vectors in the cell below.\n",
        "\n",
        "**Question**: Briefly explain why we need to normalize our data before starting the training.\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "Normalization is a crucial step in data preprocessing for several reasons:\n",
        "\n",
        "1. **Scale Invariance**: Many machine learning algorithms are sensitive to the scale of the features. Normalizing the data ensures that each feature contributes proportionally to the result, regardless of its original scale. This helps prevent certain features from dominating the model simply because they have larger magnitudes.\n",
        "\n",
        "2. **Convergence Speed**: Normalizing the data often helps optimization algorithms converge more quickly. By scaling the features to a similar range, gradient descent methods can navigate the loss landscape more efficiently.\n",
        "\n",
        "3. **Regularization**: Some regularization techniques, like L1 and L2 regularization, implicitly assume that all features are on the same scale. Normalization helps maintain this assumption, improving the effectiveness of regularization.\n",
        "\n",
        "4. **Interpretability**: Normalized data can be easier to interpret. When features are on different scales, it becomes challenging to compare their coefficients' magnitudes and infer their relative importance in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e757eb0",
      "metadata": {
        "id": "9e757eb0",
        "outputId": "2733579c-603c-4bbc-d5e5-c74c6d638474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.00066149 -0.60536985 -0.85021999]\n",
            " [-1.45366949  1.73051062 -0.20238503]\n",
            " [ 0.26239007  1.69140966  0.64234794]\n",
            " [-0.68111085 -1.28703215 -0.70744413]\n",
            " [ 0.4033742   0.47210428 -0.36768115]]\n",
            "[[ 0.70832689]\n",
            " [ 0.70832689]\n",
            " [ 0.70832689]\n",
            " [-1.41177754]\n",
            " [ 0.70832689]]\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "\n",
        "# Normalize X & Y\n",
        "X_mean = X.mean(axis=0)\n",
        "X_std = X.std(axis=0)\n",
        "X = (X - X_mean) / X_std\n",
        "Y_mean = Y.mean()\n",
        "Y_std = Y.std()\n",
        "Y = (Y - Y_mean) / Y_std\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(X[:5])  # Print the first 5 rows to see some of the normalized data\n",
        "print(Y[:5])  # Print the first 5 rows of Y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5465bfa4",
      "metadata": {
        "id": "5465bfa4"
      },
      "source": [
        "Finally, we should add a column of $1$s at the beginning of $X$ to represent the bias term. Do this in the next cell. Note that after this process, $X$ should be an $m \\times (n+1)$ matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a60f1f",
      "metadata": {
        "id": "b9a60f1f",
        "outputId": "567308eb-81cc-4ec7-e739-3451120f0588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 5)\n",
            "[[ 1.          1.         -1.00066149 -0.60536985 -0.85021999]\n",
            " [ 1.          1.         -1.45366949  1.73051062 -0.20238503]\n",
            " [ 1.          1.          0.26239007  1.69140966  0.64234794]\n",
            " [ 1.          1.         -0.68111085 -1.28703215 -0.70744413]\n",
            " [ 1.          1.          0.4033742   0.47210428 -0.36768115]]\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "\n",
        "# Adding a column\n",
        "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(X.shape)  # This should print (m, n+1)\n",
        "print(X[:5])    # Print the first 5 rows to see some of the data with the bias term\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cdf0d78",
      "metadata": {
        "id": "5cdf0d78"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8714abe",
      "metadata": {
        "id": "c8714abe"
      },
      "source": [
        "### Sigmoid Function\n",
        "You should begin by implementing the $\\sigma(\\mathbf{x})$ function. Recall that the logistic regression hypothesis $\\mathcal{h}()$ is defined as:\n",
        "$$\n",
        "\\mathcal{h}_{\\theta}(\\mathbf{x}) = \\mathcal{g}(\\theta^\\mathbf{T}\\mathbf{x})\n",
        "$$\n",
        "where $\\mathcal{g}()$ is the sigmoid function as:\n",
        "$$\n",
        "\\mathcal{g}(\\mathbf{z}) = \\frac{1}{1+exp^{-\\mathbf{z}}}\n",
        "$$\n",
        "The Sigmoid function has the property that $\\mathbf{g}(+\\infty)\\approx 1$ and $\\mathcal{g}(âˆ’\\infty)\\approx0$. Test your function by calling `sigmoid(z)` on different test samples. Be certain that your sigmoid function works with both vectors and matrices - for either a vector or a matrix, your function should perform the sigmoid function on every element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6b6ae4",
      "metadata": {
        "id": "6a6b6ae4",
        "outputId": "92e627fa-d0ee-4326-fa7e-87057f44081e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigmoid of test values: [4.53978687e-05 2.68941421e-01 5.00000000e-01 7.31058579e-01\n",
            " 9.99954602e-01]\n",
            "Sigmoid of test matrix:\n",
            " [[2.68941421e-01 5.00000000e-01 7.31058579e-01]\n",
            " [9.99954602e-01 4.53978687e-05 5.00000000e-01]]\n"
          ]
        }
      ],
      "source": [
        "def sigmoid(Z):\n",
        "    '''\n",
        "    Applies the sigmoid function to each element of Z.\n",
        "    Parameters:\n",
        "        Z (numpy array): Can be a scalar, vector, or matrix.\n",
        "    Returns:\n",
        "        numpy array: Sigmoid applied elementwise, same shape as Z.\n",
        "    '''\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "# Test the sigmoid function\n",
        "test_values = np.array([-10, -1, 0, 1, 10])\n",
        "print(\"Sigmoid of test values:\", sigmoid(test_values))\n",
        "\n",
        "# Test with a matrix\n",
        "test_matrix = np.array([[-1, 0, 1], [10, -10, 0]])\n",
        "print(\"Sigmoid of test matrix:\\n\", sigmoid(test_matrix))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83846074",
      "metadata": {
        "id": "83846074"
      },
      "source": [
        "### Cost Function\n",
        "Implement the functions to compute the cost function. Recall the cost function for logistic regression is a scalar value given by:\n",
        "$$\n",
        "\\mathcal{J}(\\theta) = \\sum_{i=1}^{n}[-y^{(i)}\\log{(\\mathcal{h}_\\theta(\\mathbf{x}^{(i)}))}-(1-y^{(i)})\\log{(1-\\mathcal{h}_\\theta(\\mathbf{x}^{(i)}))}] + \\frac{\\lambda}{2}||\\theta||_2^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a9bdeb",
      "metadata": {
        "id": "26a9bdeb",
        "outputId": "183541a2-856e-4be8-96be-2eb1e03e58b1"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/65/bf82v36x7pb1bp3trrt6lssr0000gn/T/ipykernel_2393/191655184.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mregLambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputeCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should print the calculated cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/65/bf82v36x7pb1bp3trrt6lssr0000gn/T/ipykernel_2393/191655184.py\u001b[0m in \u001b[0;36mcomputeCost\u001b[0;34m(theta, X, y, regLambda)\u001b[0m\n\u001b[1;32m     13\u001b[0m     '''\n\u001b[1;32m     14\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m  \u001b[0;31m# number of training examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# hypothesis function, matrix-vector product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Cross-entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def computeCost(theta, X, y, regLambda):\n",
        "    '''\n",
        "    Computes the logistic regression cost with regularization.\n",
        "    Arguments:\n",
        "        theta (numpy array): A d-dimensional numpy vector.\n",
        "        X (numpy array): An n-by-d numpy matrix of features.\n",
        "        y (numpy array): An n-dimensional numpy vector of target values.\n",
        "        regLambda (float): The scalar regularization constant.\n",
        "    Returns:\n",
        "        float: The computed scalar value of the cost.\n",
        "    '''\n",
        "    m = y.size  # number of training examples\n",
        "    h = sigmoid(X @ theta)  # hypothesis function, matrix-vector product\n",
        "\n",
        "    # Cross-entropy loss\n",
        "    term1 = np.dot(y.T, np.log(h))\n",
        "    term2 = np.dot((1 - y).T, np.log(1 - h))\n",
        "    loss = -(term1 + term2) / m\n",
        "\n",
        "    # Regularization (excluding the bias term theta[0])\n",
        "    reg = regLambda / (2 * m) * np.dot(theta[1:], theta[1:])\n",
        "\n",
        "    # Total cost\n",
        "    total_cost = loss + reg\n",
        "    return total_cost.item()  # Ensure it's a scalar, not a numpy array with one element\n",
        "\n",
        "# Example usage:\n",
        "# Assume some theta, X, y, and regLambda values\n",
        "theta = np.array([0.1, 0.2, 0.3])\n",
        "X = np.array([[1, 2], [1, 3], [1, 4]])\n",
        "y = np.array([0, 1, 0])\n",
        "regLambda = 0.01\n",
        "\n",
        "print(computeCost(theta, X, y, regLambda))  # Should print the calculated cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eaf1146",
      "metadata": {
        "id": "6eaf1146"
      },
      "source": [
        "### Gradient of the Cost Function\n",
        "Now, we want to calculate the gradient of the cost function. The gradient of the cost function is a d-dimensional vector.\\\n",
        "We must be careful not to regularize the $\\theta_0$ parameter (corresponding to the first feature we add to each instance), and so the 0's element is given by:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial \\theta_0} = \\sum_{i=1}^n (\\mathcal{h}_\\theta(\\mathbf{x}^{(i)})-y^{(i)})\n",
        "$$\n",
        "\n",
        "Question: What is the answer to this problem for the $j^{th}$ element (for $j=1...d$)?\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f7c2dd",
      "metadata": {
        "id": "35f7c2dd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def computeGradient(theta, X, y, regLambda):\n",
        "    '''\n",
        "    Computes the gradient of the objective function\n",
        "    Arguments:\n",
        "        theta is d-dimensional numpy vector\n",
        "        X is a n-by-d numpy matrix\n",
        "        y is an n-dimensional numpy vector\n",
        "        regLambda is the scalar regularization constant\n",
        "    Returns:\n",
        "        the gradient, a d-dimensional vector\n",
        "    '''\n",
        "    m, n = X.shape  # m is the number of training examples, n is the number of features\n",
        "    h = sigmoid(X.dot(theta))  # Compute the hypothesis for all examples\n",
        "    error = h - y  # Error term\n",
        "\n",
        "    # Gradient calculation\n",
        "    grad = (1 / m) * (X.T.dot(error))  # Base gradient without regularization\n",
        "\n",
        "    # Regularization term for all but the bias term (first element of theta)\n",
        "    reg_term = (regLambda / m) * np.copy(theta)\n",
        "    reg_term[0] = 0  # No regularization for the bias term\n",
        "\n",
        "    # Combine the base gradient with the regularization term\n",
        "    grad += reg_term\n",
        "\n",
        "    return grad\n",
        "\n",
        "def sigmoid(z):\n",
        "    '''\n",
        "    Sigmoid function for logistic regression\n",
        "    '''\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24bc86bd",
      "metadata": {
        "id": "24bc86bd"
      },
      "source": [
        "### Training and Prediction\n",
        "Once you have the cost and gradient functions complete, implemen tthe fit and predict methods.\\\n",
        "Your fit method should train the model via gradient descent, relying on the cost and gradient functions. This function should return two parameters. The first parameter is $\\theta$, and the second parameter is a `numpy` array that contains the loss in each iteration. This array is indicated by `loss_history` in the code.\\\n",
        "Instead of simply running gradient descent for a specific number of iterations, we will use a more sophisticated method: we will stop it after the solution hasconverged. Stop the gradient descent procedure when $\\theta$ stops changing between consecutive iterations. You can detect this convergence when:\n",
        "$$\n",
        "||\\theta_{new}-\\theta_{old}||_2 <= \\epsilon,\n",
        "$$\n",
        "for some small $\\epsilon$ (e.g, $\\epsilon=10E-4$).\\\n",
        "For readability, weâ€™d recommend implementing this convergence test as a dedicated function `hasConverged`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc0cad78",
      "metadata": {
        "id": "cc0cad78"
      },
      "outputs": [],
      "source": [
        "def fit(X, y, regLambda=0.01, alpha=0.01, epsilon=1e-4, maxNumIters=100):\n",
        "    '''\n",
        "    Trains the model using gradient descent\n",
        "    Arguments:\n",
        "        X           : n-by-d numpy matrix\n",
        "        y           : n-dimensional numpy vector\n",
        "        regLambda   : scalar regularization constant\n",
        "        alpha       : gradient descent learning rate\n",
        "        epsilon     : convergence threshold\n",
        "        maxNumIters : maximum number of gradient descent iterations\n",
        "    Returns:\n",
        "        theta       : learned parameters\n",
        "        loss_history: list of loss values per iteration\n",
        "    '''\n",
        "    m, n = X.shape  # Number of examples and number of features\n",
        "    theta = np.zeros(n)  # Initialize parameters\n",
        "    loss_history = []  # To store the cost at each iteration\n",
        "\n",
        "    for _ in range(maxNumIters):\n",
        "        # Compute gradient and cost\n",
        "        grad = computeGradient(theta, X, y, regLambda)\n",
        "        cost = computeCost(theta, X, y, regLambda)\n",
        "        loss_history.append(cost)\n",
        "\n",
        "        # Update parameters\n",
        "        theta_new = theta - alpha * grad\n",
        "\n",
        "        # Check for convergence\n",
        "        if hasConverged(theta, theta_new, epsilon):\n",
        "            theta = theta_new\n",
        "            break\n",
        "\n",
        "        theta = theta_new\n",
        "\n",
        "    return theta, loss_history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def hasConverged(theta_old, theta_new, epsilon):\n",
        "    '''\n",
        "    Checks if the optimization has converged\n",
        "    Arguments:\n",
        "        theta_old : previous parameter vector\n",
        "        theta_new : current parameter vector\n",
        "        epsilon   : convergence threshold\n",
        "    Returns:\n",
        "        Boolean indicating whether convergence criterion has been met\n",
        "    '''\n",
        "    # Calculate the Euclidean norm of the difference between old and new theta\n",
        "    diff = np.linalg.norm(theta_new - theta_old)\n",
        "    return diff <= epsilon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb648852",
      "metadata": {
        "id": "bb648852"
      },
      "source": [
        "Finally, we want to evaluate our loss for this problem. Complete the cell below to calculate and print the loss of each iteration and the final theta of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "252e556f",
      "metadata": {
        "id": "252e556f",
        "outputId": "0ec55c9a-a51c-40a1-f73f-d69ae77ef8c4"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (5,10000) (5,) (5,10000) ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/65/bf82v36x7pb1bp3trrt6lssr0000gn/T/ipykernel_2393/3785640880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assuming X and Y are already defined and preprocessed appropriately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# calculating theta and loss of each iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m### START CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/65/bf82v36x7pb1bp3trrt6lssr0000gn/T/ipykernel_2393/604254503.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(X, y, regLambda, alpha, epsilon, maxNumIters)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxNumIters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Compute gradient and cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/65/bf82v36x7pb1bp3trrt6lssr0000gn/T/ipykernel_2393/2218791510.py\u001b[0m in \u001b[0;36mcomputeGradient\u001b[0;34m(theta, X, y, regLambda)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Combine the base gradient with the regularization term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreg_term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,10000) (5,) (5,10000) "
          ]
        }
      ],
      "source": [
        "# Assuming X and Y are already defined and preprocessed appropriately\n",
        "theta, loss_history = fit(X, Y)  # calculating theta and loss of each iteration\n",
        "\n",
        "### START CODE HERE ###\n",
        "\n",
        "# Print the final theta\n",
        "print(\"Final theta parameters:\", theta)\n",
        "\n",
        "# Plot the loss over iterations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(loss_history, label='Loss per iteration')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss History during Gradient Descent')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "### END CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b3fab6",
      "metadata": {
        "id": "f4b3fab6"
      },
      "source": [
        "### Testing Your Implementation\n",
        "To test your logistic regression implementation, first you should use `train_test_split` function to split dataset into three parts:\n",
        "\n",
        "- 70% for the training set\n",
        "- 20% for the validation set\n",
        "- 10% for the test set\n",
        "\n",
        "Do this in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4518fe11",
      "metadata": {
        "id": "4518fe11"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train, X_val, Y_val, X_test, Y_test = None, None, None, None, None, None\n",
        "\n",
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fbe5d7",
      "metadata": {
        "id": "f9fbe5d7"
      },
      "source": [
        "Then, you should complete `predict` function to find the weight vector and the loss on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95c2fe20",
      "metadata": {
        "id": "95c2fe20"
      },
      "outputs": [],
      "source": [
        "def predict(X, theta):\n",
        "    '''\n",
        "    Use the model to predict values for each instance in X\n",
        "    Arguments:\n",
        "        theta is d-dimensional numpy vector\n",
        "        X     is a n-by-d numpy matrix\n",
        "    Returns:\n",
        "        an n-dimensional numpy vector of the predictions, the output should be binary (use h_theta > .5)\n",
        "    '''\n",
        "\n",
        "    Y = None\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "246d1c02",
      "metadata": {
        "id": "246d1c02"
      },
      "source": [
        "Now, run the `fit` and `predict` function for different values of the learning rate and regularization constant. Plot the `loss_history` of these different values for train and test data both in the same figure.\n",
        "\n",
        "**Question**: Discuss the effect of the learning rate and regularization constant and find the best values of these parameters.\n",
        "\n",
        "**Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2af382",
      "metadata": {
        "id": "cd2af382"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11babf15",
      "metadata": {
        "id": "11babf15"
      },
      "source": [
        "## Naive Bayes\n",
        "\n",
        "In this part, you will use the `GaussianNB` classifier to classify the data. You should not change the default parameters of this classifier. First, train the classifier on the training set and then find the accuracy of it on the test set.\n",
        "\n",
        "**Question**: What is the accuracy of this method on test set?\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "**Explanation of the Code:**\n",
        "\n",
        "- **Data Loading and Preparation:** The dataset is loaded using pandas, and feature columns (`X1`, `X2`, `X3`) are extracted along with the target column (`Y`). Ensure that the path and column names match your dataset.\n",
        "- **Training/Test Split:** The dataset is randomly split into a training set (70%) and a test set (30%).\n",
        "- **GaussianNB Classifier:** A `GaussianNB` classifier is instantiated with default parameters, trained on the training set, and used to make predictions on the test set.\n",
        "- **Accuracy Calculation:** The `accuracy_score` function computes the accuracy of the classifier by comparing the predicted labels to the actual labels in the test set.\n",
        "\n",
        "Replace `\"path_to_your_data.csv\"` with the actual path to your dataset. Adjust the feature and label column names according to your dataset's schema. This script will print the accuracy of the GaussianNB classifier on your test data, providing a direct answer to how well the classifier performs on your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef450fb",
      "metadata": {
        "id": "1ef450fb",
        "outputId": "6435a2ca-f2f3-4d60-efdd-5e593634ae63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of GaussianNB on the test set: 93.97%\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your data\n",
        "data = pd.read_csv('Q3/data_logistic.csv')\n",
        "X = data[['X1', 'X2', 'X3']].values  # Adjust column names as necessary\n",
        "y = data['Y'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create GaussianNB instance\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of GaussianNB on the test set: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b371657d",
      "metadata": {
        "id": "b371657d"
      },
      "source": [
        "## LDA (Linear Discriminant Analysis)\n",
        "\n",
        "In this part, you will use the `LinearDiscriminantAnalysis` classifier to classify the data. You should not change the default parameters of this classifier. First, train the classifier on the training set and then find the accuracy of it on the test set.\n",
        "\n",
        "**Question**: What is the accuracy of this method on test set?\n",
        "\n",
        "**Answer**:\n",
        "**Explanation of the Code:**\n",
        "\n",
        "1. **Data Loading:** Load your dataset using pandas. You'll need to adjust the path and the column names according to your dataset structure.\n",
        "\n",
        "2. **Splitting the Data:** The data is split into training and testing sets using `train_test_split`. Here, 30% of the data is used for testing, which is a common split ratio.\n",
        "\n",
        "3. **Creating and Training LDA:** An instance of `LinearDiscriminantAnalysis` is created with default parameters. The model is then trained using the training data.\n",
        "\n",
        "4. **Making Predictions and Calculating Accuracy:** After training, predictions are made on the test set. The accuracy of these predictions is then calculated using `accuracy_score`, which compares the predicted labels with the actual labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92cc8743",
      "metadata": {
        "id": "92cc8743",
        "outputId": "beeda58b-3345-488b-8efd-e8022b3a25ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of LDA on the test set: 99.00%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load your data\n",
        "# Here you should load your data similarly as you did earlier or using an appropriate method for your data format\n",
        "data = pd.read_csv('Q3/data_logistic.csv')\n",
        "X = data[['X1', 'X2', 'X3']].values  # Adjust column names as necessary\n",
        "Y = data['Y'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create LDA instance\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "\n",
        "# Train the model\n",
        "lda.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = lda.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of LDA on the test set: {:.2f}%\".format(accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47736bdf",
      "metadata": {
        "id": "47736bdf"
      },
      "source": [
        "## Conclution\n",
        "\n",
        "**Question**: What is the best method for classifying this dataset? What is the best accuracy on the test set?\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "To determine the best method for classifying your dataset, you would compare the accuracies obtained from the different classifiers you have used. From our discussions and code examples, you likely tested at least two classifiers:\n",
        "\n",
        "1. **Linear Discriminant Analysis (LDA)**\n",
        "2. **Gaussian Naive Bayes (GaussianNB)**\n",
        "\n",
        "The accuracy of each classifier on the test set can be obtained by running the respective code snippets provided and analyzing the results.\n",
        "\n",
        "Hereâ€™s how you would summarize and conclude which classifier performs better:\n",
        "\n",
        "### Step-by-Step Comparison:\n",
        "\n",
        "1. **Evaluate Each Classifier**: Implement each classifier as discussed, ensuring the dataset is consistently split between training and testing for fair comparison. Calculate the accuracy for each classifier on the test set.\n",
        "\n",
        "2. **Compare Accuracies**: The accuracy percentages will directly tell you which classifier has higher performance on your dataset. The classifier with the highest accuracy on the test set is generally considered the best for your specific dataset under the testing conditions used.\n",
        "\n",
        "3. **Consider Overfitting**: Ensure that the better-performing classifier is not just memorizing the training data but generalizing well to new, unseen data (the test set). This involves looking not only at the accuracy but also potentially at other metrics like precision, recall, and the confusion matrix.\n",
        "\n",
        "### Example Conclusion:\n",
        "\n",
        "Suppose after running the code for both classifiers, you found:\n",
        "- **LDA Accuracy**: 92%\n",
        "- **GaussianNB Accuracy**: 89%\n",
        "\n",
        "In this hypothetical case, the **LDA classifier** is the better model for this dataset as it achieves a higher accuracy on the test set. Thus, you might conclude:\n",
        "\n",
        "\"The best method for classifying this dataset is the Linear Discriminant Analysis (LDA), achieving an accuracy of 92% on the test set. This suggests that LDA is more effective at capturing the underlying patterns in this particular dataset compared to Gaussian Naive Bayes, which achieved an accuracy of 89%.\"\n",
        "\n",
        "### Final Note:\n",
        "\n",
        "When determining the \"best\" classifier, it's crucial to ensure the model is suitable for the operational context and computational resources available. Moreover, further tuning and validation, such as cross-validation or adjusting model parameters, could potentially improve model performance even further."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}