{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenperfection/ML/blob/CHW03/Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b46fe41",
      "metadata": {
        "id": "1b46fe41"
      },
      "source": [
        "<h1 align=\"center\">Introduction to Machine Learning - Course Code: 25737</h1>\n",
        "<h4 align=\"center\">Instructor: Dr. Amiri</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
        "<h4 align=\"center\">Computer Assignment 3</h4>\n",
        "<h4 align=\"center\">\n",
        "\n",
        "Question 1\n",
        "\n",
        "</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24a0fc13",
      "metadata": {
        "id": "24a0fc13"
      },
      "source": [
        "# Personal Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "44babb65",
      "metadata": {
        "id": "44babb65"
      },
      "outputs": [],
      "source": [
        "# Set your student number\n",
        "student_number = 99102083\n",
        "Name = 'Mohsen'\n",
        "Last_Name = 'Kamalabadi Farahani'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca4a337a",
      "metadata": {
        "id": "ca4a337a"
      },
      "source": [
        "# Rules\n",
        "- You are not allowed to add or remove cells. You **must use the provided space to write your code**. If you don't follow this rule, **your Practical Assignment won't be graded**.  \n",
        "\n",
        "- Collaboration and using the internet is allowed, but your code **must be written by yourself**. **Copying code** from each other or from available resources will result in a **zero score for the assignment**.\n",
        "\n",
        "- You are not allowed to use `torch.nn`, `torch.optim` and any activation function and loss function implemented in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "12b76789",
      "metadata": {
        "id": "12b76789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648d0507-7f1b-40cc-e4d5-d05e22e2a133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.3.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->torchvision)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install torchvision\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886188c7",
      "metadata": {
        "id": "886188c7"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "55a0adcc",
      "metadata": {
        "id": "55a0adcc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from typing import Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18510868",
      "metadata": {
        "id": "18510868"
      },
      "source": [
        "## Datasets and Dataloaders\n",
        "\n",
        "Here, we download and load the train and test `FashionMNIST` dataset with the desired transforms. Then, we define the dataloaders for `train` and `test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dc8759e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc8759e2",
        "outputId": "bc28b99a-d6c3-47eb-cf16-00c7ca235aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 19229573.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/train-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 334372.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 6242037.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 15291980.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_set = FashionMNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_set = FashionMNIST(root='.', train=False, download=True, transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df47fcb",
      "metadata": {
        "id": "5df47fcb"
      },
      "source": [
        "\n",
        "Here you have to calculate the number of classes amd input dimention of the first layer (how many pixels does each image have?)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8f6763e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f6763e6",
        "outputId": "fc7d6a5c-dd0e-40e3-da0a-ae528115a750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 10\n",
            "Input dimension: 784\n"
          ]
        }
      ],
      "source": [
        "## FILL HERE\n",
        "# input_dim = .....\n",
        "num_classes = len(train_set.classes)\n",
        "input_dim = train_set[0][0].numel()\n",
        "\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Input dimension: {input_dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c695ff60",
      "metadata": {
        "id": "c695ff60"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, 64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, 64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9dac6c2",
      "metadata": {
        "id": "f9dac6c2"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Visualize 1 random image from each class by using `plt.subplots`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e3d6b0c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "e3d6b0c1",
        "outputId": "aea2436a-c556-454b-ebcb-ec3bb6ad8292"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "class_images = {}\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    for i in range(images.size(0)):\n",
        "        label = labels[i].item()\n",
        "        if label not in class_images:\n",
        "            class_images[label] = images[i]\n",
        "        if len(class_images) == num_classes:\n",
        "            break\n",
        "    if len(class_images) == num_classes:\n",
        "        break\n",
        "\n",
        "fig, axes = plt.subplots(1, num_classes, figsize=(15, 15))\n",
        "\n",
        "class_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "for i, (label, image) in enumerate(class_images.items()):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(image.squeeze(), cmap='gray')\n",
        "    ax.set_title(class_labels[label])\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a94c5aba",
      "metadata": {
        "id": "a94c5aba"
      },
      "source": [
        "## Initializing model's parameters\n",
        "\n",
        "In this part, we create the model and initialize its parameters and store the values of these parameters in the variable `parameters` which is a dictionary including the weigths and biases of each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e6d40952",
      "metadata": {
        "id": "e6d40952"
      },
      "outputs": [],
      "source": [
        "def add_linear_layer(parameters: dict, shape, device, i=None):\n",
        "    \"\"\"\n",
        "    This function adds parameters of a linear unit of shape `shape` to the `parameters` dictionary.\n",
        "    \"\"\"\n",
        "    n_in, n_out = shape\n",
        "    with torch.no_grad():\n",
        "        w = torch.zeros(*shape, device=device)\n",
        "        # kaiming initialization for ReLU activations:\n",
        "        bound = 1 / np.sqrt(n_in).item()\n",
        "        w.uniform_(-bound, bound)\n",
        "        b = torch.zeros(n_out, device=device)  # no need to (1, n_out). it will broadcast itself.\n",
        "    w.requires_grad = True\n",
        "    b.requires_grad = True\n",
        "    # `i` is used to give numbers to parameter names\n",
        "    parameters.update({f'w{i}': w, f'b{i}': b})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce914706",
      "metadata": {
        "id": "ce914706"
      },
      "source": [
        "Now we define our neural network with the given layers and add the weights and biases to the dictionary `parameters`. **You are allowed to modify the values of the layers**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8f3867d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f3867d7",
        "outputId": "9eb310ed-6f83-4095-cf29-801722f7cab8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['w0', 'b0', 'w1', 'b1', 'w2', 'b2', 'w3', 'b3', 'w4', 'b4'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# input_dim : input dimention of the first layer, which you have calculated before.\n",
        "layers = [\n",
        "    (input_dim, 512),\n",
        "    (512, 256),\n",
        "    (256, 128),\n",
        "    (128, 64),\n",
        "    (64, num_classes)\n",
        "]\n",
        "num_layers = len(layers)\n",
        "parameters = {}\n",
        "\n",
        "# setting the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# adding the parameters to the dictionary\n",
        "for i, shape in enumerate(layers):\n",
        "    add_linear_layer(parameters, shape, device, i)\n",
        "\n",
        "parameters.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfd2c8e",
      "metadata": {
        "id": "8bfd2c8e"
      },
      "source": [
        "## Defining the required functions\n",
        "\n",
        "In this section, we should define the required functions. For each of these functions, the inputs and the desired outputs are given and you should write all or part of the function. **You are not allowed to use the activation functions and the loss functions implemented in torch**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b413d8",
      "metadata": {
        "id": "f3b413d8"
      },
      "source": [
        "Computing affine and relu outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bebeeb0e",
      "metadata": {
        "id": "bebeeb0e"
      },
      "outputs": [],
      "source": [
        "def affine_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the affine transformation (Wx + b).\n",
        "    x: input tensor of shape (N, d_in)\n",
        "    w: weights tensor of shape (d_in, d_out)\n",
        "    b: bias tensor of shape (d_out)\n",
        "    \"\"\"\n",
        "    return x @ w + b\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    Applies the ReLU activation function element-wise.\n",
        "    x: input tensor\n",
        "    \"\"\"\n",
        "    return torch.maximum(x, torch.zeros_like(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9baa5e",
      "metadata": {
        "id": "5d9baa5e"
      },
      "source": [
        "Function `model` returns output of the whole model for the input `x` using the parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d2562962",
      "metadata": {
        "id": "d2562962"
      },
      "outputs": [],
      "source": [
        "def model(x: torch.Tensor, parameters, num_layers=num_layers):\n",
        "    # number of batches\n",
        "    B = x.shape[0]\n",
        "    x = x.view(B, -1)  # Flatten the input\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        w = parameters[f'w{i}']\n",
        "        b = parameters[f'b{i}']\n",
        "        x = affine_forward(x, w, b)\n",
        "\n",
        "        if i < num_layers - 1:\n",
        "            x = relu(x)\n",
        "\n",
        "    output = x\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d17a9b4c",
      "metadata": {
        "id": "d17a9b4c"
      },
      "source": [
        "Implementing cross entropy loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6959621c",
      "metadata": {
        "id": "6959621c"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(scores, y):\n",
        "    \"\"\"\n",
        "    Computes the cross entropy loss.\n",
        "\n",
        "    scores: Tensor of shape (N, C) where N is the batch size and C is the number of classes.\n",
        "    y: Tensor of shape (N,) where each value is the true class label for each sample.\n",
        "\n",
        "    Returns:\n",
        "    loss: Scalar tensor representing the average cross entropy loss over the batch.\n",
        "    \"\"\"\n",
        "    n = len(y)\n",
        "    exp_scores = torch.exp(scores)\n",
        "    probabilities = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True)\n",
        "    log_probabilities = torch.log(probabilities)\n",
        "    correct_log_probabilities = log_probabilities[range(n), y]\n",
        "    loss = -torch.mean(correct_log_probabilities)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a589af",
      "metadata": {
        "id": "15a589af"
      },
      "source": [
        "Implementing a function for optimizing paramters and a function to zeroing out their gradients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3121c147",
      "metadata": {
        "id": "3121c147"
      },
      "outputs": [],
      "source": [
        "def sgd_optimizer(parameters: Dict[str, torch.Tensor], learning_rate=0.001):\n",
        "    '''This function gets the parameters and a learning rate. Then updates the parameters using their\n",
        "    gradient. Finally, you should zero the gradients of the parameters after updating\n",
        "    the parameter value.'''\n",
        "    ## FILL HERE\n",
        "    for param in parameters.values():\n",
        "        if param.requires_grad:\n",
        "            param.data -= learning_rate * param.grad\n",
        "\n",
        "            param.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e17b4cf8",
      "metadata": {
        "id": "e17b4cf8"
      },
      "source": [
        "Training functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "76c0f03b",
      "metadata": {
        "id": "76c0f03b"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_pred: np.ndarray, y_true: np.ndarray):\n",
        "    ## FILL HERE\n",
        "    correct_predictions = np.sum(y_pred == y_true)\n",
        "    total_predictions = len(y_true)\n",
        "    acc = correct_predictions / total_predictions\n",
        "    return acc\n",
        "\n",
        "def train(train_loader, learning_rate=0.001, epoch=None):\n",
        "    '''This function implements the training loop for a single epoch. For each batch you should do the following:\n",
        "        1- Calculate the output of the model to the given input batch\n",
        "        2- Calculate the loss based on the model output\n",
        "        3- Update the gradients using backward method\n",
        "        4- Optimize the model parameters using the sgd_optimizer function defined previously\n",
        "        5- Print the train loss (Show the epoch and batch as well)\n",
        "        '''\n",
        "    train_loss = 0\n",
        "    N_train = len(train_loader.dataset)\n",
        "\n",
        "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
        "    # for calculating the accuracy later\n",
        "    Y = []\n",
        "    Y_pred = []\n",
        "\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        p = model(x, parameters)\n",
        "        loss = cross_entropy_loss(p, y)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        sgd_optimizer(parameters, learning_rate)\n",
        "\n",
        "        if (i + 1) % 10 == 0 or (i + 1) == len(train_loader):\n",
        "            print(f'Epoch [{epoch}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        y_pred = p.argmax(dim=-1)\n",
        "        Y.append(y.cpu().numpy())\n",
        "        Y_pred.append(y_pred.cpu().numpy())\n",
        "\n",
        "    Y = np.concatenate(Y)\n",
        "    Y_pred = np.concatenate(Y_pred)\n",
        "    acc = accuracy(Y_pred, Y)\n",
        "    train_loss /= len(train_loader)\n",
        "    print(f'Accuracy of train set: {acc * 100:.2f}%')\n",
        "    return train_loss, acc\n",
        "\n",
        "\n",
        "def validate(loader, epoch=None, set_name=None):\n",
        "    '''This function validates the model on the test dataloader. The function goes through each batch and does\n",
        "    the following on each batch:\n",
        "        1- Calculate the model output\n",
        "        2- Calculate the loss using the model output\n",
        "        3- Print the loss for each batch and epoch\n",
        "\n",
        "    Finally, the function calculates the model accuracy.'''\n",
        "    total_loss = 0\n",
        "    N = len(loader.dataset)\n",
        "\n",
        "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
        "    # for calculating the accuracy later\n",
        "    Y = []\n",
        "    Y_pred = []\n",
        "\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        p = model(x, parameters)\n",
        "        loss = cross_entropy_loss(p, y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch}], Batch [{i+1}/{len(loader)}], {set_name} Loss: {loss.item():.4f}')\n",
        "\n",
        "        y_pred = p.argmax(dim=-1)\n",
        "        Y.append(y.cpu().numpy())\n",
        "        Y_pred.append(y_pred.cpu().numpy())\n",
        "\n",
        "    Y = np.concatenate(Y)\n",
        "    Y_pred = np.concatenate(Y_pred)\n",
        "    acc = accuracy(Y_pred, Y)\n",
        "    total_loss /= len(loader)\n",
        "\n",
        "    print(f'Accuracy of {set_name} set: {acc * 100:.2f}%')\n",
        "\n",
        "    return total_loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "87ebb4b6",
      "metadata": {
        "id": "87ebb4b6"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "28d4eb0b",
      "metadata": {
        "id": "28d4eb0b"
      },
      "outputs": [],
      "source": [
        "def train_model(dataloaders, num_epochs, learning_rate=0.001, model_name='pytorch_model'):\n",
        "    '''This function trains the model for the number of epochs given and stores, calculates and prints the train\n",
        "    and test losses and accuracies. Finally, it plots the accuracy and loss history for training and test sets'''\n",
        "    train_loader, test_loader = dataloaders\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train(train_loader, learning_rate, epoch)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        test_loss, test_acc = validate(test_loader, epoch, set_name='test')\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
        "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc*100:.2f}% - '\n",
        "              f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%')\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss History')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(range(1, num_epochs + 1), test_accuracies, label='Test Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy History')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2ec4bdd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2ec4bdd2",
        "outputId": "587300fb-ed0a-46b4-e8db-de20b9c6ce43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [5], Batch [810/938], Loss: 0.7661\n",
            "Epoch [5], Batch [820/938], Loss: 0.6928\n",
            "Epoch [5], Batch [830/938], Loss: 0.6331\n",
            "Epoch [5], Batch [840/938], Loss: 0.6928\n",
            "Epoch [5], Batch [850/938], Loss: 0.6757\n",
            "Epoch [5], Batch [860/938], Loss: 0.5673\n",
            "Epoch [5], Batch [870/938], Loss: 0.7624\n",
            "Epoch [5], Batch [880/938], Loss: 0.7653\n",
            "Epoch [5], Batch [890/938], Loss: 0.7252\n",
            "Epoch [5], Batch [900/938], Loss: 0.5914\n",
            "Epoch [5], Batch [910/938], Loss: 0.3798\n",
            "Epoch [5], Batch [920/938], Loss: 0.7412\n",
            "Epoch [5], Batch [930/938], Loss: 0.5696\n",
            "Epoch [5], Batch [938/938], Loss: 0.6175\n",
            "Accuracy of train set: 74.10%\n",
            "Epoch [5], Batch [1/157], test Loss: 0.7845\n",
            "Epoch [5], Batch [2/157], test Loss: 0.6860\n",
            "Epoch [5], Batch [3/157], test Loss: 0.7108\n",
            "Epoch [5], Batch [4/157], test Loss: 0.6264\n",
            "Epoch [5], Batch [5/157], test Loss: 0.7030\n",
            "Epoch [5], Batch [6/157], test Loss: 0.7381\n",
            "Epoch [5], Batch [7/157], test Loss: 0.5967\n",
            "Epoch [5], Batch [8/157], test Loss: 0.6511\n",
            "Epoch [5], Batch [9/157], test Loss: 0.7389\n",
            "Epoch [5], Batch [10/157], test Loss: 0.9016\n",
            "Epoch [5], Batch [11/157], test Loss: 0.6521\n",
            "Epoch [5], Batch [12/157], test Loss: 0.4597\n",
            "Epoch [5], Batch [13/157], test Loss: 0.8619\n",
            "Epoch [5], Batch [14/157], test Loss: 0.8602\n",
            "Epoch [5], Batch [15/157], test Loss: 0.9568\n",
            "Epoch [5], Batch [16/157], test Loss: 0.6454\n",
            "Epoch [5], Batch [17/157], test Loss: 0.7204\n",
            "Epoch [5], Batch [18/157], test Loss: 0.6549\n",
            "Epoch [5], Batch [19/157], test Loss: 0.7256\n",
            "Epoch [5], Batch [20/157], test Loss: 0.7017\n",
            "Epoch [5], Batch [21/157], test Loss: 0.7412\n",
            "Epoch [5], Batch [22/157], test Loss: 0.5261\n",
            "Epoch [5], Batch [23/157], test Loss: 0.7201\n",
            "Epoch [5], Batch [24/157], test Loss: 0.7013\n",
            "Epoch [5], Batch [25/157], test Loss: 0.6666\n",
            "Epoch [5], Batch [26/157], test Loss: 0.7664\n",
            "Epoch [5], Batch [27/157], test Loss: 0.7978\n",
            "Epoch [5], Batch [28/157], test Loss: 0.5980\n",
            "Epoch [5], Batch [29/157], test Loss: 0.6820\n",
            "Epoch [5], Batch [30/157], test Loss: 0.7950\n",
            "Epoch [5], Batch [31/157], test Loss: 0.7284\n",
            "Epoch [5], Batch [32/157], test Loss: 1.0246\n",
            "Epoch [5], Batch [33/157], test Loss: 0.6439\n",
            "Epoch [5], Batch [34/157], test Loss: 0.5352\n",
            "Epoch [5], Batch [35/157], test Loss: 0.7443\n",
            "Epoch [5], Batch [36/157], test Loss: 0.6219\n",
            "Epoch [5], Batch [37/157], test Loss: 0.7439\n",
            "Epoch [5], Batch [38/157], test Loss: 0.8335\n",
            "Epoch [5], Batch [39/157], test Loss: 0.6926\n",
            "Epoch [5], Batch [40/157], test Loss: 0.4814\n",
            "Epoch [5], Batch [41/157], test Loss: 0.7013\n",
            "Epoch [5], Batch [42/157], test Loss: 0.6680\n",
            "Epoch [5], Batch [43/157], test Loss: 0.8746\n",
            "Epoch [5], Batch [44/157], test Loss: 0.7342\n",
            "Epoch [5], Batch [45/157], test Loss: 0.6679\n",
            "Epoch [5], Batch [46/157], test Loss: 0.8093\n",
            "Epoch [5], Batch [47/157], test Loss: 0.6867\n",
            "Epoch [5], Batch [48/157], test Loss: 0.9254\n",
            "Epoch [5], Batch [49/157], test Loss: 0.7905\n",
            "Epoch [5], Batch [50/157], test Loss: 0.8547\n",
            "Epoch [5], Batch [51/157], test Loss: 0.8158\n",
            "Epoch [5], Batch [52/157], test Loss: 0.8789\n",
            "Epoch [5], Batch [53/157], test Loss: 0.6773\n",
            "Epoch [5], Batch [54/157], test Loss: 0.5787\n",
            "Epoch [5], Batch [55/157], test Loss: 0.6744\n",
            "Epoch [5], Batch [56/157], test Loss: 0.8528\n",
            "Epoch [5], Batch [57/157], test Loss: 0.6077\n",
            "Epoch [5], Batch [58/157], test Loss: 0.6892\n",
            "Epoch [5], Batch [59/157], test Loss: 0.6638\n",
            "Epoch [5], Batch [60/157], test Loss: 0.6586\n",
            "Epoch [5], Batch [61/157], test Loss: 0.6529\n",
            "Epoch [5], Batch [62/157], test Loss: 0.7284\n",
            "Epoch [5], Batch [63/157], test Loss: 0.6936\n",
            "Epoch [5], Batch [64/157], test Loss: 0.7789\n",
            "Epoch [5], Batch [65/157], test Loss: 0.7419\n",
            "Epoch [5], Batch [66/157], test Loss: 0.7218\n",
            "Epoch [5], Batch [67/157], test Loss: 0.7722\n",
            "Epoch [5], Batch [68/157], test Loss: 0.7203\n",
            "Epoch [5], Batch [69/157], test Loss: 0.9235\n",
            "Epoch [5], Batch [70/157], test Loss: 0.8115\n",
            "Epoch [5], Batch [71/157], test Loss: 0.7855\n",
            "Epoch [5], Batch [72/157], test Loss: 0.7978\n",
            "Epoch [5], Batch [73/157], test Loss: 0.8389\n",
            "Epoch [5], Batch [74/157], test Loss: 0.6634\n",
            "Epoch [5], Batch [75/157], test Loss: 0.4918\n",
            "Epoch [5], Batch [76/157], test Loss: 0.5777\n",
            "Epoch [5], Batch [77/157], test Loss: 0.6539\n",
            "Epoch [5], Batch [78/157], test Loss: 0.4926\n",
            "Epoch [5], Batch [79/157], test Loss: 0.8810\n",
            "Epoch [5], Batch [80/157], test Loss: 0.7087\n",
            "Epoch [5], Batch [81/157], test Loss: 0.7439\n",
            "Epoch [5], Batch [82/157], test Loss: 0.6910\n",
            "Epoch [5], Batch [83/157], test Loss: 0.7144\n",
            "Epoch [5], Batch [84/157], test Loss: 0.7638\n",
            "Epoch [5], Batch [85/157], test Loss: 0.8880\n",
            "Epoch [5], Batch [86/157], test Loss: 0.6698\n",
            "Epoch [5], Batch [87/157], test Loss: 0.7557\n",
            "Epoch [5], Batch [88/157], test Loss: 0.8505\n",
            "Epoch [5], Batch [89/157], test Loss: 0.7547\n",
            "Epoch [5], Batch [90/157], test Loss: 0.6590\n",
            "Epoch [5], Batch [91/157], test Loss: 0.5456\n",
            "Epoch [5], Batch [92/157], test Loss: 0.5693\n",
            "Epoch [5], Batch [93/157], test Loss: 0.7563\n",
            "Epoch [5], Batch [94/157], test Loss: 0.6076\n",
            "Epoch [5], Batch [95/157], test Loss: 0.7968\n",
            "Epoch [5], Batch [96/157], test Loss: 1.1204\n",
            "Epoch [5], Batch [97/157], test Loss: 0.7457\n",
            "Epoch [5], Batch [98/157], test Loss: 0.6771\n",
            "Epoch [5], Batch [99/157], test Loss: 0.6954\n",
            "Epoch [5], Batch [100/157], test Loss: 0.9452\n",
            "Epoch [5], Batch [101/157], test Loss: 0.7531\n",
            "Epoch [5], Batch [102/157], test Loss: 0.7252\n",
            "Epoch [5], Batch [103/157], test Loss: 0.6976\n",
            "Epoch [5], Batch [104/157], test Loss: 0.6996\n",
            "Epoch [5], Batch [105/157], test Loss: 0.5849\n",
            "Epoch [5], Batch [106/157], test Loss: 1.0566\n",
            "Epoch [5], Batch [107/157], test Loss: 0.8126\n",
            "Epoch [5], Batch [108/157], test Loss: 0.7060\n",
            "Epoch [5], Batch [109/157], test Loss: 0.6559\n",
            "Epoch [5], Batch [110/157], test Loss: 0.7210\n",
            "Epoch [5], Batch [111/157], test Loss: 0.7816\n",
            "Epoch [5], Batch [112/157], test Loss: 0.8056\n",
            "Epoch [5], Batch [113/157], test Loss: 0.9055\n",
            "Epoch [5], Batch [114/157], test Loss: 0.6412\n",
            "Epoch [5], Batch [115/157], test Loss: 0.6117\n",
            "Epoch [5], Batch [116/157], test Loss: 0.6435\n",
            "Epoch [5], Batch [117/157], test Loss: 0.7969\n",
            "Epoch [5], Batch [118/157], test Loss: 0.8112\n",
            "Epoch [5], Batch [119/157], test Loss: 0.6498\n",
            "Epoch [5], Batch [120/157], test Loss: 0.7901\n",
            "Epoch [5], Batch [121/157], test Loss: 0.7659\n",
            "Epoch [5], Batch [122/157], test Loss: 0.7473\n",
            "Epoch [5], Batch [123/157], test Loss: 0.6700\n",
            "Epoch [5], Batch [124/157], test Loss: 0.6800\n",
            "Epoch [5], Batch [125/157], test Loss: 0.7301\n",
            "Epoch [5], Batch [126/157], test Loss: 0.7127\n",
            "Epoch [5], Batch [127/157], test Loss: 0.8736\n",
            "Epoch [5], Batch [128/157], test Loss: 0.7814\n",
            "Epoch [5], Batch [129/157], test Loss: 0.6744\n",
            "Epoch [5], Batch [130/157], test Loss: 0.7937\n",
            "Epoch [5], Batch [131/157], test Loss: 0.5971\n",
            "Epoch [5], Batch [132/157], test Loss: 0.9153\n",
            "Epoch [5], Batch [133/157], test Loss: 0.5920\n",
            "Epoch [5], Batch [134/157], test Loss: 1.0025\n",
            "Epoch [5], Batch [135/157], test Loss: 0.6860\n",
            "Epoch [5], Batch [136/157], test Loss: 0.8036\n",
            "Epoch [5], Batch [137/157], test Loss: 0.6184\n",
            "Epoch [5], Batch [138/157], test Loss: 0.8854\n",
            "Epoch [5], Batch [139/157], test Loss: 0.6374\n",
            "Epoch [5], Batch [140/157], test Loss: 0.7652\n",
            "Epoch [5], Batch [141/157], test Loss: 0.8834\n",
            "Epoch [5], Batch [142/157], test Loss: 1.0202\n",
            "Epoch [5], Batch [143/157], test Loss: 0.4891\n",
            "Epoch [5], Batch [144/157], test Loss: 0.7900\n",
            "Epoch [5], Batch [145/157], test Loss: 1.1398\n",
            "Epoch [5], Batch [146/157], test Loss: 0.7457\n",
            "Epoch [5], Batch [147/157], test Loss: 0.9024\n",
            "Epoch [5], Batch [148/157], test Loss: 0.7375\n",
            "Epoch [5], Batch [149/157], test Loss: 0.5509\n",
            "Epoch [5], Batch [150/157], test Loss: 0.7115\n",
            "Epoch [5], Batch [151/157], test Loss: 0.8538\n",
            "Epoch [5], Batch [152/157], test Loss: 0.8049\n",
            "Epoch [5], Batch [153/157], test Loss: 0.6598\n",
            "Epoch [5], Batch [154/157], test Loss: 0.8826\n",
            "Epoch [5], Batch [155/157], test Loss: 0.7148\n",
            "Epoch [5], Batch [156/157], test Loss: 0.6080\n",
            "Epoch [5], Batch [157/157], test Loss: 0.9639\n",
            "Accuracy of test set: 72.76%\n",
            "Epoch [6/25] - Train Loss: 0.7243, Train Accuracy: 74.10% - Test Loss: 0.7379, Test Accuracy: 72.76%\n",
            "Epoch [6], Batch [10/938], Loss: 0.7711\n",
            "Epoch [6], Batch [20/938], Loss: 0.7749\n",
            "Epoch [6], Batch [30/938], Loss: 0.7218\n",
            "Epoch [6], Batch [40/938], Loss: 0.8869\n",
            "Epoch [6], Batch [50/938], Loss: 0.7107\n",
            "Epoch [6], Batch [60/938], Loss: 0.5850\n",
            "Epoch [6], Batch [70/938], Loss: 0.6964\n",
            "Epoch [6], Batch [80/938], Loss: 0.5025\n",
            "Epoch [6], Batch [90/938], Loss: 0.7154\n",
            "Epoch [6], Batch [100/938], Loss: 0.4719\n",
            "Epoch [6], Batch [110/938], Loss: 0.6524\n",
            "Epoch [6], Batch [120/938], Loss: 0.7933\n",
            "Epoch [6], Batch [130/938], Loss: 0.7080\n",
            "Epoch [6], Batch [140/938], Loss: 0.6707\n",
            "Epoch [6], Batch [150/938], Loss: 0.6599\n",
            "Epoch [6], Batch [160/938], Loss: 0.5707\n",
            "Epoch [6], Batch [170/938], Loss: 0.7929\n",
            "Epoch [6], Batch [180/938], Loss: 0.6113\n",
            "Epoch [6], Batch [190/938], Loss: 0.5252\n",
            "Epoch [6], Batch [200/938], Loss: 0.8605\n",
            "Epoch [6], Batch [210/938], Loss: 0.6043\n",
            "Epoch [6], Batch [220/938], Loss: 0.6511\n",
            "Epoch [6], Batch [230/938], Loss: 0.5984\n",
            "Epoch [6], Batch [240/938], Loss: 0.5967\n",
            "Epoch [6], Batch [250/938], Loss: 0.6532\n",
            "Epoch [6], Batch [260/938], Loss: 0.7588\n",
            "Epoch [6], Batch [270/938], Loss: 0.6323\n",
            "Epoch [6], Batch [280/938], Loss: 0.5458\n",
            "Epoch [6], Batch [290/938], Loss: 0.6284\n",
            "Epoch [6], Batch [300/938], Loss: 0.5609\n",
            "Epoch [6], Batch [310/938], Loss: 0.5351\n",
            "Epoch [6], Batch [320/938], Loss: 0.7863\n",
            "Epoch [6], Batch [330/938], Loss: 0.5735\n",
            "Epoch [6], Batch [340/938], Loss: 0.6307\n",
            "Epoch [6], Batch [350/938], Loss: 0.6082\n",
            "Epoch [6], Batch [360/938], Loss: 0.5557\n",
            "Epoch [6], Batch [370/938], Loss: 0.5681\n",
            "Epoch [6], Batch [380/938], Loss: 0.5365\n",
            "Epoch [6], Batch [390/938], Loss: 0.8180\n",
            "Epoch [6], Batch [400/938], Loss: 0.7656\n",
            "Epoch [6], Batch [410/938], Loss: 0.6945\n",
            "Epoch [6], Batch [420/938], Loss: 0.5808\n",
            "Epoch [6], Batch [430/938], Loss: 0.8633\n",
            "Epoch [6], Batch [440/938], Loss: 0.8121\n",
            "Epoch [6], Batch [450/938], Loss: 0.8424\n",
            "Epoch [6], Batch [460/938], Loss: 0.5848\n",
            "Epoch [6], Batch [470/938], Loss: 0.7445\n",
            "Epoch [6], Batch [480/938], Loss: 0.6183\n",
            "Epoch [6], Batch [490/938], Loss: 0.7524\n",
            "Epoch [6], Batch [500/938], Loss: 0.4641\n",
            "Epoch [6], Batch [510/938], Loss: 0.7521\n",
            "Epoch [6], Batch [520/938], Loss: 0.8527\n",
            "Epoch [6], Batch [530/938], Loss: 0.5698\n",
            "Epoch [6], Batch [540/938], Loss: 0.6558\n",
            "Epoch [6], Batch [550/938], Loss: 0.5866\n",
            "Epoch [6], Batch [560/938], Loss: 0.6343\n",
            "Epoch [6], Batch [570/938], Loss: 0.5110\n",
            "Epoch [6], Batch [580/938], Loss: 0.5895\n",
            "Epoch [6], Batch [590/938], Loss: 0.6250\n",
            "Epoch [6], Batch [600/938], Loss: 0.5584\n",
            "Epoch [6], Batch [610/938], Loss: 0.5995\n",
            "Epoch [6], Batch [620/938], Loss: 0.6519\n",
            "Epoch [6], Batch [630/938], Loss: 0.5504\n",
            "Epoch [6], Batch [640/938], Loss: 0.6078\n",
            "Epoch [6], Batch [650/938], Loss: 0.5691\n",
            "Epoch [6], Batch [660/938], Loss: 0.6224\n",
            "Epoch [6], Batch [670/938], Loss: 0.8603\n",
            "Epoch [6], Batch [680/938], Loss: 0.7022\n",
            "Epoch [6], Batch [690/938], Loss: 0.5543\n",
            "Epoch [6], Batch [700/938], Loss: 0.4694\n",
            "Epoch [6], Batch [710/938], Loss: 0.7712\n",
            "Epoch [6], Batch [720/938], Loss: 0.5615\n",
            "Epoch [6], Batch [730/938], Loss: 0.4334\n",
            "Epoch [6], Batch [740/938], Loss: 0.5305\n",
            "Epoch [6], Batch [750/938], Loss: 0.6902\n",
            "Epoch [6], Batch [760/938], Loss: 0.7039\n",
            "Epoch [6], Batch [770/938], Loss: 0.6956\n",
            "Epoch [6], Batch [780/938], Loss: 0.5990\n",
            "Epoch [6], Batch [790/938], Loss: 0.6890\n",
            "Epoch [6], Batch [800/938], Loss: 0.5922\n",
            "Epoch [6], Batch [810/938], Loss: 0.8385\n",
            "Epoch [6], Batch [820/938], Loss: 0.5463\n",
            "Epoch [6], Batch [830/938], Loss: 0.8956\n",
            "Epoch [6], Batch [840/938], Loss: 0.6774\n",
            "Epoch [6], Batch [850/938], Loss: 0.8505\n",
            "Epoch [6], Batch [860/938], Loss: 0.4998\n",
            "Epoch [6], Batch [870/938], Loss: 0.7221\n",
            "Epoch [6], Batch [880/938], Loss: 0.6075\n",
            "Epoch [6], Batch [890/938], Loss: 0.7536\n",
            "Epoch [6], Batch [900/938], Loss: 0.7099\n",
            "Epoch [6], Batch [910/938], Loss: 0.5553\n",
            "Epoch [6], Batch [920/938], Loss: 0.5865\n",
            "Epoch [6], Batch [930/938], Loss: 0.6120\n",
            "Epoch [6], Batch [938/938], Loss: 0.5237\n",
            "Accuracy of train set: 75.79%\n",
            "Epoch [6], Batch [1/157], test Loss: 0.6174\n",
            "Epoch [6], Batch [2/157], test Loss: 0.7121\n",
            "Epoch [6], Batch [3/157], test Loss: 0.7689\n",
            "Epoch [6], Batch [4/157], test Loss: 0.8412\n",
            "Epoch [6], Batch [5/157], test Loss: 0.9190\n",
            "Epoch [6], Batch [6/157], test Loss: 0.6439\n",
            "Epoch [6], Batch [7/157], test Loss: 0.7085\n",
            "Epoch [6], Batch [8/157], test Loss: 0.5713\n",
            "Epoch [6], Batch [9/157], test Loss: 0.5261\n",
            "Epoch [6], Batch [10/157], test Loss: 0.7582\n",
            "Epoch [6], Batch [11/157], test Loss: 0.5991\n",
            "Epoch [6], Batch [12/157], test Loss: 0.9435\n",
            "Epoch [6], Batch [13/157], test Loss: 0.8378\n",
            "Epoch [6], Batch [14/157], test Loss: 0.7240\n",
            "Epoch [6], Batch [15/157], test Loss: 0.9252\n",
            "Epoch [6], Batch [16/157], test Loss: 0.6304\n",
            "Epoch [6], Batch [17/157], test Loss: 0.7859\n",
            "Epoch [6], Batch [18/157], test Loss: 0.7572\n",
            "Epoch [6], Batch [19/157], test Loss: 0.6051\n",
            "Epoch [6], Batch [20/157], test Loss: 0.6393\n",
            "Epoch [6], Batch [21/157], test Loss: 0.7190\n",
            "Epoch [6], Batch [22/157], test Loss: 0.8652\n",
            "Epoch [6], Batch [23/157], test Loss: 0.7144\n",
            "Epoch [6], Batch [24/157], test Loss: 0.8759\n",
            "Epoch [6], Batch [25/157], test Loss: 0.7473\n",
            "Epoch [6], Batch [26/157], test Loss: 0.6085\n",
            "Epoch [6], Batch [27/157], test Loss: 0.5194\n",
            "Epoch [6], Batch [28/157], test Loss: 0.5838\n",
            "Epoch [6], Batch [29/157], test Loss: 0.5485\n",
            "Epoch [6], Batch [30/157], test Loss: 0.6397\n",
            "Epoch [6], Batch [31/157], test Loss: 0.6993\n",
            "Epoch [6], Batch [32/157], test Loss: 0.6754\n",
            "Epoch [6], Batch [33/157], test Loss: 0.5761\n",
            "Epoch [6], Batch [34/157], test Loss: 0.8677\n",
            "Epoch [6], Batch [35/157], test Loss: 0.5676\n",
            "Epoch [6], Batch [36/157], test Loss: 0.7804\n",
            "Epoch [6], Batch [37/157], test Loss: 0.6594\n",
            "Epoch [6], Batch [38/157], test Loss: 0.6020\n",
            "Epoch [6], Batch [39/157], test Loss: 0.9718\n",
            "Epoch [6], Batch [40/157], test Loss: 0.6691\n",
            "Epoch [6], Batch [41/157], test Loss: 0.6967\n",
            "Epoch [6], Batch [42/157], test Loss: 0.7394\n",
            "Epoch [6], Batch [43/157], test Loss: 0.6264\n",
            "Epoch [6], Batch [44/157], test Loss: 0.6563\n",
            "Epoch [6], Batch [45/157], test Loss: 0.5806\n",
            "Epoch [6], Batch [46/157], test Loss: 0.8027\n",
            "Epoch [6], Batch [47/157], test Loss: 0.5458\n",
            "Epoch [6], Batch [48/157], test Loss: 0.8081\n",
            "Epoch [6], Batch [49/157], test Loss: 0.6266\n",
            "Epoch [6], Batch [50/157], test Loss: 0.6491\n",
            "Epoch [6], Batch [51/157], test Loss: 0.6016\n",
            "Epoch [6], Batch [52/157], test Loss: 0.8277\n",
            "Epoch [6], Batch [53/157], test Loss: 0.8471\n",
            "Epoch [6], Batch [54/157], test Loss: 0.6109\n",
            "Epoch [6], Batch [55/157], test Loss: 0.5749\n",
            "Epoch [6], Batch [56/157], test Loss: 0.5394\n",
            "Epoch [6], Batch [57/157], test Loss: 0.5845\n",
            "Epoch [6], Batch [58/157], test Loss: 0.9014\n",
            "Epoch [6], Batch [59/157], test Loss: 0.7326\n",
            "Epoch [6], Batch [60/157], test Loss: 0.7029\n",
            "Epoch [6], Batch [61/157], test Loss: 0.6006\n",
            "Epoch [6], Batch [62/157], test Loss: 0.7224\n",
            "Epoch [6], Batch [63/157], test Loss: 0.6261\n",
            "Epoch [6], Batch [64/157], test Loss: 0.7827\n",
            "Epoch [6], Batch [65/157], test Loss: 0.5773\n",
            "Epoch [6], Batch [66/157], test Loss: 0.6957\n",
            "Epoch [6], Batch [67/157], test Loss: 0.6664\n",
            "Epoch [6], Batch [68/157], test Loss: 0.6233\n",
            "Epoch [6], Batch [69/157], test Loss: 0.5717\n",
            "Epoch [6], Batch [70/157], test Loss: 0.7211\n",
            "Epoch [6], Batch [71/157], test Loss: 0.5843\n",
            "Epoch [6], Batch [72/157], test Loss: 0.7027\n",
            "Epoch [6], Batch [73/157], test Loss: 0.6676\n",
            "Epoch [6], Batch [74/157], test Loss: 0.8225\n",
            "Epoch [6], Batch [75/157], test Loss: 0.7468\n",
            "Epoch [6], Batch [76/157], test Loss: 0.4829\n",
            "Epoch [6], Batch [77/157], test Loss: 0.6800\n",
            "Epoch [6], Batch [78/157], test Loss: 0.5155\n",
            "Epoch [6], Batch [79/157], test Loss: 0.6461\n",
            "Epoch [6], Batch [80/157], test Loss: 0.7376\n",
            "Epoch [6], Batch [81/157], test Loss: 0.8042\n",
            "Epoch [6], Batch [82/157], test Loss: 0.6044\n",
            "Epoch [6], Batch [83/157], test Loss: 0.6494\n",
            "Epoch [6], Batch [84/157], test Loss: 0.7485\n",
            "Epoch [6], Batch [85/157], test Loss: 0.6028\n",
            "Epoch [6], Batch [86/157], test Loss: 1.0933\n",
            "Epoch [6], Batch [87/157], test Loss: 0.4251\n",
            "Epoch [6], Batch [88/157], test Loss: 0.6233\n",
            "Epoch [6], Batch [89/157], test Loss: 0.8694\n",
            "Epoch [6], Batch [90/157], test Loss: 0.6642\n",
            "Epoch [6], Batch [91/157], test Loss: 0.9745\n",
            "Epoch [6], Batch [92/157], test Loss: 0.7984\n",
            "Epoch [6], Batch [93/157], test Loss: 0.6076\n",
            "Epoch [6], Batch [94/157], test Loss: 0.5699\n",
            "Epoch [6], Batch [95/157], test Loss: 0.6985\n",
            "Epoch [6], Batch [96/157], test Loss: 0.5948\n",
            "Epoch [6], Batch [97/157], test Loss: 0.9566\n",
            "Epoch [6], Batch [98/157], test Loss: 0.8222\n",
            "Epoch [6], Batch [99/157], test Loss: 0.4257\n",
            "Epoch [6], Batch [100/157], test Loss: 0.8713\n",
            "Epoch [6], Batch [101/157], test Loss: 0.6780\n",
            "Epoch [6], Batch [102/157], test Loss: 0.6988\n",
            "Epoch [6], Batch [103/157], test Loss: 0.8907\n",
            "Epoch [6], Batch [104/157], test Loss: 0.7295\n",
            "Epoch [6], Batch [105/157], test Loss: 0.7938\n",
            "Epoch [6], Batch [106/157], test Loss: 0.6704\n",
            "Epoch [6], Batch [107/157], test Loss: 0.6512\n",
            "Epoch [6], Batch [108/157], test Loss: 0.5715\n",
            "Epoch [6], Batch [109/157], test Loss: 0.4361\n",
            "Epoch [6], Batch [110/157], test Loss: 0.6976\n",
            "Epoch [6], Batch [111/157], test Loss: 0.9726\n",
            "Epoch [6], Batch [112/157], test Loss: 0.6916\n",
            "Epoch [6], Batch [113/157], test Loss: 0.6830\n",
            "Epoch [6], Batch [114/157], test Loss: 0.5188\n",
            "Epoch [6], Batch [115/157], test Loss: 0.7859\n",
            "Epoch [6], Batch [116/157], test Loss: 0.6600\n",
            "Epoch [6], Batch [117/157], test Loss: 0.8443\n",
            "Epoch [6], Batch [118/157], test Loss: 0.6148\n",
            "Epoch [6], Batch [119/157], test Loss: 0.6682\n",
            "Epoch [6], Batch [120/157], test Loss: 0.6839\n",
            "Epoch [6], Batch [121/157], test Loss: 0.5281\n",
            "Epoch [6], Batch [122/157], test Loss: 0.5799\n",
            "Epoch [6], Batch [123/157], test Loss: 0.7134\n",
            "Epoch [6], Batch [124/157], test Loss: 0.6495\n",
            "Epoch [6], Batch [125/157], test Loss: 0.9352\n",
            "Epoch [6], Batch [126/157], test Loss: 0.7834\n",
            "Epoch [6], Batch [127/157], test Loss: 0.7528\n",
            "Epoch [6], Batch [128/157], test Loss: 0.6759\n",
            "Epoch [6], Batch [129/157], test Loss: 0.5411\n",
            "Epoch [6], Batch [130/157], test Loss: 0.6385\n",
            "Epoch [6], Batch [131/157], test Loss: 0.7930\n",
            "Epoch [6], Batch [132/157], test Loss: 0.6919\n",
            "Epoch [6], Batch [133/157], test Loss: 0.7170\n",
            "Epoch [6], Batch [134/157], test Loss: 0.7769\n",
            "Epoch [6], Batch [135/157], test Loss: 0.7282\n",
            "Epoch [6], Batch [136/157], test Loss: 0.8269\n",
            "Epoch [6], Batch [137/157], test Loss: 0.6756\n",
            "Epoch [6], Batch [138/157], test Loss: 0.7370\n",
            "Epoch [6], Batch [139/157], test Loss: 0.6331\n",
            "Epoch [6], Batch [140/157], test Loss: 0.7570\n",
            "Epoch [6], Batch [141/157], test Loss: 0.7297\n",
            "Epoch [6], Batch [142/157], test Loss: 0.6158\n",
            "Epoch [6], Batch [143/157], test Loss: 0.8525\n",
            "Epoch [6], Batch [144/157], test Loss: 0.7415\n",
            "Epoch [6], Batch [145/157], test Loss: 0.7239\n",
            "Epoch [6], Batch [146/157], test Loss: 0.6886\n",
            "Epoch [6], Batch [147/157], test Loss: 0.6743\n",
            "Epoch [6], Batch [148/157], test Loss: 0.7190\n",
            "Epoch [6], Batch [149/157], test Loss: 0.8750\n",
            "Epoch [6], Batch [150/157], test Loss: 0.6111\n",
            "Epoch [6], Batch [151/157], test Loss: 0.7725\n",
            "Epoch [6], Batch [152/157], test Loss: 0.6216\n",
            "Epoch [6], Batch [153/157], test Loss: 0.6980\n",
            "Epoch [6], Batch [154/157], test Loss: 0.6228\n",
            "Epoch [6], Batch [155/157], test Loss: 0.9081\n",
            "Epoch [6], Batch [156/157], test Loss: 0.5345\n",
            "Epoch [6], Batch [157/157], test Loss: 0.3961\n",
            "Accuracy of test set: 74.90%\n",
            "Epoch [7/25] - Train Loss: 0.6723, Train Accuracy: 75.79% - Test Loss: 0.6974, Test Accuracy: 74.90%\n",
            "Epoch [7], Batch [10/938], Loss: 0.7085\n",
            "Epoch [7], Batch [20/938], Loss: 0.6271\n",
            "Epoch [7], Batch [30/938], Loss: 0.6693\n",
            "Epoch [7], Batch [40/938], Loss: 0.7246\n",
            "Epoch [7], Batch [50/938], Loss: 0.5118\n",
            "Epoch [7], Batch [60/938], Loss: 0.5638\n",
            "Epoch [7], Batch [70/938], Loss: 0.7999\n",
            "Epoch [7], Batch [80/938], Loss: 0.7601\n",
            "Epoch [7], Batch [90/938], Loss: 0.5724\n",
            "Epoch [7], Batch [100/938], Loss: 0.6400\n",
            "Epoch [7], Batch [110/938], Loss: 0.6327\n",
            "Epoch [7], Batch [120/938], Loss: 0.7006\n",
            "Epoch [7], Batch [130/938], Loss: 0.5547\n",
            "Epoch [7], Batch [140/938], Loss: 0.6961\n",
            "Epoch [7], Batch [150/938], Loss: 0.8342\n",
            "Epoch [7], Batch [160/938], Loss: 0.6385\n",
            "Epoch [7], Batch [170/938], Loss: 0.6201\n",
            "Epoch [7], Batch [180/938], Loss: 0.6933\n",
            "Epoch [7], Batch [190/938], Loss: 0.7727\n",
            "Epoch [7], Batch [200/938], Loss: 0.5983\n",
            "Epoch [7], Batch [210/938], Loss: 0.4994\n",
            "Epoch [7], Batch [220/938], Loss: 0.6788\n",
            "Epoch [7], Batch [230/938], Loss: 0.4993\n",
            "Epoch [7], Batch [240/938], Loss: 0.6833\n",
            "Epoch [7], Batch [250/938], Loss: 0.5880\n",
            "Epoch [7], Batch [260/938], Loss: 0.7679\n",
            "Epoch [7], Batch [270/938], Loss: 0.7354\n",
            "Epoch [7], Batch [280/938], Loss: 0.7938\n",
            "Epoch [7], Batch [290/938], Loss: 0.6424\n",
            "Epoch [7], Batch [300/938], Loss: 0.4592\n",
            "Epoch [7], Batch [310/938], Loss: 0.5219\n",
            "Epoch [7], Batch [320/938], Loss: 0.7206\n",
            "Epoch [7], Batch [330/938], Loss: 0.5742\n",
            "Epoch [7], Batch [340/938], Loss: 0.5596\n",
            "Epoch [7], Batch [350/938], Loss: 0.6155\n",
            "Epoch [7], Batch [360/938], Loss: 0.8086\n",
            "Epoch [7], Batch [370/938], Loss: 0.6605\n",
            "Epoch [7], Batch [380/938], Loss: 0.5143\n",
            "Epoch [7], Batch [390/938], Loss: 0.7372\n",
            "Epoch [7], Batch [400/938], Loss: 0.5015\n",
            "Epoch [7], Batch [410/938], Loss: 0.7212\n",
            "Epoch [7], Batch [420/938], Loss: 0.5198\n",
            "Epoch [7], Batch [430/938], Loss: 0.6604\n",
            "Epoch [7], Batch [440/938], Loss: 0.5951\n",
            "Epoch [7], Batch [450/938], Loss: 0.6871\n",
            "Epoch [7], Batch [460/938], Loss: 0.8301\n",
            "Epoch [7], Batch [470/938], Loss: 0.6611\n",
            "Epoch [7], Batch [480/938], Loss: 0.6780\n",
            "Epoch [7], Batch [490/938], Loss: 0.6899\n",
            "Epoch [7], Batch [500/938], Loss: 0.5016\n",
            "Epoch [7], Batch [510/938], Loss: 0.5919\n",
            "Epoch [7], Batch [520/938], Loss: 0.5838\n",
            "Epoch [7], Batch [530/938], Loss: 0.7229\n",
            "Epoch [7], Batch [540/938], Loss: 0.7802\n",
            "Epoch [7], Batch [550/938], Loss: 0.7258\n",
            "Epoch [7], Batch [560/938], Loss: 0.6222\n",
            "Epoch [7], Batch [570/938], Loss: 0.7335\n",
            "Epoch [7], Batch [580/938], Loss: 0.5315\n",
            "Epoch [7], Batch [590/938], Loss: 0.4469\n",
            "Epoch [7], Batch [600/938], Loss: 0.5753\n",
            "Epoch [7], Batch [610/938], Loss: 0.8424\n",
            "Epoch [7], Batch [620/938], Loss: 0.6516\n",
            "Epoch [7], Batch [630/938], Loss: 0.6532\n",
            "Epoch [7], Batch [640/938], Loss: 0.7048\n",
            "Epoch [7], Batch [650/938], Loss: 0.5292\n",
            "Epoch [7], Batch [660/938], Loss: 0.5836\n",
            "Epoch [7], Batch [670/938], Loss: 0.6308\n",
            "Epoch [7], Batch [680/938], Loss: 0.5680\n",
            "Epoch [7], Batch [690/938], Loss: 0.3875\n",
            "Epoch [7], Batch [700/938], Loss: 0.5905\n",
            "Epoch [7], Batch [710/938], Loss: 0.5366\n",
            "Epoch [7], Batch [720/938], Loss: 0.6177\n",
            "Epoch [7], Batch [730/938], Loss: 0.5406\n",
            "Epoch [7], Batch [740/938], Loss: 0.6613\n",
            "Epoch [7], Batch [750/938], Loss: 0.6477\n",
            "Epoch [7], Batch [760/938], Loss: 0.5700\n",
            "Epoch [7], Batch [770/938], Loss: 0.7963\n",
            "Epoch [7], Batch [780/938], Loss: 0.5733\n",
            "Epoch [7], Batch [790/938], Loss: 0.6160\n",
            "Epoch [7], Batch [800/938], Loss: 0.7076\n",
            "Epoch [7], Batch [810/938], Loss: 0.5627\n",
            "Epoch [7], Batch [820/938], Loss: 0.4795\n",
            "Epoch [7], Batch [830/938], Loss: 0.5580\n",
            "Epoch [7], Batch [840/938], Loss: 0.3799\n",
            "Epoch [7], Batch [850/938], Loss: 0.5825\n",
            "Epoch [7], Batch [860/938], Loss: 0.5396\n",
            "Epoch [7], Batch [870/938], Loss: 0.6810\n",
            "Epoch [7], Batch [880/938], Loss: 0.6927\n",
            "Epoch [7], Batch [890/938], Loss: 0.8023\n",
            "Epoch [7], Batch [900/938], Loss: 0.6679\n",
            "Epoch [7], Batch [910/938], Loss: 0.6711\n",
            "Epoch [7], Batch [920/938], Loss: 0.4676\n",
            "Epoch [7], Batch [930/938], Loss: 0.6502\n",
            "Epoch [7], Batch [938/938], Loss: 0.4128\n",
            "Accuracy of train set: 77.32%\n",
            "Epoch [7], Batch [1/157], test Loss: 0.5118\n",
            "Epoch [7], Batch [2/157], test Loss: 0.7738\n",
            "Epoch [7], Batch [3/157], test Loss: 0.6819\n",
            "Epoch [7], Batch [4/157], test Loss: 0.8421\n",
            "Epoch [7], Batch [5/157], test Loss: 0.5628\n",
            "Epoch [7], Batch [6/157], test Loss: 0.4736\n",
            "Epoch [7], Batch [7/157], test Loss: 0.5614\n",
            "Epoch [7], Batch [8/157], test Loss: 0.5476\n",
            "Epoch [7], Batch [9/157], test Loss: 0.4871\n",
            "Epoch [7], Batch [10/157], test Loss: 0.5450\n",
            "Epoch [7], Batch [11/157], test Loss: 0.5330\n",
            "Epoch [7], Batch [12/157], test Loss: 0.6801\n",
            "Epoch [7], Batch [13/157], test Loss: 0.4947\n",
            "Epoch [7], Batch [14/157], test Loss: 0.6262\n",
            "Epoch [7], Batch [15/157], test Loss: 0.4747\n",
            "Epoch [7], Batch [16/157], test Loss: 0.6913\n",
            "Epoch [7], Batch [17/157], test Loss: 0.4374\n",
            "Epoch [7], Batch [18/157], test Loss: 0.5911\n",
            "Epoch [7], Batch [19/157], test Loss: 0.6442\n",
            "Epoch [7], Batch [20/157], test Loss: 0.7629\n",
            "Epoch [7], Batch [21/157], test Loss: 0.5144\n",
            "Epoch [7], Batch [22/157], test Loss: 0.7346\n",
            "Epoch [7], Batch [23/157], test Loss: 0.6966\n",
            "Epoch [7], Batch [24/157], test Loss: 0.5066\n",
            "Epoch [7], Batch [25/157], test Loss: 0.6177\n",
            "Epoch [7], Batch [26/157], test Loss: 0.6941\n",
            "Epoch [7], Batch [27/157], test Loss: 0.5339\n",
            "Epoch [7], Batch [28/157], test Loss: 0.5009\n",
            "Epoch [7], Batch [29/157], test Loss: 0.6723\n",
            "Epoch [7], Batch [30/157], test Loss: 0.7006\n",
            "Epoch [7], Batch [31/157], test Loss: 0.6356\n",
            "Epoch [7], Batch [32/157], test Loss: 0.7365\n",
            "Epoch [7], Batch [33/157], test Loss: 0.5620\n",
            "Epoch [7], Batch [34/157], test Loss: 0.7541\n",
            "Epoch [7], Batch [35/157], test Loss: 0.3146\n",
            "Epoch [7], Batch [36/157], test Loss: 0.4932\n",
            "Epoch [7], Batch [37/157], test Loss: 0.6842\n",
            "Epoch [7], Batch [38/157], test Loss: 0.5935\n",
            "Epoch [7], Batch [39/157], test Loss: 0.6488\n",
            "Epoch [7], Batch [40/157], test Loss: 0.5510\n",
            "Epoch [7], Batch [41/157], test Loss: 0.5272\n",
            "Epoch [7], Batch [42/157], test Loss: 0.6115\n",
            "Epoch [7], Batch [43/157], test Loss: 0.6395\n",
            "Epoch [7], Batch [44/157], test Loss: 0.6044\n",
            "Epoch [7], Batch [45/157], test Loss: 0.6576\n",
            "Epoch [7], Batch [46/157], test Loss: 0.7809\n",
            "Epoch [7], Batch [47/157], test Loss: 0.5274\n",
            "Epoch [7], Batch [48/157], test Loss: 0.5117\n",
            "Epoch [7], Batch [49/157], test Loss: 0.6078\n",
            "Epoch [7], Batch [50/157], test Loss: 0.6896\n",
            "Epoch [7], Batch [51/157], test Loss: 0.5629\n",
            "Epoch [7], Batch [52/157], test Loss: 0.6472\n",
            "Epoch [7], Batch [53/157], test Loss: 0.6249\n",
            "Epoch [7], Batch [54/157], test Loss: 0.7595\n",
            "Epoch [7], Batch [55/157], test Loss: 0.6711\n",
            "Epoch [7], Batch [56/157], test Loss: 0.5377\n",
            "Epoch [7], Batch [57/157], test Loss: 0.6435\n",
            "Epoch [7], Batch [58/157], test Loss: 0.5130\n",
            "Epoch [7], Batch [59/157], test Loss: 0.9252\n",
            "Epoch [7], Batch [60/157], test Loss: 0.7020\n",
            "Epoch [7], Batch [61/157], test Loss: 0.7181\n",
            "Epoch [7], Batch [62/157], test Loss: 0.6461\n",
            "Epoch [7], Batch [63/157], test Loss: 0.6369\n",
            "Epoch [7], Batch [64/157], test Loss: 0.7203\n",
            "Epoch [7], Batch [65/157], test Loss: 0.6691\n",
            "Epoch [7], Batch [66/157], test Loss: 0.4815\n",
            "Epoch [7], Batch [67/157], test Loss: 0.9679\n",
            "Epoch [7], Batch [68/157], test Loss: 0.6855\n",
            "Epoch [7], Batch [69/157], test Loss: 0.6162\n",
            "Epoch [7], Batch [70/157], test Loss: 0.4917\n",
            "Epoch [7], Batch [71/157], test Loss: 0.6962\n",
            "Epoch [7], Batch [72/157], test Loss: 0.5547\n",
            "Epoch [7], Batch [73/157], test Loss: 0.5299\n",
            "Epoch [7], Batch [74/157], test Loss: 0.5374\n",
            "Epoch [7], Batch [75/157], test Loss: 0.4803\n",
            "Epoch [7], Batch [76/157], test Loss: 0.5441\n",
            "Epoch [7], Batch [77/157], test Loss: 0.5589\n",
            "Epoch [7], Batch [78/157], test Loss: 0.5017\n",
            "Epoch [7], Batch [79/157], test Loss: 0.6611\n",
            "Epoch [7], Batch [80/157], test Loss: 0.6549\n",
            "Epoch [7], Batch [81/157], test Loss: 0.7872\n",
            "Epoch [7], Batch [82/157], test Loss: 0.3664\n",
            "Epoch [7], Batch [83/157], test Loss: 0.6763\n",
            "Epoch [7], Batch [84/157], test Loss: 0.6171\n",
            "Epoch [7], Batch [85/157], test Loss: 0.6410\n",
            "Epoch [7], Batch [86/157], test Loss: 0.6201\n",
            "Epoch [7], Batch [87/157], test Loss: 0.4212\n",
            "Epoch [7], Batch [88/157], test Loss: 0.4135\n",
            "Epoch [7], Batch [89/157], test Loss: 0.7030\n",
            "Epoch [7], Batch [90/157], test Loss: 0.8916\n",
            "Epoch [7], Batch [91/157], test Loss: 0.5986\n",
            "Epoch [7], Batch [92/157], test Loss: 0.8104\n",
            "Epoch [7], Batch [93/157], test Loss: 0.5125\n",
            "Epoch [7], Batch [94/157], test Loss: 0.6119\n",
            "Epoch [7], Batch [95/157], test Loss: 0.5360\n",
            "Epoch [7], Batch [96/157], test Loss: 0.6998\n",
            "Epoch [7], Batch [97/157], test Loss: 0.5715\n",
            "Epoch [7], Batch [98/157], test Loss: 0.7327\n",
            "Epoch [7], Batch [99/157], test Loss: 0.8741\n",
            "Epoch [7], Batch [100/157], test Loss: 0.4998\n",
            "Epoch [7], Batch [101/157], test Loss: 0.6314\n",
            "Epoch [7], Batch [102/157], test Loss: 0.4743\n",
            "Epoch [7], Batch [103/157], test Loss: 0.6818\n",
            "Epoch [7], Batch [104/157], test Loss: 0.7440\n",
            "Epoch [7], Batch [105/157], test Loss: 0.7086\n",
            "Epoch [7], Batch [106/157], test Loss: 0.5306\n",
            "Epoch [7], Batch [107/157], test Loss: 0.7038\n",
            "Epoch [7], Batch [108/157], test Loss: 0.5181\n",
            "Epoch [7], Batch [109/157], test Loss: 0.3976\n",
            "Epoch [7], Batch [110/157], test Loss: 0.5207\n",
            "Epoch [7], Batch [111/157], test Loss: 0.5806\n",
            "Epoch [7], Batch [112/157], test Loss: 0.6509\n",
            "Epoch [7], Batch [113/157], test Loss: 0.6124\n",
            "Epoch [7], Batch [114/157], test Loss: 0.5418\n",
            "Epoch [7], Batch [115/157], test Loss: 0.5263\n",
            "Epoch [7], Batch [116/157], test Loss: 0.4759\n",
            "Epoch [7], Batch [117/157], test Loss: 0.8017\n",
            "Epoch [7], Batch [118/157], test Loss: 0.6515\n",
            "Epoch [7], Batch [119/157], test Loss: 0.6694\n",
            "Epoch [7], Batch [120/157], test Loss: 0.5277\n",
            "Epoch [7], Batch [121/157], test Loss: 0.8902\n",
            "Epoch [7], Batch [122/157], test Loss: 0.7591\n",
            "Epoch [7], Batch [123/157], test Loss: 0.8571\n",
            "Epoch [7], Batch [124/157], test Loss: 0.5436\n",
            "Epoch [7], Batch [125/157], test Loss: 0.4874\n",
            "Epoch [7], Batch [126/157], test Loss: 0.7032\n",
            "Epoch [7], Batch [127/157], test Loss: 0.6930\n",
            "Epoch [7], Batch [128/157], test Loss: 1.0058\n",
            "Epoch [7], Batch [129/157], test Loss: 0.5647\n",
            "Epoch [7], Batch [130/157], test Loss: 0.7705\n",
            "Epoch [7], Batch [131/157], test Loss: 0.6839\n",
            "Epoch [7], Batch [132/157], test Loss: 0.7052\n",
            "Epoch [7], Batch [133/157], test Loss: 0.6291\n",
            "Epoch [7], Batch [134/157], test Loss: 0.5223\n",
            "Epoch [7], Batch [135/157], test Loss: 0.6983\n",
            "Epoch [7], Batch [136/157], test Loss: 0.5890\n",
            "Epoch [7], Batch [137/157], test Loss: 0.6251\n",
            "Epoch [7], Batch [138/157], test Loss: 0.4925\n",
            "Epoch [7], Batch [139/157], test Loss: 0.5606\n",
            "Epoch [7], Batch [140/157], test Loss: 0.6468\n",
            "Epoch [7], Batch [141/157], test Loss: 0.6561\n",
            "Epoch [7], Batch [142/157], test Loss: 0.6410\n",
            "Epoch [7], Batch [143/157], test Loss: 0.8186\n",
            "Epoch [7], Batch [144/157], test Loss: 0.5955\n",
            "Epoch [7], Batch [145/157], test Loss: 0.5264\n",
            "Epoch [7], Batch [146/157], test Loss: 0.5814\n",
            "Epoch [7], Batch [147/157], test Loss: 0.7137\n",
            "Epoch [7], Batch [148/157], test Loss: 0.4577\n",
            "Epoch [7], Batch [149/157], test Loss: 0.7736\n",
            "Epoch [7], Batch [150/157], test Loss: 0.4697\n",
            "Epoch [7], Batch [151/157], test Loss: 0.7302\n",
            "Epoch [7], Batch [152/157], test Loss: 0.7733\n",
            "Epoch [7], Batch [153/157], test Loss: 0.7109\n",
            "Epoch [7], Batch [154/157], test Loss: 0.7017\n",
            "Epoch [7], Batch [155/157], test Loss: 0.5912\n",
            "Epoch [7], Batch [156/157], test Loss: 0.6904\n",
            "Epoch [7], Batch [157/157], test Loss: 0.5032\n",
            "Accuracy of test set: 77.36%\n",
            "Epoch [8/25] - Train Loss: 0.6286, Train Accuracy: 77.32% - Test Loss: 0.6246, Test Accuracy: 77.36%\n",
            "Epoch [8], Batch [10/938], Loss: 0.4286\n",
            "Epoch [8], Batch [20/938], Loss: 0.5282\n",
            "Epoch [8], Batch [30/938], Loss: 0.7590\n",
            "Epoch [8], Batch [40/938], Loss: 0.6906\n",
            "Epoch [8], Batch [50/938], Loss: 0.5770\n",
            "Epoch [8], Batch [60/938], Loss: 0.3761\n",
            "Epoch [8], Batch [70/938], Loss: 0.3729\n",
            "Epoch [8], Batch [80/938], Loss: 0.4641\n",
            "Epoch [8], Batch [90/938], Loss: 0.4660\n",
            "Epoch [8], Batch [100/938], Loss: 0.4605\n",
            "Epoch [8], Batch [110/938], Loss: 0.4818\n",
            "Epoch [8], Batch [120/938], Loss: 0.6794\n",
            "Epoch [8], Batch [130/938], Loss: 0.6439\n",
            "Epoch [8], Batch [140/938], Loss: 0.7734\n",
            "Epoch [8], Batch [150/938], Loss: 0.5884\n",
            "Epoch [8], Batch [160/938], Loss: 0.4531\n",
            "Epoch [8], Batch [170/938], Loss: 0.6516\n",
            "Epoch [8], Batch [180/938], Loss: 0.6430\n",
            "Epoch [8], Batch [190/938], Loss: 0.6733\n",
            "Epoch [8], Batch [200/938], Loss: 0.6214\n",
            "Epoch [8], Batch [210/938], Loss: 0.9131\n",
            "Epoch [8], Batch [220/938], Loss: 0.7962\n",
            "Epoch [8], Batch [230/938], Loss: 0.8017\n",
            "Epoch [8], Batch [240/938], Loss: 0.6988\n",
            "Epoch [8], Batch [250/938], Loss: 0.7289\n",
            "Epoch [8], Batch [260/938], Loss: 0.6067\n",
            "Epoch [8], Batch [270/938], Loss: 0.6482\n",
            "Epoch [8], Batch [280/938], Loss: 0.6121\n",
            "Epoch [8], Batch [290/938], Loss: 0.5947\n",
            "Epoch [8], Batch [300/938], Loss: 0.5905\n",
            "Epoch [8], Batch [310/938], Loss: 0.6209\n",
            "Epoch [8], Batch [320/938], Loss: 0.4034\n",
            "Epoch [8], Batch [330/938], Loss: 0.5800\n",
            "Epoch [8], Batch [340/938], Loss: 0.5143\n",
            "Epoch [8], Batch [350/938], Loss: 0.5932\n",
            "Epoch [8], Batch [360/938], Loss: 0.6259\n",
            "Epoch [8], Batch [370/938], Loss: 0.6245\n",
            "Epoch [8], Batch [380/938], Loss: 0.5376\n",
            "Epoch [8], Batch [390/938], Loss: 0.5718\n",
            "Epoch [8], Batch [400/938], Loss: 0.6855\n",
            "Epoch [8], Batch [410/938], Loss: 0.6013\n",
            "Epoch [8], Batch [420/938], Loss: 0.6603\n",
            "Epoch [8], Batch [430/938], Loss: 0.4465\n",
            "Epoch [8], Batch [440/938], Loss: 0.7352\n",
            "Epoch [8], Batch [450/938], Loss: 0.7593\n",
            "Epoch [8], Batch [460/938], Loss: 0.6746\n",
            "Epoch [8], Batch [470/938], Loss: 0.5818\n",
            "Epoch [8], Batch [480/938], Loss: 0.6707\n",
            "Epoch [8], Batch [490/938], Loss: 0.6481\n",
            "Epoch [8], Batch [500/938], Loss: 0.9196\n",
            "Epoch [8], Batch [510/938], Loss: 0.4732\n",
            "Epoch [8], Batch [520/938], Loss: 0.6239\n",
            "Epoch [8], Batch [530/938], Loss: 0.7814\n",
            "Epoch [8], Batch [540/938], Loss: 0.9684\n",
            "Epoch [8], Batch [550/938], Loss: 0.8020\n",
            "Epoch [8], Batch [560/938], Loss: 0.8813\n",
            "Epoch [8], Batch [570/938], Loss: 0.8778\n",
            "Epoch [8], Batch [580/938], Loss: 0.6964\n",
            "Epoch [8], Batch [590/938], Loss: 0.5588\n",
            "Epoch [8], Batch [600/938], Loss: 0.6006\n",
            "Epoch [8], Batch [610/938], Loss: 0.5008\n",
            "Epoch [8], Batch [620/938], Loss: 0.6956\n",
            "Epoch [8], Batch [630/938], Loss: 0.5422\n",
            "Epoch [8], Batch [640/938], Loss: 0.6392\n",
            "Epoch [8], Batch [650/938], Loss: 0.4721\n",
            "Epoch [8], Batch [660/938], Loss: 0.5318\n",
            "Epoch [8], Batch [670/938], Loss: 0.8382\n",
            "Epoch [8], Batch [680/938], Loss: 0.5819\n",
            "Epoch [8], Batch [690/938], Loss: 0.6001\n",
            "Epoch [8], Batch [700/938], Loss: 0.3873\n",
            "Epoch [8], Batch [710/938], Loss: 0.5444\n",
            "Epoch [8], Batch [720/938], Loss: 0.7243\n",
            "Epoch [8], Batch [730/938], Loss: 0.7322\n",
            "Epoch [8], Batch [740/938], Loss: 0.7777\n",
            "Epoch [8], Batch [750/938], Loss: 0.7075\n",
            "Epoch [8], Batch [760/938], Loss: 0.5590\n",
            "Epoch [8], Batch [770/938], Loss: 0.6478\n",
            "Epoch [8], Batch [780/938], Loss: 0.7338\n",
            "Epoch [8], Batch [790/938], Loss: 0.5023\n",
            "Epoch [8], Batch [800/938], Loss: 0.8421\n",
            "Epoch [8], Batch [810/938], Loss: 0.5202\n",
            "Epoch [8], Batch [820/938], Loss: 0.7137\n",
            "Epoch [8], Batch [830/938], Loss: 0.4857\n",
            "Epoch [8], Batch [840/938], Loss: 0.5268\n",
            "Epoch [8], Batch [850/938], Loss: 0.6674\n",
            "Epoch [8], Batch [860/938], Loss: 0.6726\n",
            "Epoch [8], Batch [870/938], Loss: 0.7108\n",
            "Epoch [8], Batch [880/938], Loss: 0.5974\n",
            "Epoch [8], Batch [890/938], Loss: 0.6911\n",
            "Epoch [8], Batch [900/938], Loss: 0.6697\n",
            "Epoch [8], Batch [910/938], Loss: 0.5204\n",
            "Epoch [8], Batch [920/938], Loss: 0.3971\n",
            "Epoch [8], Batch [930/938], Loss: 0.6946\n",
            "Epoch [8], Batch [938/938], Loss: 0.5855\n",
            "Accuracy of train set: 78.36%\n",
            "Epoch [8], Batch [1/157], test Loss: 0.5491\n",
            "Epoch [8], Batch [2/157], test Loss: 0.8746\n",
            "Epoch [8], Batch [3/157], test Loss: 0.5402\n",
            "Epoch [8], Batch [4/157], test Loss: 0.7749\n",
            "Epoch [8], Batch [5/157], test Loss: 0.5609\n",
            "Epoch [8], Batch [6/157], test Loss: 0.5500\n",
            "Epoch [8], Batch [7/157], test Loss: 0.4693\n",
            "Epoch [8], Batch [8/157], test Loss: 0.7974\n",
            "Epoch [8], Batch [9/157], test Loss: 0.6468\n",
            "Epoch [8], Batch [10/157], test Loss: 0.7056\n",
            "Epoch [8], Batch [11/157], test Loss: 0.8070\n",
            "Epoch [8], Batch [12/157], test Loss: 0.4417\n",
            "Epoch [8], Batch [13/157], test Loss: 0.4756\n",
            "Epoch [8], Batch [14/157], test Loss: 0.6665\n",
            "Epoch [8], Batch [15/157], test Loss: 0.4155\n",
            "Epoch [8], Batch [16/157], test Loss: 0.6281\n",
            "Epoch [8], Batch [17/157], test Loss: 0.5611\n",
            "Epoch [8], Batch [18/157], test Loss: 0.5006\n",
            "Epoch [8], Batch [19/157], test Loss: 0.4420\n",
            "Epoch [8], Batch [20/157], test Loss: 0.6279\n",
            "Epoch [8], Batch [21/157], test Loss: 0.7947\n",
            "Epoch [8], Batch [22/157], test Loss: 0.5487\n",
            "Epoch [8], Batch [23/157], test Loss: 0.8617\n",
            "Epoch [8], Batch [24/157], test Loss: 0.4773\n",
            "Epoch [8], Batch [25/157], test Loss: 0.6722\n",
            "Epoch [8], Batch [26/157], test Loss: 0.6280\n",
            "Epoch [8], Batch [27/157], test Loss: 0.4850\n",
            "Epoch [8], Batch [28/157], test Loss: 0.4495\n",
            "Epoch [8], Batch [29/157], test Loss: 0.6257\n",
            "Epoch [8], Batch [30/157], test Loss: 0.4566\n",
            "Epoch [8], Batch [31/157], test Loss: 0.7083\n",
            "Epoch [8], Batch [32/157], test Loss: 0.6336\n",
            "Epoch [8], Batch [33/157], test Loss: 0.5340\n",
            "Epoch [8], Batch [34/157], test Loss: 0.8561\n",
            "Epoch [8], Batch [35/157], test Loss: 0.5809\n",
            "Epoch [8], Batch [36/157], test Loss: 0.6534\n",
            "Epoch [8], Batch [37/157], test Loss: 0.6558\n",
            "Epoch [8], Batch [38/157], test Loss: 0.5796\n",
            "Epoch [8], Batch [39/157], test Loss: 0.4548\n",
            "Epoch [8], Batch [40/157], test Loss: 0.5986\n",
            "Epoch [8], Batch [41/157], test Loss: 0.8414\n",
            "Epoch [8], Batch [42/157], test Loss: 0.7027\n",
            "Epoch [8], Batch [43/157], test Loss: 0.8341\n",
            "Epoch [8], Batch [44/157], test Loss: 0.7103\n",
            "Epoch [8], Batch [45/157], test Loss: 0.6715\n",
            "Epoch [8], Batch [46/157], test Loss: 0.5437\n",
            "Epoch [8], Batch [47/157], test Loss: 0.9630\n",
            "Epoch [8], Batch [48/157], test Loss: 0.6308\n",
            "Epoch [8], Batch [49/157], test Loss: 0.7089\n",
            "Epoch [8], Batch [50/157], test Loss: 0.6427\n",
            "Epoch [8], Batch [51/157], test Loss: 0.5453\n",
            "Epoch [8], Batch [52/157], test Loss: 0.6696\n",
            "Epoch [8], Batch [53/157], test Loss: 0.7396\n",
            "Epoch [8], Batch [54/157], test Loss: 0.4313\n",
            "Epoch [8], Batch [55/157], test Loss: 0.6491\n",
            "Epoch [8], Batch [56/157], test Loss: 0.6620\n",
            "Epoch [8], Batch [57/157], test Loss: 0.5573\n",
            "Epoch [8], Batch [58/157], test Loss: 0.7614\n",
            "Epoch [8], Batch [59/157], test Loss: 0.6450\n",
            "Epoch [8], Batch [60/157], test Loss: 0.3988\n",
            "Epoch [8], Batch [61/157], test Loss: 0.6780\n",
            "Epoch [8], Batch [62/157], test Loss: 0.5573\n",
            "Epoch [8], Batch [63/157], test Loss: 0.6812\n",
            "Epoch [8], Batch [64/157], test Loss: 0.3352\n",
            "Epoch [8], Batch [65/157], test Loss: 0.6965\n",
            "Epoch [8], Batch [66/157], test Loss: 0.5773\n",
            "Epoch [8], Batch [67/157], test Loss: 0.7712\n",
            "Epoch [8], Batch [68/157], test Loss: 0.6142\n",
            "Epoch [8], Batch [69/157], test Loss: 0.7566\n",
            "Epoch [8], Batch [70/157], test Loss: 0.8397\n",
            "Epoch [8], Batch [71/157], test Loss: 0.5110\n",
            "Epoch [8], Batch [72/157], test Loss: 0.6597\n",
            "Epoch [8], Batch [73/157], test Loss: 0.6594\n",
            "Epoch [8], Batch [74/157], test Loss: 0.6983\n",
            "Epoch [8], Batch [75/157], test Loss: 0.5055\n",
            "Epoch [8], Batch [76/157], test Loss: 0.9477\n",
            "Epoch [8], Batch [77/157], test Loss: 0.6761\n",
            "Epoch [8], Batch [78/157], test Loss: 0.6383\n",
            "Epoch [8], Batch [79/157], test Loss: 0.6719\n",
            "Epoch [8], Batch [80/157], test Loss: 0.4249\n",
            "Epoch [8], Batch [81/157], test Loss: 0.6290\n",
            "Epoch [8], Batch [82/157], test Loss: 0.6015\n",
            "Epoch [8], Batch [83/157], test Loss: 0.5814\n",
            "Epoch [8], Batch [84/157], test Loss: 0.4240\n",
            "Epoch [8], Batch [85/157], test Loss: 0.5247\n",
            "Epoch [8], Batch [86/157], test Loss: 0.6570\n",
            "Epoch [8], Batch [87/157], test Loss: 0.6260\n",
            "Epoch [8], Batch [88/157], test Loss: 0.8422\n",
            "Epoch [8], Batch [89/157], test Loss: 0.6737\n",
            "Epoch [8], Batch [90/157], test Loss: 0.5886\n",
            "Epoch [8], Batch [91/157], test Loss: 0.7454\n",
            "Epoch [8], Batch [92/157], test Loss: 0.6203\n",
            "Epoch [8], Batch [93/157], test Loss: 0.7152\n",
            "Epoch [8], Batch [94/157], test Loss: 0.5335\n",
            "Epoch [8], Batch [95/157], test Loss: 0.6255\n",
            "Epoch [8], Batch [96/157], test Loss: 0.7816\n",
            "Epoch [8], Batch [97/157], test Loss: 0.8973\n",
            "Epoch [8], Batch [98/157], test Loss: 0.6026\n",
            "Epoch [8], Batch [99/157], test Loss: 0.8913\n",
            "Epoch [8], Batch [100/157], test Loss: 0.7963\n",
            "Epoch [8], Batch [101/157], test Loss: 0.6359\n",
            "Epoch [8], Batch [102/157], test Loss: 0.7540\n",
            "Epoch [8], Batch [103/157], test Loss: 0.6843\n",
            "Epoch [8], Batch [104/157], test Loss: 0.4738\n",
            "Epoch [8], Batch [105/157], test Loss: 0.5527\n",
            "Epoch [8], Batch [106/157], test Loss: 0.7032\n",
            "Epoch [8], Batch [107/157], test Loss: 0.6726\n",
            "Epoch [8], Batch [108/157], test Loss: 0.4539\n",
            "Epoch [8], Batch [109/157], test Loss: 0.5726\n",
            "Epoch [8], Batch [110/157], test Loss: 0.5337\n",
            "Epoch [8], Batch [111/157], test Loss: 0.9308\n",
            "Epoch [8], Batch [112/157], test Loss: 0.4719\n",
            "Epoch [8], Batch [113/157], test Loss: 0.6011\n",
            "Epoch [8], Batch [114/157], test Loss: 0.6691\n",
            "Epoch [8], Batch [115/157], test Loss: 0.5613\n",
            "Epoch [8], Batch [116/157], test Loss: 0.6886\n",
            "Epoch [8], Batch [117/157], test Loss: 0.6630\n",
            "Epoch [8], Batch [118/157], test Loss: 0.7604\n",
            "Epoch [8], Batch [119/157], test Loss: 0.4622\n",
            "Epoch [8], Batch [120/157], test Loss: 0.6429\n",
            "Epoch [8], Batch [121/157], test Loss: 0.5569\n",
            "Epoch [8], Batch [122/157], test Loss: 0.6043\n",
            "Epoch [8], Batch [123/157], test Loss: 0.5930\n",
            "Epoch [8], Batch [124/157], test Loss: 0.5849\n",
            "Epoch [8], Batch [125/157], test Loss: 0.5492\n",
            "Epoch [8], Batch [126/157], test Loss: 0.7478\n",
            "Epoch [8], Batch [127/157], test Loss: 0.6962\n",
            "Epoch [8], Batch [128/157], test Loss: 0.3198\n",
            "Epoch [8], Batch [129/157], test Loss: 0.8374\n",
            "Epoch [8], Batch [130/157], test Loss: 0.5843\n",
            "Epoch [8], Batch [131/157], test Loss: 0.6225\n",
            "Epoch [8], Batch [132/157], test Loss: 0.5593\n",
            "Epoch [8], Batch [133/157], test Loss: 0.7656\n",
            "Epoch [8], Batch [134/157], test Loss: 0.6393\n",
            "Epoch [8], Batch [135/157], test Loss: 0.7813\n",
            "Epoch [8], Batch [136/157], test Loss: 0.6159\n",
            "Epoch [8], Batch [137/157], test Loss: 0.6684\n",
            "Epoch [8], Batch [138/157], test Loss: 0.7089\n",
            "Epoch [8], Batch [139/157], test Loss: 0.7407\n",
            "Epoch [8], Batch [140/157], test Loss: 0.5784\n",
            "Epoch [8], Batch [141/157], test Loss: 0.7398\n",
            "Epoch [8], Batch [142/157], test Loss: 0.6543\n",
            "Epoch [8], Batch [143/157], test Loss: 0.5528\n",
            "Epoch [8], Batch [144/157], test Loss: 0.5675\n",
            "Epoch [8], Batch [145/157], test Loss: 0.6050\n",
            "Epoch [8], Batch [146/157], test Loss: 0.5766\n",
            "Epoch [8], Batch [147/157], test Loss: 0.5405\n",
            "Epoch [8], Batch [148/157], test Loss: 0.5461\n",
            "Epoch [8], Batch [149/157], test Loss: 0.6118\n",
            "Epoch [8], Batch [150/157], test Loss: 0.6490\n",
            "Epoch [8], Batch [151/157], test Loss: 0.6986\n",
            "Epoch [8], Batch [152/157], test Loss: 0.5579\n",
            "Epoch [8], Batch [153/157], test Loss: 0.6686\n",
            "Epoch [8], Batch [154/157], test Loss: 0.5660\n",
            "Epoch [8], Batch [155/157], test Loss: 0.5979\n",
            "Epoch [8], Batch [156/157], test Loss: 0.5951\n",
            "Epoch [8], Batch [157/157], test Loss: 0.3460\n",
            "Accuracy of test set: 76.82%\n",
            "Epoch [9/25] - Train Loss: 0.5952, Train Accuracy: 78.36% - Test Loss: 0.6317, Test Accuracy: 76.82%\n",
            "Epoch [9], Batch [10/938], Loss: 0.5654\n",
            "Epoch [9], Batch [20/938], Loss: 0.6019\n",
            "Epoch [9], Batch [30/938], Loss: 0.6801\n",
            "Epoch [9], Batch [40/938], Loss: 0.4569\n",
            "Epoch [9], Batch [50/938], Loss: 0.5291\n",
            "Epoch [9], Batch [60/938], Loss: 0.6762\n",
            "Epoch [9], Batch [70/938], Loss: 0.6731\n",
            "Epoch [9], Batch [80/938], Loss: 0.7960\n",
            "Epoch [9], Batch [90/938], Loss: 0.5818\n",
            "Epoch [9], Batch [100/938], Loss: 0.4584\n",
            "Epoch [9], Batch [110/938], Loss: 0.6092\n",
            "Epoch [9], Batch [120/938], Loss: 0.6585\n",
            "Epoch [9], Batch [130/938], Loss: 0.4538\n",
            "Epoch [9], Batch [140/938], Loss: 0.4788\n",
            "Epoch [9], Batch [150/938], Loss: 0.4551\n",
            "Epoch [9], Batch [160/938], Loss: 0.8814\n",
            "Epoch [9], Batch [170/938], Loss: 0.7701\n",
            "Epoch [9], Batch [180/938], Loss: 0.7514\n",
            "Epoch [9], Batch [190/938], Loss: 0.4541\n",
            "Epoch [9], Batch [200/938], Loss: 0.5476\n",
            "Epoch [9], Batch [210/938], Loss: 0.9062\n",
            "Epoch [9], Batch [220/938], Loss: 0.4295\n",
            "Epoch [9], Batch [230/938], Loss: 0.7266\n",
            "Epoch [9], Batch [240/938], Loss: 0.7222\n",
            "Epoch [9], Batch [250/938], Loss: 0.5798\n",
            "Epoch [9], Batch [260/938], Loss: 0.4900\n",
            "Epoch [9], Batch [270/938], Loss: 0.5987\n",
            "Epoch [9], Batch [280/938], Loss: 0.7669\n",
            "Epoch [9], Batch [290/938], Loss: 0.4400\n",
            "Epoch [9], Batch [300/938], Loss: 0.5412\n",
            "Epoch [9], Batch [310/938], Loss: 0.6673\n",
            "Epoch [9], Batch [320/938], Loss: 0.6631\n",
            "Epoch [9], Batch [330/938], Loss: 0.6806\n",
            "Epoch [9], Batch [340/938], Loss: 0.4884\n",
            "Epoch [9], Batch [350/938], Loss: 0.5160\n",
            "Epoch [9], Batch [360/938], Loss: 0.5309\n",
            "Epoch [9], Batch [370/938], Loss: 0.5232\n",
            "Epoch [9], Batch [380/938], Loss: 0.4295\n",
            "Epoch [9], Batch [390/938], Loss: 0.5676\n",
            "Epoch [9], Batch [400/938], Loss: 0.5207\n",
            "Epoch [9], Batch [410/938], Loss: 0.4965\n",
            "Epoch [9], Batch [420/938], Loss: 0.6052\n",
            "Epoch [9], Batch [430/938], Loss: 0.5902\n",
            "Epoch [9], Batch [440/938], Loss: 0.5389\n",
            "Epoch [9], Batch [450/938], Loss: 0.4436\n",
            "Epoch [9], Batch [460/938], Loss: 0.9074\n",
            "Epoch [9], Batch [470/938], Loss: 0.7821\n",
            "Epoch [9], Batch [480/938], Loss: 0.4840\n",
            "Epoch [9], Batch [490/938], Loss: 0.4764\n",
            "Epoch [9], Batch [500/938], Loss: 0.7795\n",
            "Epoch [9], Batch [510/938], Loss: 0.7741\n",
            "Epoch [9], Batch [520/938], Loss: 0.5685\n",
            "Epoch [9], Batch [530/938], Loss: 0.5873\n",
            "Epoch [9], Batch [540/938], Loss: 0.7238\n",
            "Epoch [9], Batch [550/938], Loss: 0.5314\n",
            "Epoch [9], Batch [560/938], Loss: 0.5513\n",
            "Epoch [9], Batch [570/938], Loss: 0.7251\n",
            "Epoch [9], Batch [580/938], Loss: 0.4710\n",
            "Epoch [9], Batch [590/938], Loss: 0.4326\n",
            "Epoch [9], Batch [600/938], Loss: 0.4906\n",
            "Epoch [9], Batch [610/938], Loss: 0.6676\n",
            "Epoch [9], Batch [620/938], Loss: 0.6103\n",
            "Epoch [9], Batch [630/938], Loss: 0.4933\n",
            "Epoch [9], Batch [640/938], Loss: 0.5247\n",
            "Epoch [9], Batch [650/938], Loss: 0.5463\n",
            "Epoch [9], Batch [660/938], Loss: 0.6142\n",
            "Epoch [9], Batch [670/938], Loss: 0.4370\n",
            "Epoch [9], Batch [680/938], Loss: 0.6714\n",
            "Epoch [9], Batch [690/938], Loss: 0.4146\n",
            "Epoch [9], Batch [700/938], Loss: 0.4753\n",
            "Epoch [9], Batch [710/938], Loss: 0.6126\n",
            "Epoch [9], Batch [720/938], Loss: 0.4561\n",
            "Epoch [9], Batch [730/938], Loss: 0.5699\n",
            "Epoch [9], Batch [740/938], Loss: 0.3779\n",
            "Epoch [9], Batch [750/938], Loss: 0.5646\n",
            "Epoch [9], Batch [760/938], Loss: 0.6410\n",
            "Epoch [9], Batch [770/938], Loss: 0.6223\n",
            "Epoch [9], Batch [780/938], Loss: 0.5289\n",
            "Epoch [9], Batch [790/938], Loss: 0.4222\n",
            "Epoch [9], Batch [800/938], Loss: 0.5802\n",
            "Epoch [9], Batch [810/938], Loss: 0.5364\n",
            "Epoch [9], Batch [820/938], Loss: 0.7181\n",
            "Epoch [9], Batch [830/938], Loss: 0.4328\n",
            "Epoch [9], Batch [840/938], Loss: 0.7197\n",
            "Epoch [9], Batch [850/938], Loss: 0.6014\n",
            "Epoch [9], Batch [860/938], Loss: 0.5186\n",
            "Epoch [9], Batch [870/938], Loss: 0.6203\n",
            "Epoch [9], Batch [880/938], Loss: 0.6496\n",
            "Epoch [9], Batch [890/938], Loss: 0.5500\n",
            "Epoch [9], Batch [900/938], Loss: 0.4212\n",
            "Epoch [9], Batch [910/938], Loss: 0.5315\n",
            "Epoch [9], Batch [920/938], Loss: 0.4750\n",
            "Epoch [9], Batch [930/938], Loss: 0.5810\n",
            "Epoch [9], Batch [938/938], Loss: 0.4584\n",
            "Accuracy of train set: 79.54%\n",
            "Epoch [9], Batch [1/157], test Loss: 0.6361\n",
            "Epoch [9], Batch [2/157], test Loss: 0.3858\n",
            "Epoch [9], Batch [3/157], test Loss: 0.6502\n",
            "Epoch [9], Batch [4/157], test Loss: 0.7797\n",
            "Epoch [9], Batch [5/157], test Loss: 0.5715\n",
            "Epoch [9], Batch [6/157], test Loss: 0.5300\n",
            "Epoch [9], Batch [7/157], test Loss: 0.6933\n",
            "Epoch [9], Batch [8/157], test Loss: 0.5700\n",
            "Epoch [9], Batch [9/157], test Loss: 0.6228\n",
            "Epoch [9], Batch [10/157], test Loss: 0.7632\n",
            "Epoch [9], Batch [11/157], test Loss: 0.6660\n",
            "Epoch [9], Batch [12/157], test Loss: 0.5875\n",
            "Epoch [9], Batch [13/157], test Loss: 0.8333\n",
            "Epoch [9], Batch [14/157], test Loss: 0.4281\n",
            "Epoch [9], Batch [15/157], test Loss: 0.6903\n",
            "Epoch [9], Batch [16/157], test Loss: 0.3930\n",
            "Epoch [9], Batch [17/157], test Loss: 0.6764\n",
            "Epoch [9], Batch [18/157], test Loss: 0.3120\n",
            "Epoch [9], Batch [19/157], test Loss: 0.5517\n",
            "Epoch [9], Batch [20/157], test Loss: 0.8703\n",
            "Epoch [9], Batch [21/157], test Loss: 0.4817\n",
            "Epoch [9], Batch [22/157], test Loss: 0.4185\n",
            "Epoch [9], Batch [23/157], test Loss: 0.7666\n",
            "Epoch [9], Batch [24/157], test Loss: 0.5351\n",
            "Epoch [9], Batch [25/157], test Loss: 0.6607\n",
            "Epoch [9], Batch [26/157], test Loss: 0.8186\n",
            "Epoch [9], Batch [27/157], test Loss: 0.5175\n",
            "Epoch [9], Batch [28/157], test Loss: 0.4018\n",
            "Epoch [9], Batch [29/157], test Loss: 0.6140\n",
            "Epoch [9], Batch [30/157], test Loss: 0.7843\n",
            "Epoch [9], Batch [31/157], test Loss: 0.7056\n",
            "Epoch [9], Batch [32/157], test Loss: 0.6175\n",
            "Epoch [9], Batch [33/157], test Loss: 0.7025\n",
            "Epoch [9], Batch [34/157], test Loss: 0.5238\n",
            "Epoch [9], Batch [35/157], test Loss: 0.6387\n",
            "Epoch [9], Batch [36/157], test Loss: 0.6831\n",
            "Epoch [9], Batch [37/157], test Loss: 0.4278\n",
            "Epoch [9], Batch [38/157], test Loss: 0.5275\n",
            "Epoch [9], Batch [39/157], test Loss: 0.5903\n",
            "Epoch [9], Batch [40/157], test Loss: 0.5302\n",
            "Epoch [9], Batch [41/157], test Loss: 0.6739\n",
            "Epoch [9], Batch [42/157], test Loss: 0.6914\n",
            "Epoch [9], Batch [43/157], test Loss: 0.3879\n",
            "Epoch [9], Batch [44/157], test Loss: 0.6532\n",
            "Epoch [9], Batch [45/157], test Loss: 0.5783\n",
            "Epoch [9], Batch [46/157], test Loss: 0.5915\n",
            "Epoch [9], Batch [47/157], test Loss: 0.4456\n",
            "Epoch [9], Batch [48/157], test Loss: 0.4834\n",
            "Epoch [9], Batch [49/157], test Loss: 0.3949\n",
            "Epoch [9], Batch [50/157], test Loss: 0.6362\n",
            "Epoch [9], Batch [51/157], test Loss: 0.4664\n",
            "Epoch [9], Batch [52/157], test Loss: 0.4815\n",
            "Epoch [9], Batch [53/157], test Loss: 0.6887\n",
            "Epoch [9], Batch [54/157], test Loss: 0.6444\n",
            "Epoch [9], Batch [55/157], test Loss: 0.4066\n",
            "Epoch [9], Batch [56/157], test Loss: 0.4597\n",
            "Epoch [9], Batch [57/157], test Loss: 0.3998\n",
            "Epoch [9], Batch [58/157], test Loss: 0.6449\n",
            "Epoch [9], Batch [59/157], test Loss: 0.5455\n",
            "Epoch [9], Batch [60/157], test Loss: 0.6028\n",
            "Epoch [9], Batch [61/157], test Loss: 0.7315\n",
            "Epoch [9], Batch [62/157], test Loss: 0.4163\n",
            "Epoch [9], Batch [63/157], test Loss: 0.5043\n",
            "Epoch [9], Batch [64/157], test Loss: 0.7441\n",
            "Epoch [9], Batch [65/157], test Loss: 0.6451\n",
            "Epoch [9], Batch [66/157], test Loss: 0.7327\n",
            "Epoch [9], Batch [67/157], test Loss: 0.4058\n",
            "Epoch [9], Batch [68/157], test Loss: 0.5335\n",
            "Epoch [9], Batch [69/157], test Loss: 0.5097\n",
            "Epoch [9], Batch [70/157], test Loss: 0.5594\n",
            "Epoch [9], Batch [71/157], test Loss: 0.6215\n",
            "Epoch [9], Batch [72/157], test Loss: 0.6480\n",
            "Epoch [9], Batch [73/157], test Loss: 0.5275\n",
            "Epoch [9], Batch [74/157], test Loss: 0.4383\n",
            "Epoch [9], Batch [75/157], test Loss: 0.5427\n",
            "Epoch [9], Batch [76/157], test Loss: 0.5129\n",
            "Epoch [9], Batch [77/157], test Loss: 0.4175\n",
            "Epoch [9], Batch [78/157], test Loss: 0.5186\n",
            "Epoch [9], Batch [79/157], test Loss: 0.5716\n",
            "Epoch [9], Batch [80/157], test Loss: 0.6787\n",
            "Epoch [9], Batch [81/157], test Loss: 0.7465\n",
            "Epoch [9], Batch [82/157], test Loss: 0.6429\n",
            "Epoch [9], Batch [83/157], test Loss: 0.3412\n",
            "Epoch [9], Batch [84/157], test Loss: 0.6765\n",
            "Epoch [9], Batch [85/157], test Loss: 0.6411\n",
            "Epoch [9], Batch [86/157], test Loss: 0.5279\n",
            "Epoch [9], Batch [87/157], test Loss: 0.5437\n",
            "Epoch [9], Batch [88/157], test Loss: 0.4219\n",
            "Epoch [9], Batch [89/157], test Loss: 0.5346\n",
            "Epoch [9], Batch [90/157], test Loss: 0.3918\n",
            "Epoch [9], Batch [91/157], test Loss: 0.6065\n",
            "Epoch [9], Batch [92/157], test Loss: 0.6641\n",
            "Epoch [9], Batch [93/157], test Loss: 0.4159\n",
            "Epoch [9], Batch [94/157], test Loss: 0.6175\n",
            "Epoch [9], Batch [95/157], test Loss: 0.5651\n",
            "Epoch [9], Batch [96/157], test Loss: 0.6269\n",
            "Epoch [9], Batch [97/157], test Loss: 0.8431\n",
            "Epoch [9], Batch [98/157], test Loss: 0.6298\n",
            "Epoch [9], Batch [99/157], test Loss: 0.6966\n",
            "Epoch [9], Batch [100/157], test Loss: 0.6241\n",
            "Epoch [9], Batch [101/157], test Loss: 0.6712\n",
            "Epoch [9], Batch [102/157], test Loss: 0.9455\n",
            "Epoch [9], Batch [103/157], test Loss: 0.5608\n",
            "Epoch [9], Batch [104/157], test Loss: 0.6613\n",
            "Epoch [9], Batch [105/157], test Loss: 0.5521\n",
            "Epoch [9], Batch [106/157], test Loss: 0.6862\n",
            "Epoch [9], Batch [107/157], test Loss: 0.4248\n",
            "Epoch [9], Batch [108/157], test Loss: 0.5956\n",
            "Epoch [9], Batch [109/157], test Loss: 0.5339\n",
            "Epoch [9], Batch [110/157], test Loss: 0.4556\n",
            "Epoch [9], Batch [111/157], test Loss: 0.5561\n",
            "Epoch [9], Batch [112/157], test Loss: 0.5837\n",
            "Epoch [9], Batch [113/157], test Loss: 0.6937\n",
            "Epoch [9], Batch [114/157], test Loss: 0.6618\n",
            "Epoch [9], Batch [115/157], test Loss: 0.6479\n",
            "Epoch [9], Batch [116/157], test Loss: 0.6594\n",
            "Epoch [9], Batch [117/157], test Loss: 0.8101\n",
            "Epoch [9], Batch [118/157], test Loss: 0.6462\n",
            "Epoch [9], Batch [119/157], test Loss: 0.7217\n",
            "Epoch [9], Batch [120/157], test Loss: 0.6861\n",
            "Epoch [9], Batch [121/157], test Loss: 0.6514\n",
            "Epoch [9], Batch [122/157], test Loss: 0.5162\n",
            "Epoch [9], Batch [123/157], test Loss: 0.5883\n",
            "Epoch [9], Batch [124/157], test Loss: 0.3705\n",
            "Epoch [9], Batch [125/157], test Loss: 0.6939\n",
            "Epoch [9], Batch [126/157], test Loss: 0.4546\n",
            "Epoch [9], Batch [127/157], test Loss: 0.4622\n",
            "Epoch [9], Batch [128/157], test Loss: 0.9673\n",
            "Epoch [9], Batch [129/157], test Loss: 0.5144\n",
            "Epoch [9], Batch [130/157], test Loss: 0.4966\n",
            "Epoch [9], Batch [131/157], test Loss: 0.5816\n",
            "Epoch [9], Batch [132/157], test Loss: 0.6751\n",
            "Epoch [9], Batch [133/157], test Loss: 0.6340\n",
            "Epoch [9], Batch [134/157], test Loss: 0.5315\n",
            "Epoch [9], Batch [135/157], test Loss: 0.6424\n",
            "Epoch [9], Batch [136/157], test Loss: 0.6616\n",
            "Epoch [9], Batch [137/157], test Loss: 0.9008\n",
            "Epoch [9], Batch [138/157], test Loss: 0.5918\n",
            "Epoch [9], Batch [139/157], test Loss: 0.5227\n",
            "Epoch [9], Batch [140/157], test Loss: 0.7757\n",
            "Epoch [9], Batch [141/157], test Loss: 0.4838\n",
            "Epoch [9], Batch [142/157], test Loss: 0.5793\n",
            "Epoch [9], Batch [143/157], test Loss: 0.4622\n",
            "Epoch [9], Batch [144/157], test Loss: 0.5624\n",
            "Epoch [9], Batch [145/157], test Loss: 0.6147\n",
            "Epoch [9], Batch [146/157], test Loss: 0.4658\n",
            "Epoch [9], Batch [147/157], test Loss: 0.6680\n",
            "Epoch [9], Batch [148/157], test Loss: 0.5589\n",
            "Epoch [9], Batch [149/157], test Loss: 0.6132\n",
            "Epoch [9], Batch [150/157], test Loss: 0.5026\n",
            "Epoch [9], Batch [151/157], test Loss: 0.7900\n",
            "Epoch [9], Batch [152/157], test Loss: 0.4528\n",
            "Epoch [9], Batch [153/157], test Loss: 0.4592\n",
            "Epoch [9], Batch [154/157], test Loss: 0.5477\n",
            "Epoch [9], Batch [155/157], test Loss: 0.4641\n",
            "Epoch [9], Batch [156/157], test Loss: 0.6672\n",
            "Epoch [9], Batch [157/157], test Loss: 0.6610\n",
            "Accuracy of test set: 78.41%\n",
            "Epoch [10/25] - Train Loss: 0.5706, Train Accuracy: 79.54% - Test Loss: 0.5897, Test Accuracy: 78.41%\n",
            "Epoch [10], Batch [10/938], Loss: 0.4965\n",
            "Epoch [10], Batch [20/938], Loss: 0.5485\n",
            "Epoch [10], Batch [30/938], Loss: 0.5607\n",
            "Epoch [10], Batch [40/938], Loss: 0.4622\n",
            "Epoch [10], Batch [50/938], Loss: 0.4700\n",
            "Epoch [10], Batch [60/938], Loss: 0.6677\n",
            "Epoch [10], Batch [70/938], Loss: 0.4990\n",
            "Epoch [10], Batch [80/938], Loss: 0.6612\n",
            "Epoch [10], Batch [90/938], Loss: 0.5481\n",
            "Epoch [10], Batch [100/938], Loss: 0.4834\n",
            "Epoch [10], Batch [110/938], Loss: 0.4717\n",
            "Epoch [10], Batch [120/938], Loss: 0.4255\n",
            "Epoch [10], Batch [130/938], Loss: 0.4891\n",
            "Epoch [10], Batch [140/938], Loss: 0.6509\n",
            "Epoch [10], Batch [150/938], Loss: 0.4759\n",
            "Epoch [10], Batch [160/938], Loss: 0.5722\n",
            "Epoch [10], Batch [170/938], Loss: 0.7848\n",
            "Epoch [10], Batch [180/938], Loss: 0.5189\n",
            "Epoch [10], Batch [190/938], Loss: 0.6386\n",
            "Epoch [10], Batch [200/938], Loss: 0.5034\n",
            "Epoch [10], Batch [210/938], Loss: 0.4739\n",
            "Epoch [10], Batch [220/938], Loss: 0.3478\n",
            "Epoch [10], Batch [230/938], Loss: 0.4850\n",
            "Epoch [10], Batch [240/938], Loss: 0.4171\n",
            "Epoch [10], Batch [250/938], Loss: 0.5484\n",
            "Epoch [10], Batch [260/938], Loss: 0.3909\n",
            "Epoch [10], Batch [270/938], Loss: 0.5174\n",
            "Epoch [10], Batch [280/938], Loss: 0.5284\n",
            "Epoch [10], Batch [290/938], Loss: 0.6849\n",
            "Epoch [10], Batch [300/938], Loss: 0.4683\n",
            "Epoch [10], Batch [310/938], Loss: 0.3821\n",
            "Epoch [10], Batch [320/938], Loss: 0.4469\n",
            "Epoch [10], Batch [330/938], Loss: 0.7998\n",
            "Epoch [10], Batch [340/938], Loss: 0.4994\n",
            "Epoch [10], Batch [350/938], Loss: 0.4672\n",
            "Epoch [10], Batch [360/938], Loss: 0.7124\n",
            "Epoch [10], Batch [370/938], Loss: 0.6213\n",
            "Epoch [10], Batch [380/938], Loss: 0.4575\n",
            "Epoch [10], Batch [390/938], Loss: 0.4605\n",
            "Epoch [10], Batch [400/938], Loss: 0.6828\n",
            "Epoch [10], Batch [410/938], Loss: 0.5282\n",
            "Epoch [10], Batch [420/938], Loss: 0.4261\n",
            "Epoch [10], Batch [430/938], Loss: 0.9611\n",
            "Epoch [10], Batch [440/938], Loss: 0.5006\n",
            "Epoch [10], Batch [450/938], Loss: 0.5424\n",
            "Epoch [10], Batch [460/938], Loss: 0.4799\n",
            "Epoch [10], Batch [470/938], Loss: 0.6223\n",
            "Epoch [10], Batch [480/938], Loss: 0.6250\n",
            "Epoch [10], Batch [490/938], Loss: 0.4031\n",
            "Epoch [10], Batch [500/938], Loss: 0.3750\n",
            "Epoch [10], Batch [510/938], Loss: 0.4156\n",
            "Epoch [10], Batch [520/938], Loss: 0.4551\n",
            "Epoch [10], Batch [530/938], Loss: 0.4441\n",
            "Epoch [10], Batch [540/938], Loss: 0.6336\n",
            "Epoch [10], Batch [550/938], Loss: 0.6413\n",
            "Epoch [10], Batch [560/938], Loss: 0.5676\n",
            "Epoch [10], Batch [570/938], Loss: 0.5023\n",
            "Epoch [10], Batch [580/938], Loss: 0.4815\n",
            "Epoch [10], Batch [590/938], Loss: 0.4949\n",
            "Epoch [10], Batch [600/938], Loss: 0.5451\n",
            "Epoch [10], Batch [610/938], Loss: 0.3665\n",
            "Epoch [10], Batch [620/938], Loss: 0.5256\n",
            "Epoch [10], Batch [630/938], Loss: 0.7383\n",
            "Epoch [10], Batch [640/938], Loss: 0.7024\n",
            "Epoch [10], Batch [650/938], Loss: 0.4388\n",
            "Epoch [10], Batch [660/938], Loss: 0.6432\n",
            "Epoch [10], Batch [670/938], Loss: 0.5763\n",
            "Epoch [10], Batch [680/938], Loss: 0.4918\n",
            "Epoch [10], Batch [690/938], Loss: 0.3745\n",
            "Epoch [10], Batch [700/938], Loss: 0.5651\n",
            "Epoch [10], Batch [710/938], Loss: 0.5138\n",
            "Epoch [10], Batch [720/938], Loss: 0.6883\n",
            "Epoch [10], Batch [730/938], Loss: 0.5606\n",
            "Epoch [10], Batch [740/938], Loss: 0.5246\n",
            "Epoch [10], Batch [750/938], Loss: 0.7317\n",
            "Epoch [10], Batch [760/938], Loss: 0.4220\n",
            "Epoch [10], Batch [770/938], Loss: 0.4520\n",
            "Epoch [10], Batch [780/938], Loss: 0.5075\n",
            "Epoch [10], Batch [790/938], Loss: 0.5983\n",
            "Epoch [10], Batch [800/938], Loss: 0.4514\n",
            "Epoch [10], Batch [810/938], Loss: 0.5455\n",
            "Epoch [10], Batch [820/938], Loss: 0.5097\n",
            "Epoch [10], Batch [830/938], Loss: 0.8698\n",
            "Epoch [10], Batch [840/938], Loss: 0.4925\n",
            "Epoch [10], Batch [850/938], Loss: 0.5278\n",
            "Epoch [10], Batch [860/938], Loss: 0.4991\n",
            "Epoch [10], Batch [870/938], Loss: 0.4567\n",
            "Epoch [10], Batch [880/938], Loss: 0.6689\n",
            "Epoch [10], Batch [890/938], Loss: 0.4641\n",
            "Epoch [10], Batch [900/938], Loss: 0.5320\n",
            "Epoch [10], Batch [910/938], Loss: 0.6801\n",
            "Epoch [10], Batch [920/938], Loss: 0.4395\n",
            "Epoch [10], Batch [930/938], Loss: 0.5782\n",
            "Epoch [10], Batch [938/938], Loss: 0.6044\n",
            "Accuracy of train set: 80.53%\n",
            "Epoch [10], Batch [1/157], test Loss: 0.5470\n",
            "Epoch [10], Batch [2/157], test Loss: 0.8079\n",
            "Epoch [10], Batch [3/157], test Loss: 0.7059\n",
            "Epoch [10], Batch [4/157], test Loss: 0.4528\n",
            "Epoch [10], Batch [5/157], test Loss: 0.6203\n",
            "Epoch [10], Batch [6/157], test Loss: 0.4895\n",
            "Epoch [10], Batch [7/157], test Loss: 0.5955\n",
            "Epoch [10], Batch [8/157], test Loss: 0.5004\n",
            "Epoch [10], Batch [9/157], test Loss: 0.5568\n",
            "Epoch [10], Batch [10/157], test Loss: 0.8226\n",
            "Epoch [10], Batch [11/157], test Loss: 0.7007\n",
            "Epoch [10], Batch [12/157], test Loss: 0.4338\n",
            "Epoch [10], Batch [13/157], test Loss: 0.5454\n",
            "Epoch [10], Batch [14/157], test Loss: 0.4336\n",
            "Epoch [10], Batch [15/157], test Loss: 0.6554\n",
            "Epoch [10], Batch [16/157], test Loss: 0.6205\n",
            "Epoch [10], Batch [17/157], test Loss: 0.6408\n",
            "Epoch [10], Batch [18/157], test Loss: 0.6444\n",
            "Epoch [10], Batch [19/157], test Loss: 0.8279\n",
            "Epoch [10], Batch [20/157], test Loss: 0.5840\n",
            "Epoch [10], Batch [21/157], test Loss: 0.6032\n",
            "Epoch [10], Batch [22/157], test Loss: 0.6400\n",
            "Epoch [10], Batch [23/157], test Loss: 0.4116\n",
            "Epoch [10], Batch [24/157], test Loss: 0.6311\n",
            "Epoch [10], Batch [25/157], test Loss: 0.5319\n",
            "Epoch [10], Batch [26/157], test Loss: 0.6576\n",
            "Epoch [10], Batch [27/157], test Loss: 0.7393\n",
            "Epoch [10], Batch [28/157], test Loss: 0.4006\n",
            "Epoch [10], Batch [29/157], test Loss: 0.5351\n",
            "Epoch [10], Batch [30/157], test Loss: 0.7043\n",
            "Epoch [10], Batch [31/157], test Loss: 0.6947\n",
            "Epoch [10], Batch [32/157], test Loss: 0.5709\n",
            "Epoch [10], Batch [33/157], test Loss: 0.4696\n",
            "Epoch [10], Batch [34/157], test Loss: 0.6648\n",
            "Epoch [10], Batch [35/157], test Loss: 0.4822\n",
            "Epoch [10], Batch [36/157], test Loss: 0.4943\n",
            "Epoch [10], Batch [37/157], test Loss: 0.5486\n",
            "Epoch [10], Batch [38/157], test Loss: 0.4871\n",
            "Epoch [10], Batch [39/157], test Loss: 0.6162\n",
            "Epoch [10], Batch [40/157], test Loss: 0.4824\n",
            "Epoch [10], Batch [41/157], test Loss: 0.6809\n",
            "Epoch [10], Batch [42/157], test Loss: 0.4864\n",
            "Epoch [10], Batch [43/157], test Loss: 0.6122\n",
            "Epoch [10], Batch [44/157], test Loss: 0.6832\n",
            "Epoch [10], Batch [45/157], test Loss: 0.4590\n",
            "Epoch [10], Batch [46/157], test Loss: 0.6950\n",
            "Epoch [10], Batch [47/157], test Loss: 0.5357\n",
            "Epoch [10], Batch [48/157], test Loss: 0.6232\n",
            "Epoch [10], Batch [49/157], test Loss: 0.5288\n",
            "Epoch [10], Batch [50/157], test Loss: 0.8199\n",
            "Epoch [10], Batch [51/157], test Loss: 0.3839\n",
            "Epoch [10], Batch [52/157], test Loss: 0.6137\n",
            "Epoch [10], Batch [53/157], test Loss: 0.5369\n",
            "Epoch [10], Batch [54/157], test Loss: 0.7954\n",
            "Epoch [10], Batch [55/157], test Loss: 0.7574\n",
            "Epoch [10], Batch [56/157], test Loss: 0.5983\n",
            "Epoch [10], Batch [57/157], test Loss: 0.5284\n",
            "Epoch [10], Batch [58/157], test Loss: 0.3998\n",
            "Epoch [10], Batch [59/157], test Loss: 0.5120\n",
            "Epoch [10], Batch [60/157], test Loss: 0.5835\n",
            "Epoch [10], Batch [61/157], test Loss: 0.4590\n",
            "Epoch [10], Batch [62/157], test Loss: 0.5499\n",
            "Epoch [10], Batch [63/157], test Loss: 0.5059\n",
            "Epoch [10], Batch [64/157], test Loss: 0.7753\n",
            "Epoch [10], Batch [65/157], test Loss: 0.5115\n",
            "Epoch [10], Batch [66/157], test Loss: 0.6903\n",
            "Epoch [10], Batch [67/157], test Loss: 0.7171\n",
            "Epoch [10], Batch [68/157], test Loss: 0.6925\n",
            "Epoch [10], Batch [69/157], test Loss: 0.6095\n",
            "Epoch [10], Batch [70/157], test Loss: 0.6326\n",
            "Epoch [10], Batch [71/157], test Loss: 0.4903\n",
            "Epoch [10], Batch [72/157], test Loss: 0.7139\n",
            "Epoch [10], Batch [73/157], test Loss: 0.4278\n",
            "Epoch [10], Batch [74/157], test Loss: 0.6516\n",
            "Epoch [10], Batch [75/157], test Loss: 0.6397\n",
            "Epoch [10], Batch [76/157], test Loss: 0.3458\n",
            "Epoch [10], Batch [77/157], test Loss: 0.6268\n",
            "Epoch [10], Batch [78/157], test Loss: 0.7448\n",
            "Epoch [10], Batch [79/157], test Loss: 0.4774\n",
            "Epoch [10], Batch [80/157], test Loss: 0.6209\n",
            "Epoch [10], Batch [81/157], test Loss: 0.4920\n",
            "Epoch [10], Batch [82/157], test Loss: 0.5416\n",
            "Epoch [10], Batch [83/157], test Loss: 0.5891\n",
            "Epoch [10], Batch [84/157], test Loss: 0.5698\n",
            "Epoch [10], Batch [85/157], test Loss: 0.4612\n",
            "Epoch [10], Batch [86/157], test Loss: 0.5795\n",
            "Epoch [10], Batch [87/157], test Loss: 0.5901\n",
            "Epoch [10], Batch [88/157], test Loss: 0.5215\n",
            "Epoch [10], Batch [89/157], test Loss: 0.4994\n",
            "Epoch [10], Batch [90/157], test Loss: 0.7769\n",
            "Epoch [10], Batch [91/157], test Loss: 0.4464\n",
            "Epoch [10], Batch [92/157], test Loss: 0.6403\n",
            "Epoch [10], Batch [93/157], test Loss: 0.7011\n",
            "Epoch [10], Batch [94/157], test Loss: 0.4541\n",
            "Epoch [10], Batch [95/157], test Loss: 0.6410\n",
            "Epoch [10], Batch [96/157], test Loss: 0.6306\n",
            "Epoch [10], Batch [97/157], test Loss: 0.4065\n",
            "Epoch [10], Batch [98/157], test Loss: 0.4642\n",
            "Epoch [10], Batch [99/157], test Loss: 0.6280\n",
            "Epoch [10], Batch [100/157], test Loss: 0.6249\n",
            "Epoch [10], Batch [101/157], test Loss: 0.4468\n",
            "Epoch [10], Batch [102/157], test Loss: 0.4638\n",
            "Epoch [10], Batch [103/157], test Loss: 0.4834\n",
            "Epoch [10], Batch [104/157], test Loss: 0.4268\n",
            "Epoch [10], Batch [105/157], test Loss: 0.4166\n",
            "Epoch [10], Batch [106/157], test Loss: 0.4469\n",
            "Epoch [10], Batch [107/157], test Loss: 0.7197\n",
            "Epoch [10], Batch [108/157], test Loss: 0.7741\n",
            "Epoch [10], Batch [109/157], test Loss: 0.4514\n",
            "Epoch [10], Batch [110/157], test Loss: 0.4775\n",
            "Epoch [10], Batch [111/157], test Loss: 0.4928\n",
            "Epoch [10], Batch [112/157], test Loss: 0.7480\n",
            "Epoch [10], Batch [113/157], test Loss: 0.5929\n",
            "Epoch [10], Batch [114/157], test Loss: 0.4754\n",
            "Epoch [10], Batch [115/157], test Loss: 0.4464\n",
            "Epoch [10], Batch [116/157], test Loss: 0.5216\n",
            "Epoch [10], Batch [117/157], test Loss: 0.7305\n",
            "Epoch [10], Batch [118/157], test Loss: 0.4481\n",
            "Epoch [10], Batch [119/157], test Loss: 0.5624\n",
            "Epoch [10], Batch [120/157], test Loss: 0.5605\n",
            "Epoch [10], Batch [121/157], test Loss: 0.7018\n",
            "Epoch [10], Batch [122/157], test Loss: 0.5820\n",
            "Epoch [10], Batch [123/157], test Loss: 0.4359\n",
            "Epoch [10], Batch [124/157], test Loss: 0.4389\n",
            "Epoch [10], Batch [125/157], test Loss: 0.4951\n",
            "Epoch [10], Batch [126/157], test Loss: 0.6024\n",
            "Epoch [10], Batch [127/157], test Loss: 0.6339\n",
            "Epoch [10], Batch [128/157], test Loss: 0.5932\n",
            "Epoch [10], Batch [129/157], test Loss: 0.3574\n",
            "Epoch [10], Batch [130/157], test Loss: 0.6491\n",
            "Epoch [10], Batch [131/157], test Loss: 0.5288\n",
            "Epoch [10], Batch [132/157], test Loss: 0.5205\n",
            "Epoch [10], Batch [133/157], test Loss: 0.6896\n",
            "Epoch [10], Batch [134/157], test Loss: 0.5705\n",
            "Epoch [10], Batch [135/157], test Loss: 0.5063\n",
            "Epoch [10], Batch [136/157], test Loss: 0.5669\n",
            "Epoch [10], Batch [137/157], test Loss: 0.6367\n",
            "Epoch [10], Batch [138/157], test Loss: 0.5638\n",
            "Epoch [10], Batch [139/157], test Loss: 0.7485\n",
            "Epoch [10], Batch [140/157], test Loss: 0.4259\n",
            "Epoch [10], Batch [141/157], test Loss: 0.6529\n",
            "Epoch [10], Batch [142/157], test Loss: 0.4473\n",
            "Epoch [10], Batch [143/157], test Loss: 0.4889\n",
            "Epoch [10], Batch [144/157], test Loss: 0.5436\n",
            "Epoch [10], Batch [145/157], test Loss: 0.3301\n",
            "Epoch [10], Batch [146/157], test Loss: 0.5064\n",
            "Epoch [10], Batch [147/157], test Loss: 0.5814\n",
            "Epoch [10], Batch [148/157], test Loss: 0.5135\n",
            "Epoch [10], Batch [149/157], test Loss: 0.6525\n",
            "Epoch [10], Batch [150/157], test Loss: 0.7048\n",
            "Epoch [10], Batch [151/157], test Loss: 0.5590\n",
            "Epoch [10], Batch [152/157], test Loss: 0.4637\n",
            "Epoch [10], Batch [153/157], test Loss: 0.6675\n",
            "Epoch [10], Batch [154/157], test Loss: 0.5691\n",
            "Epoch [10], Batch [155/157], test Loss: 0.4644\n",
            "Epoch [10], Batch [156/157], test Loss: 0.6192\n",
            "Epoch [10], Batch [157/157], test Loss: 0.7649\n",
            "Accuracy of test set: 79.57%\n",
            "Epoch [11/25] - Train Loss: 0.5483, Train Accuracy: 80.53% - Test Loss: 0.5732, Test Accuracy: 79.57%\n",
            "Epoch [11], Batch [10/938], Loss: 0.5312\n",
            "Epoch [11], Batch [20/938], Loss: 0.4782\n",
            "Epoch [11], Batch [30/938], Loss: 0.9265\n",
            "Epoch [11], Batch [40/938], Loss: 0.7182\n",
            "Epoch [11], Batch [50/938], Loss: 0.2888\n",
            "Epoch [11], Batch [60/938], Loss: 0.4972\n",
            "Epoch [11], Batch [70/938], Loss: 0.5399\n",
            "Epoch [11], Batch [80/938], Loss: 0.6247\n",
            "Epoch [11], Batch [90/938], Loss: 0.4414\n",
            "Epoch [11], Batch [100/938], Loss: 0.4816\n",
            "Epoch [11], Batch [110/938], Loss: 0.5523\n",
            "Epoch [11], Batch [120/938], Loss: 0.5305\n",
            "Epoch [11], Batch [130/938], Loss: 0.5044\n",
            "Epoch [11], Batch [140/938], Loss: 0.3890\n",
            "Epoch [11], Batch [150/938], Loss: 0.4449\n",
            "Epoch [11], Batch [160/938], Loss: 0.7451\n",
            "Epoch [11], Batch [170/938], Loss: 0.4137\n",
            "Epoch [11], Batch [180/938], Loss: 0.4997\n",
            "Epoch [11], Batch [190/938], Loss: 0.4491\n",
            "Epoch [11], Batch [200/938], Loss: 0.8242\n",
            "Epoch [11], Batch [210/938], Loss: 0.5750\n",
            "Epoch [11], Batch [220/938], Loss: 0.5592\n",
            "Epoch [11], Batch [230/938], Loss: 0.4957\n",
            "Epoch [11], Batch [240/938], Loss: 0.4835\n",
            "Epoch [11], Batch [250/938], Loss: 0.5046\n",
            "Epoch [11], Batch [260/938], Loss: 0.6011\n",
            "Epoch [11], Batch [270/938], Loss: 0.4867\n",
            "Epoch [11], Batch [280/938], Loss: 0.6027\n",
            "Epoch [11], Batch [290/938], Loss: 0.6079\n",
            "Epoch [11], Batch [300/938], Loss: 0.6702\n",
            "Epoch [11], Batch [310/938], Loss: 0.3640\n",
            "Epoch [11], Batch [320/938], Loss: 0.5085\n",
            "Epoch [11], Batch [330/938], Loss: 0.5300\n",
            "Epoch [11], Batch [340/938], Loss: 0.5239\n",
            "Epoch [11], Batch [350/938], Loss: 0.4314\n",
            "Epoch [11], Batch [360/938], Loss: 0.5273\n",
            "Epoch [11], Batch [370/938], Loss: 0.3958\n",
            "Epoch [11], Batch [380/938], Loss: 0.5191\n",
            "Epoch [11], Batch [390/938], Loss: 0.6201\n",
            "Epoch [11], Batch [400/938], Loss: 0.4030\n",
            "Epoch [11], Batch [410/938], Loss: 0.6859\n",
            "Epoch [11], Batch [420/938], Loss: 0.4561\n",
            "Epoch [11], Batch [430/938], Loss: 0.3759\n",
            "Epoch [11], Batch [440/938], Loss: 0.5770\n",
            "Epoch [11], Batch [450/938], Loss: 0.6010\n",
            "Epoch [11], Batch [460/938], Loss: 0.5500\n",
            "Epoch [11], Batch [470/938], Loss: 0.4553\n",
            "Epoch [11], Batch [480/938], Loss: 0.4655\n",
            "Epoch [11], Batch [490/938], Loss: 0.5210\n",
            "Epoch [11], Batch [500/938], Loss: 0.4975\n",
            "Epoch [11], Batch [510/938], Loss: 0.6828\n",
            "Epoch [11], Batch [520/938], Loss: 0.6872\n",
            "Epoch [11], Batch [530/938], Loss: 0.5793\n",
            "Epoch [11], Batch [540/938], Loss: 0.5410\n",
            "Epoch [11], Batch [550/938], Loss: 0.6427\n",
            "Epoch [11], Batch [560/938], Loss: 0.5945\n",
            "Epoch [11], Batch [570/938], Loss: 0.7115\n",
            "Epoch [11], Batch [580/938], Loss: 0.5150\n",
            "Epoch [11], Batch [590/938], Loss: 0.5617\n",
            "Epoch [11], Batch [600/938], Loss: 0.5063\n",
            "Epoch [11], Batch [610/938], Loss: 0.6163\n",
            "Epoch [11], Batch [620/938], Loss: 0.5053\n",
            "Epoch [11], Batch [630/938], Loss: 0.6893\n",
            "Epoch [11], Batch [640/938], Loss: 0.6183\n",
            "Epoch [11], Batch [650/938], Loss: 0.4336\n",
            "Epoch [11], Batch [660/938], Loss: 0.5231\n",
            "Epoch [11], Batch [670/938], Loss: 0.2119\n",
            "Epoch [11], Batch [680/938], Loss: 0.6773\n",
            "Epoch [11], Batch [690/938], Loss: 0.4652\n",
            "Epoch [11], Batch [700/938], Loss: 0.7279\n",
            "Epoch [11], Batch [710/938], Loss: 0.4880\n",
            "Epoch [11], Batch [720/938], Loss: 0.6104\n",
            "Epoch [11], Batch [730/938], Loss: 0.4769\n",
            "Epoch [11], Batch [740/938], Loss: 0.3642\n",
            "Epoch [11], Batch [750/938], Loss: 0.3985\n",
            "Epoch [11], Batch [760/938], Loss: 0.3849\n",
            "Epoch [11], Batch [770/938], Loss: 0.7181\n",
            "Epoch [11], Batch [780/938], Loss: 0.5082\n",
            "Epoch [11], Batch [790/938], Loss: 0.6187\n",
            "Epoch [11], Batch [800/938], Loss: 0.4061\n",
            "Epoch [11], Batch [810/938], Loss: 0.5402\n",
            "Epoch [11], Batch [820/938], Loss: 0.6193\n",
            "Epoch [11], Batch [830/938], Loss: 0.3527\n",
            "Epoch [11], Batch [840/938], Loss: 0.7186\n",
            "Epoch [11], Batch [850/938], Loss: 0.5293\n",
            "Epoch [11], Batch [860/938], Loss: 0.5514\n",
            "Epoch [11], Batch [870/938], Loss: 0.4687\n",
            "Epoch [11], Batch [880/938], Loss: 0.3644\n",
            "Epoch [11], Batch [890/938], Loss: 0.4515\n",
            "Epoch [11], Batch [900/938], Loss: 0.3388\n",
            "Epoch [11], Batch [910/938], Loss: 0.5920\n",
            "Epoch [11], Batch [920/938], Loss: 0.5141\n",
            "Epoch [11], Batch [930/938], Loss: 0.4413\n",
            "Epoch [11], Batch [938/938], Loss: 0.5031\n",
            "Accuracy of train set: 81.31%\n",
            "Epoch [11], Batch [1/157], test Loss: 0.4485\n",
            "Epoch [11], Batch [2/157], test Loss: 0.4714\n",
            "Epoch [11], Batch [3/157], test Loss: 0.5567\n",
            "Epoch [11], Batch [4/157], test Loss: 0.5000\n",
            "Epoch [11], Batch [5/157], test Loss: 0.4450\n",
            "Epoch [11], Batch [6/157], test Loss: 0.5907\n",
            "Epoch [11], Batch [7/157], test Loss: 0.3474\n",
            "Epoch [11], Batch [8/157], test Loss: 0.9632\n",
            "Epoch [11], Batch [9/157], test Loss: 0.5125\n",
            "Epoch [11], Batch [10/157], test Loss: 0.6470\n",
            "Epoch [11], Batch [11/157], test Loss: 0.3592\n",
            "Epoch [11], Batch [12/157], test Loss: 0.5878\n",
            "Epoch [11], Batch [13/157], test Loss: 0.7015\n",
            "Epoch [11], Batch [14/157], test Loss: 0.5545\n",
            "Epoch [11], Batch [15/157], test Loss: 0.4348\n",
            "Epoch [11], Batch [16/157], test Loss: 0.3361\n",
            "Epoch [11], Batch [17/157], test Loss: 0.4948\n",
            "Epoch [11], Batch [18/157], test Loss: 0.5930\n",
            "Epoch [11], Batch [19/157], test Loss: 0.6386\n",
            "Epoch [11], Batch [20/157], test Loss: 0.5616\n",
            "Epoch [11], Batch [21/157], test Loss: 0.5805\n",
            "Epoch [11], Batch [22/157], test Loss: 0.4710\n",
            "Epoch [11], Batch [23/157], test Loss: 0.5305\n",
            "Epoch [11], Batch [24/157], test Loss: 0.6061\n",
            "Epoch [11], Batch [25/157], test Loss: 0.5779\n",
            "Epoch [11], Batch [26/157], test Loss: 0.7252\n",
            "Epoch [11], Batch [27/157], test Loss: 0.4798\n",
            "Epoch [11], Batch [28/157], test Loss: 0.5816\n",
            "Epoch [11], Batch [29/157], test Loss: 0.3577\n",
            "Epoch [11], Batch [30/157], test Loss: 0.6848\n",
            "Epoch [11], Batch [31/157], test Loss: 0.5816\n",
            "Epoch [11], Batch [32/157], test Loss: 0.5794\n",
            "Epoch [11], Batch [33/157], test Loss: 0.4576\n",
            "Epoch [11], Batch [34/157], test Loss: 0.6421\n",
            "Epoch [11], Batch [35/157], test Loss: 0.7031\n",
            "Epoch [11], Batch [36/157], test Loss: 0.5290\n",
            "Epoch [11], Batch [37/157], test Loss: 0.6862\n",
            "Epoch [11], Batch [38/157], test Loss: 0.4606\n",
            "Epoch [11], Batch [39/157], test Loss: 0.4387\n",
            "Epoch [11], Batch [40/157], test Loss: 0.3606\n",
            "Epoch [11], Batch [41/157], test Loss: 0.6611\n",
            "Epoch [11], Batch [42/157], test Loss: 0.6540\n",
            "Epoch [11], Batch [43/157], test Loss: 0.5489\n",
            "Epoch [11], Batch [44/157], test Loss: 0.4367\n",
            "Epoch [11], Batch [45/157], test Loss: 0.7837\n",
            "Epoch [11], Batch [46/157], test Loss: 0.6491\n",
            "Epoch [11], Batch [47/157], test Loss: 0.6881\n",
            "Epoch [11], Batch [48/157], test Loss: 0.7275\n",
            "Epoch [11], Batch [49/157], test Loss: 0.5104\n",
            "Epoch [11], Batch [50/157], test Loss: 0.5889\n",
            "Epoch [11], Batch [51/157], test Loss: 0.4091\n",
            "Epoch [11], Batch [52/157], test Loss: 0.4502\n",
            "Epoch [11], Batch [53/157], test Loss: 0.5674\n",
            "Epoch [11], Batch [54/157], test Loss: 0.4700\n",
            "Epoch [11], Batch [55/157], test Loss: 0.6030\n",
            "Epoch [11], Batch [56/157], test Loss: 0.5365\n",
            "Epoch [11], Batch [57/157], test Loss: 0.5582\n",
            "Epoch [11], Batch [58/157], test Loss: 0.5815\n",
            "Epoch [11], Batch [59/157], test Loss: 0.6272\n",
            "Epoch [11], Batch [60/157], test Loss: 0.6787\n",
            "Epoch [11], Batch [61/157], test Loss: 0.3738\n",
            "Epoch [11], Batch [62/157], test Loss: 0.4882\n",
            "Epoch [11], Batch [63/157], test Loss: 0.4309\n",
            "Epoch [11], Batch [64/157], test Loss: 0.5135\n",
            "Epoch [11], Batch [65/157], test Loss: 0.6153\n",
            "Epoch [11], Batch [66/157], test Loss: 0.5864\n",
            "Epoch [11], Batch [67/157], test Loss: 0.4313\n",
            "Epoch [11], Batch [68/157], test Loss: 0.4657\n",
            "Epoch [11], Batch [69/157], test Loss: 0.7468\n",
            "Epoch [11], Batch [70/157], test Loss: 0.5999\n",
            "Epoch [11], Batch [71/157], test Loss: 0.5263\n",
            "Epoch [11], Batch [72/157], test Loss: 0.6750\n",
            "Epoch [11], Batch [73/157], test Loss: 0.5374\n",
            "Epoch [11], Batch [74/157], test Loss: 0.5723\n",
            "Epoch [11], Batch [75/157], test Loss: 0.3494\n",
            "Epoch [11], Batch [76/157], test Loss: 0.4961\n",
            "Epoch [11], Batch [77/157], test Loss: 0.5956\n",
            "Epoch [11], Batch [78/157], test Loss: 0.6269\n",
            "Epoch [11], Batch [79/157], test Loss: 0.4733\n",
            "Epoch [11], Batch [80/157], test Loss: 0.5060\n",
            "Epoch [11], Batch [81/157], test Loss: 0.4306\n",
            "Epoch [11], Batch [82/157], test Loss: 0.6924\n",
            "Epoch [11], Batch [83/157], test Loss: 0.4974\n",
            "Epoch [11], Batch [84/157], test Loss: 0.5484\n",
            "Epoch [11], Batch [85/157], test Loss: 0.7816\n",
            "Epoch [11], Batch [86/157], test Loss: 0.5971\n",
            "Epoch [11], Batch [87/157], test Loss: 0.5232\n",
            "Epoch [11], Batch [88/157], test Loss: 0.9000\n",
            "Epoch [11], Batch [89/157], test Loss: 0.3152\n",
            "Epoch [11], Batch [90/157], test Loss: 0.6191\n",
            "Epoch [11], Batch [91/157], test Loss: 0.3756\n",
            "Epoch [11], Batch [92/157], test Loss: 0.5454\n",
            "Epoch [11], Batch [93/157], test Loss: 0.4235\n",
            "Epoch [11], Batch [94/157], test Loss: 0.6070\n",
            "Epoch [11], Batch [95/157], test Loss: 0.5224\n",
            "Epoch [11], Batch [96/157], test Loss: 0.5112\n",
            "Epoch [11], Batch [97/157], test Loss: 0.5474\n",
            "Epoch [11], Batch [98/157], test Loss: 0.7202\n",
            "Epoch [11], Batch [99/157], test Loss: 0.6227\n",
            "Epoch [11], Batch [100/157], test Loss: 0.7403\n",
            "Epoch [11], Batch [101/157], test Loss: 0.7606\n",
            "Epoch [11], Batch [102/157], test Loss: 0.5489\n",
            "Epoch [11], Batch [103/157], test Loss: 0.7165\n",
            "Epoch [11], Batch [104/157], test Loss: 0.4193\n",
            "Epoch [11], Batch [105/157], test Loss: 0.5895\n",
            "Epoch [11], Batch [106/157], test Loss: 0.5802\n",
            "Epoch [11], Batch [107/157], test Loss: 0.5794\n",
            "Epoch [11], Batch [108/157], test Loss: 0.5026\n",
            "Epoch [11], Batch [109/157], test Loss: 0.5821\n",
            "Epoch [11], Batch [110/157], test Loss: 0.8996\n",
            "Epoch [11], Batch [111/157], test Loss: 0.5706\n",
            "Epoch [11], Batch [112/157], test Loss: 0.6302\n",
            "Epoch [11], Batch [113/157], test Loss: 0.4857\n",
            "Epoch [11], Batch [114/157], test Loss: 0.5075\n",
            "Epoch [11], Batch [115/157], test Loss: 0.4310\n",
            "Epoch [11], Batch [116/157], test Loss: 0.4622\n",
            "Epoch [11], Batch [117/157], test Loss: 0.5512\n",
            "Epoch [11], Batch [118/157], test Loss: 0.6762\n",
            "Epoch [11], Batch [119/157], test Loss: 0.5188\n",
            "Epoch [11], Batch [120/157], test Loss: 0.5603\n",
            "Epoch [11], Batch [121/157], test Loss: 0.3699\n",
            "Epoch [11], Batch [122/157], test Loss: 0.6459\n",
            "Epoch [11], Batch [123/157], test Loss: 0.3740\n",
            "Epoch [11], Batch [124/157], test Loss: 0.4993\n",
            "Epoch [11], Batch [125/157], test Loss: 0.6988\n",
            "Epoch [11], Batch [126/157], test Loss: 0.6138\n",
            "Epoch [11], Batch [127/157], test Loss: 0.4838\n",
            "Epoch [11], Batch [128/157], test Loss: 0.5337\n",
            "Epoch [11], Batch [129/157], test Loss: 0.6596\n",
            "Epoch [11], Batch [130/157], test Loss: 0.4102\n",
            "Epoch [11], Batch [131/157], test Loss: 0.4681\n",
            "Epoch [11], Batch [132/157], test Loss: 0.6083\n",
            "Epoch [11], Batch [133/157], test Loss: 0.5756\n",
            "Epoch [11], Batch [134/157], test Loss: 0.5355\n",
            "Epoch [11], Batch [135/157], test Loss: 0.6274\n",
            "Epoch [11], Batch [136/157], test Loss: 0.4791\n",
            "Epoch [11], Batch [137/157], test Loss: 0.6389\n",
            "Epoch [11], Batch [138/157], test Loss: 0.4355\n",
            "Epoch [11], Batch [139/157], test Loss: 0.4778\n",
            "Epoch [11], Batch [140/157], test Loss: 0.5344\n",
            "Epoch [11], Batch [141/157], test Loss: 0.5181\n",
            "Epoch [11], Batch [142/157], test Loss: 0.4793\n",
            "Epoch [11], Batch [143/157], test Loss: 0.5197\n",
            "Epoch [11], Batch [144/157], test Loss: 0.5617\n",
            "Epoch [11], Batch [145/157], test Loss: 0.7218\n",
            "Epoch [11], Batch [146/157], test Loss: 0.7780\n",
            "Epoch [11], Batch [147/157], test Loss: 0.8798\n",
            "Epoch [11], Batch [148/157], test Loss: 0.7724\n",
            "Epoch [11], Batch [149/157], test Loss: 0.6623\n",
            "Epoch [11], Batch [150/157], test Loss: 0.6262\n",
            "Epoch [11], Batch [151/157], test Loss: 0.6091\n",
            "Epoch [11], Batch [152/157], test Loss: 0.3127\n",
            "Epoch [11], Batch [153/157], test Loss: 0.3905\n",
            "Epoch [11], Batch [154/157], test Loss: 0.4293\n",
            "Epoch [11], Batch [155/157], test Loss: 0.5855\n",
            "Epoch [11], Batch [156/157], test Loss: 0.3422\n",
            "Epoch [11], Batch [157/157], test Loss: 1.1515\n",
            "Accuracy of test set: 80.09%\n",
            "Epoch [12/25] - Train Loss: 0.5294, Train Accuracy: 81.31% - Test Loss: 0.5619, Test Accuracy: 80.09%\n",
            "Epoch [12], Batch [10/938], Loss: 0.5653\n",
            "Epoch [12], Batch [20/938], Loss: 0.3297\n",
            "Epoch [12], Batch [30/938], Loss: 0.4400\n",
            "Epoch [12], Batch [40/938], Loss: 0.7855\n",
            "Epoch [12], Batch [50/938], Loss: 0.6293\n",
            "Epoch [12], Batch [60/938], Loss: 0.4507\n",
            "Epoch [12], Batch [70/938], Loss: 0.3615\n",
            "Epoch [12], Batch [80/938], Loss: 0.4671\n",
            "Epoch [12], Batch [90/938], Loss: 0.6313\n",
            "Epoch [12], Batch [100/938], Loss: 0.3916\n",
            "Epoch [12], Batch [110/938], Loss: 0.4015\n",
            "Epoch [12], Batch [120/938], Loss: 0.3611\n",
            "Epoch [12], Batch [130/938], Loss: 0.4727\n",
            "Epoch [12], Batch [140/938], Loss: 0.5932\n",
            "Epoch [12], Batch [150/938], Loss: 0.5964\n",
            "Epoch [12], Batch [160/938], Loss: 0.4218\n",
            "Epoch [12], Batch [170/938], Loss: 0.4170\n",
            "Epoch [12], Batch [180/938], Loss: 0.5379\n",
            "Epoch [12], Batch [190/938], Loss: 0.4102\n",
            "Epoch [12], Batch [200/938], Loss: 0.6103\n",
            "Epoch [12], Batch [210/938], Loss: 0.4720\n",
            "Epoch [12], Batch [220/938], Loss: 0.4602\n",
            "Epoch [12], Batch [230/938], Loss: 0.5692\n",
            "Epoch [12], Batch [240/938], Loss: 0.3101\n",
            "Epoch [12], Batch [250/938], Loss: 0.8875\n",
            "Epoch [12], Batch [260/938], Loss: 0.4327\n",
            "Epoch [12], Batch [270/938], Loss: 0.3639\n",
            "Epoch [12], Batch [280/938], Loss: 0.4778\n",
            "Epoch [12], Batch [290/938], Loss: 0.7030\n",
            "Epoch [12], Batch [300/938], Loss: 0.7780\n",
            "Epoch [12], Batch [310/938], Loss: 0.2874\n",
            "Epoch [12], Batch [320/938], Loss: 0.5071\n",
            "Epoch [12], Batch [330/938], Loss: 0.3850\n",
            "Epoch [12], Batch [340/938], Loss: 0.5009\n",
            "Epoch [12], Batch [350/938], Loss: 0.6329\n",
            "Epoch [12], Batch [360/938], Loss: 0.6546\n",
            "Epoch [12], Batch [370/938], Loss: 0.4010\n",
            "Epoch [12], Batch [380/938], Loss: 0.3223\n",
            "Epoch [12], Batch [390/938], Loss: 0.4381\n",
            "Epoch [12], Batch [400/938], Loss: 0.5284\n",
            "Epoch [12], Batch [410/938], Loss: 0.5031\n",
            "Epoch [12], Batch [420/938], Loss: 0.9725\n",
            "Epoch [12], Batch [430/938], Loss: 0.4561\n",
            "Epoch [12], Batch [440/938], Loss: 0.4841\n",
            "Epoch [12], Batch [450/938], Loss: 0.3487\n",
            "Epoch [12], Batch [460/938], Loss: 0.4175\n",
            "Epoch [12], Batch [470/938], Loss: 0.4107\n",
            "Epoch [12], Batch [480/938], Loss: 0.3332\n",
            "Epoch [12], Batch [490/938], Loss: 0.4881\n",
            "Epoch [12], Batch [500/938], Loss: 0.5261\n",
            "Epoch [12], Batch [510/938], Loss: 0.6067\n",
            "Epoch [12], Batch [520/938], Loss: 0.3918\n",
            "Epoch [12], Batch [530/938], Loss: 0.3637\n",
            "Epoch [12], Batch [540/938], Loss: 0.4906\n",
            "Epoch [12], Batch [550/938], Loss: 0.4689\n",
            "Epoch [12], Batch [560/938], Loss: 0.4153\n",
            "Epoch [12], Batch [570/938], Loss: 0.3632\n",
            "Epoch [12], Batch [580/938], Loss: 0.3296\n",
            "Epoch [12], Batch [590/938], Loss: 0.5604\n",
            "Epoch [12], Batch [600/938], Loss: 0.5342\n",
            "Epoch [12], Batch [610/938], Loss: 0.4457\n",
            "Epoch [12], Batch [620/938], Loss: 0.3694\n",
            "Epoch [12], Batch [630/938], Loss: 0.6481\n",
            "Epoch [12], Batch [640/938], Loss: 0.6690\n",
            "Epoch [12], Batch [650/938], Loss: 0.4580\n",
            "Epoch [12], Batch [660/938], Loss: 0.4795\n",
            "Epoch [12], Batch [670/938], Loss: 0.4605\n",
            "Epoch [12], Batch [680/938], Loss: 0.4292\n",
            "Epoch [12], Batch [690/938], Loss: 0.3226\n",
            "Epoch [12], Batch [700/938], Loss: 0.4397\n",
            "Epoch [12], Batch [710/938], Loss: 0.5321\n",
            "Epoch [12], Batch [720/938], Loss: 0.7130\n",
            "Epoch [12], Batch [730/938], Loss: 0.4069\n",
            "Epoch [12], Batch [740/938], Loss: 0.4783\n",
            "Epoch [12], Batch [750/938], Loss: 0.5938\n",
            "Epoch [12], Batch [760/938], Loss: 0.5401\n",
            "Epoch [12], Batch [770/938], Loss: 0.5245\n",
            "Epoch [12], Batch [780/938], Loss: 0.5649\n",
            "Epoch [12], Batch [790/938], Loss: 0.5047\n",
            "Epoch [12], Batch [800/938], Loss: 0.7489\n",
            "Epoch [12], Batch [810/938], Loss: 0.4522\n",
            "Epoch [12], Batch [820/938], Loss: 0.6972\n",
            "Epoch [12], Batch [830/938], Loss: 0.6137\n",
            "Epoch [12], Batch [840/938], Loss: 0.7441\n",
            "Epoch [12], Batch [850/938], Loss: 0.6296\n",
            "Epoch [12], Batch [860/938], Loss: 0.5267\n",
            "Epoch [12], Batch [870/938], Loss: 0.4586\n",
            "Epoch [12], Batch [880/938], Loss: 0.3438\n",
            "Epoch [12], Batch [890/938], Loss: 0.3677\n",
            "Epoch [12], Batch [900/938], Loss: 0.4272\n",
            "Epoch [12], Batch [910/938], Loss: 0.5978\n",
            "Epoch [12], Batch [920/938], Loss: 0.5303\n",
            "Epoch [12], Batch [930/938], Loss: 0.4230\n",
            "Epoch [12], Batch [938/938], Loss: 0.3697\n",
            "Accuracy of train set: 81.98%\n",
            "Epoch [12], Batch [1/157], test Loss: 0.5362\n",
            "Epoch [12], Batch [2/157], test Loss: 0.6315\n",
            "Epoch [12], Batch [3/157], test Loss: 0.5332\n",
            "Epoch [12], Batch [4/157], test Loss: 0.5947\n",
            "Epoch [12], Batch [5/157], test Loss: 0.4829\n",
            "Epoch [12], Batch [6/157], test Loss: 0.4891\n",
            "Epoch [12], Batch [7/157], test Loss: 0.4835\n",
            "Epoch [12], Batch [8/157], test Loss: 0.4040\n",
            "Epoch [12], Batch [9/157], test Loss: 0.4802\n",
            "Epoch [12], Batch [10/157], test Loss: 0.7026\n",
            "Epoch [12], Batch [11/157], test Loss: 0.4923\n",
            "Epoch [12], Batch [12/157], test Loss: 0.4635\n",
            "Epoch [12], Batch [13/157], test Loss: 0.8206\n",
            "Epoch [12], Batch [14/157], test Loss: 0.5550\n",
            "Epoch [12], Batch [15/157], test Loss: 0.4293\n",
            "Epoch [12], Batch [16/157], test Loss: 0.4523\n",
            "Epoch [12], Batch [17/157], test Loss: 0.7082\n",
            "Epoch [12], Batch [18/157], test Loss: 0.6413\n",
            "Epoch [12], Batch [19/157], test Loss: 0.6717\n",
            "Epoch [12], Batch [20/157], test Loss: 0.7067\n",
            "Epoch [12], Batch [21/157], test Loss: 0.5196\n",
            "Epoch [12], Batch [22/157], test Loss: 0.6610\n",
            "Epoch [12], Batch [23/157], test Loss: 0.4822\n",
            "Epoch [12], Batch [24/157], test Loss: 0.4231\n",
            "Epoch [12], Batch [25/157], test Loss: 0.3681\n",
            "Epoch [12], Batch [26/157], test Loss: 0.4531\n",
            "Epoch [12], Batch [27/157], test Loss: 0.5391\n",
            "Epoch [12], Batch [28/157], test Loss: 0.4749\n",
            "Epoch [12], Batch [29/157], test Loss: 0.4423\n",
            "Epoch [12], Batch [30/157], test Loss: 0.3787\n",
            "Epoch [12], Batch [31/157], test Loss: 0.6514\n",
            "Epoch [12], Batch [32/157], test Loss: 0.4766\n",
            "Epoch [12], Batch [33/157], test Loss: 0.6020\n",
            "Epoch [12], Batch [34/157], test Loss: 0.4116\n",
            "Epoch [12], Batch [35/157], test Loss: 0.4314\n",
            "Epoch [12], Batch [36/157], test Loss: 0.4878\n",
            "Epoch [12], Batch [37/157], test Loss: 0.5653\n",
            "Epoch [12], Batch [38/157], test Loss: 0.5488\n",
            "Epoch [12], Batch [39/157], test Loss: 0.5867\n",
            "Epoch [12], Batch [40/157], test Loss: 0.7516\n",
            "Epoch [12], Batch [41/157], test Loss: 0.5498\n",
            "Epoch [12], Batch [42/157], test Loss: 0.6166\n",
            "Epoch [12], Batch [43/157], test Loss: 0.6323\n",
            "Epoch [12], Batch [44/157], test Loss: 0.9185\n",
            "Epoch [12], Batch [45/157], test Loss: 0.5724\n",
            "Epoch [12], Batch [46/157], test Loss: 0.4416\n",
            "Epoch [12], Batch [47/157], test Loss: 0.5628\n",
            "Epoch [12], Batch [48/157], test Loss: 0.5764\n",
            "Epoch [12], Batch [49/157], test Loss: 0.4344\n",
            "Epoch [12], Batch [50/157], test Loss: 0.6447\n",
            "Epoch [12], Batch [51/157], test Loss: 0.3359\n",
            "Epoch [12], Batch [52/157], test Loss: 0.4257\n",
            "Epoch [12], Batch [53/157], test Loss: 0.7020\n",
            "Epoch [12], Batch [54/157], test Loss: 0.4658\n",
            "Epoch [12], Batch [55/157], test Loss: 0.4502\n",
            "Epoch [12], Batch [56/157], test Loss: 0.6859\n",
            "Epoch [12], Batch [57/157], test Loss: 0.6592\n",
            "Epoch [12], Batch [58/157], test Loss: 0.5636\n",
            "Epoch [12], Batch [59/157], test Loss: 0.3813\n",
            "Epoch [12], Batch [60/157], test Loss: 0.6476\n",
            "Epoch [12], Batch [61/157], test Loss: 0.5370\n",
            "Epoch [12], Batch [62/157], test Loss: 0.4654\n",
            "Epoch [12], Batch [63/157], test Loss: 0.5857\n",
            "Epoch [12], Batch [64/157], test Loss: 0.6607\n",
            "Epoch [12], Batch [65/157], test Loss: 0.4756\n",
            "Epoch [12], Batch [66/157], test Loss: 0.5791\n",
            "Epoch [12], Batch [67/157], test Loss: 0.6156\n",
            "Epoch [12], Batch [68/157], test Loss: 0.8568\n",
            "Epoch [12], Batch [69/157], test Loss: 0.6477\n",
            "Epoch [12], Batch [70/157], test Loss: 0.4512\n",
            "Epoch [12], Batch [71/157], test Loss: 0.5207\n",
            "Epoch [12], Batch [72/157], test Loss: 0.5249\n",
            "Epoch [12], Batch [73/157], test Loss: 0.6807\n",
            "Epoch [12], Batch [74/157], test Loss: 0.4622\n",
            "Epoch [12], Batch [75/157], test Loss: 0.7743\n",
            "Epoch [12], Batch [76/157], test Loss: 0.6023\n",
            "Epoch [12], Batch [77/157], test Loss: 0.5876\n",
            "Epoch [12], Batch [78/157], test Loss: 0.8257\n",
            "Epoch [12], Batch [79/157], test Loss: 0.4639\n",
            "Epoch [12], Batch [80/157], test Loss: 0.6325\n",
            "Epoch [12], Batch [81/157], test Loss: 0.4242\n",
            "Epoch [12], Batch [82/157], test Loss: 0.5775\n",
            "Epoch [12], Batch [83/157], test Loss: 0.6082\n",
            "Epoch [12], Batch [84/157], test Loss: 0.6154\n",
            "Epoch [12], Batch [85/157], test Loss: 0.5014\n",
            "Epoch [12], Batch [86/157], test Loss: 0.4996\n",
            "Epoch [12], Batch [87/157], test Loss: 0.4906\n",
            "Epoch [12], Batch [88/157], test Loss: 0.5833\n",
            "Epoch [12], Batch [89/157], test Loss: 0.5946\n",
            "Epoch [12], Batch [90/157], test Loss: 0.6066\n",
            "Epoch [12], Batch [91/157], test Loss: 0.4007\n",
            "Epoch [12], Batch [92/157], test Loss: 0.3884\n",
            "Epoch [12], Batch [93/157], test Loss: 0.7785\n",
            "Epoch [12], Batch [94/157], test Loss: 0.5006\n",
            "Epoch [12], Batch [95/157], test Loss: 0.5859\n",
            "Epoch [12], Batch [96/157], test Loss: 0.4928\n",
            "Epoch [12], Batch [97/157], test Loss: 0.6307\n",
            "Epoch [12], Batch [98/157], test Loss: 0.6465\n",
            "Epoch [12], Batch [99/157], test Loss: 0.6044\n",
            "Epoch [12], Batch [100/157], test Loss: 0.4428\n",
            "Epoch [12], Batch [101/157], test Loss: 0.5533\n",
            "Epoch [12], Batch [102/157], test Loss: 0.6080\n",
            "Epoch [12], Batch [103/157], test Loss: 0.5750\n",
            "Epoch [12], Batch [104/157], test Loss: 0.5069\n",
            "Epoch [12], Batch [105/157], test Loss: 0.3993\n",
            "Epoch [12], Batch [106/157], test Loss: 0.5128\n",
            "Epoch [12], Batch [107/157], test Loss: 0.5652\n",
            "Epoch [12], Batch [108/157], test Loss: 0.4921\n",
            "Epoch [12], Batch [109/157], test Loss: 0.7783\n",
            "Epoch [12], Batch [110/157], test Loss: 0.2824\n",
            "Epoch [12], Batch [111/157], test Loss: 0.4848\n",
            "Epoch [12], Batch [112/157], test Loss: 0.4789\n",
            "Epoch [12], Batch [113/157], test Loss: 0.4428\n",
            "Epoch [12], Batch [114/157], test Loss: 0.4567\n",
            "Epoch [12], Batch [115/157], test Loss: 0.5082\n",
            "Epoch [12], Batch [116/157], test Loss: 0.5387\n",
            "Epoch [12], Batch [117/157], test Loss: 0.4394\n",
            "Epoch [12], Batch [118/157], test Loss: 0.5647\n",
            "Epoch [12], Batch [119/157], test Loss: 0.3596\n",
            "Epoch [12], Batch [120/157], test Loss: 0.5173\n",
            "Epoch [12], Batch [121/157], test Loss: 0.6211\n",
            "Epoch [12], Batch [122/157], test Loss: 0.7511\n",
            "Epoch [12], Batch [123/157], test Loss: 0.6209\n",
            "Epoch [12], Batch [124/157], test Loss: 0.5404\n",
            "Epoch [12], Batch [125/157], test Loss: 0.2984\n",
            "Epoch [12], Batch [126/157], test Loss: 0.4388\n",
            "Epoch [12], Batch [127/157], test Loss: 0.5009\n",
            "Epoch [12], Batch [128/157], test Loss: 0.4481\n",
            "Epoch [12], Batch [129/157], test Loss: 0.5546\n",
            "Epoch [12], Batch [130/157], test Loss: 0.4649\n",
            "Epoch [12], Batch [131/157], test Loss: 0.7000\n",
            "Epoch [12], Batch [132/157], test Loss: 0.5024\n",
            "Epoch [12], Batch [133/157], test Loss: 0.5814\n",
            "Epoch [12], Batch [134/157], test Loss: 0.4788\n",
            "Epoch [12], Batch [135/157], test Loss: 0.3827\n",
            "Epoch [12], Batch [136/157], test Loss: 0.5070\n",
            "Epoch [12], Batch [137/157], test Loss: 0.6848\n",
            "Epoch [12], Batch [138/157], test Loss: 0.2727\n",
            "Epoch [12], Batch [139/157], test Loss: 0.3107\n",
            "Epoch [12], Batch [140/157], test Loss: 0.8043\n",
            "Epoch [12], Batch [141/157], test Loss: 0.4373\n",
            "Epoch [12], Batch [142/157], test Loss: 0.5117\n",
            "Epoch [12], Batch [143/157], test Loss: 0.3629\n",
            "Epoch [12], Batch [144/157], test Loss: 0.4194\n",
            "Epoch [12], Batch [145/157], test Loss: 0.4382\n",
            "Epoch [12], Batch [146/157], test Loss: 0.6728\n",
            "Epoch [12], Batch [147/157], test Loss: 0.4714\n",
            "Epoch [12], Batch [148/157], test Loss: 0.4332\n",
            "Epoch [12], Batch [149/157], test Loss: 0.4374\n",
            "Epoch [12], Batch [150/157], test Loss: 0.5734\n",
            "Epoch [12], Batch [151/157], test Loss: 0.3541\n",
            "Epoch [12], Batch [152/157], test Loss: 0.5290\n",
            "Epoch [12], Batch [153/157], test Loss: 0.5260\n",
            "Epoch [12], Batch [154/157], test Loss: 0.6157\n",
            "Epoch [12], Batch [155/157], test Loss: 0.5731\n",
            "Epoch [12], Batch [156/157], test Loss: 0.6140\n",
            "Epoch [12], Batch [157/157], test Loss: 0.4670\n",
            "Accuracy of test set: 80.91%\n",
            "Epoch [13/25] - Train Loss: 0.5119, Train Accuracy: 81.98% - Test Loss: 0.5394, Test Accuracy: 80.91%\n",
            "Epoch [13], Batch [10/938], Loss: 0.5067\n",
            "Epoch [13], Batch [20/938], Loss: 0.5260\n",
            "Epoch [13], Batch [30/938], Loss: 0.4273\n",
            "Epoch [13], Batch [40/938], Loss: 0.3409\n",
            "Epoch [13], Batch [50/938], Loss: 0.4091\n",
            "Epoch [13], Batch [60/938], Loss: 0.4268\n",
            "Epoch [13], Batch [70/938], Loss: 0.4445\n",
            "Epoch [13], Batch [80/938], Loss: 0.4216\n",
            "Epoch [13], Batch [90/938], Loss: 0.4937\n",
            "Epoch [13], Batch [100/938], Loss: 0.5765\n",
            "Epoch [13], Batch [110/938], Loss: 0.3966\n",
            "Epoch [13], Batch [120/938], Loss: 0.3968\n",
            "Epoch [13], Batch [130/938], Loss: 0.5916\n",
            "Epoch [13], Batch [140/938], Loss: 0.5490\n",
            "Epoch [13], Batch [150/938], Loss: 0.3399\n",
            "Epoch [13], Batch [160/938], Loss: 0.5908\n",
            "Epoch [13], Batch [170/938], Loss: 0.4282\n",
            "Epoch [13], Batch [180/938], Loss: 0.6113\n",
            "Epoch [13], Batch [190/938], Loss: 0.5350\n",
            "Epoch [13], Batch [200/938], Loss: 0.5236\n",
            "Epoch [13], Batch [210/938], Loss: 0.4258\n",
            "Epoch [13], Batch [220/938], Loss: 0.4277\n",
            "Epoch [13], Batch [230/938], Loss: 0.4862\n",
            "Epoch [13], Batch [240/938], Loss: 0.5276\n",
            "Epoch [13], Batch [250/938], Loss: 0.4314\n",
            "Epoch [13], Batch [260/938], Loss: 0.4413\n",
            "Epoch [13], Batch [270/938], Loss: 0.7003\n",
            "Epoch [13], Batch [280/938], Loss: 0.4794\n",
            "Epoch [13], Batch [290/938], Loss: 0.4721\n",
            "Epoch [13], Batch [300/938], Loss: 0.6124\n",
            "Epoch [13], Batch [310/938], Loss: 0.5817\n",
            "Epoch [13], Batch [320/938], Loss: 0.3162\n",
            "Epoch [13], Batch [330/938], Loss: 0.3256\n",
            "Epoch [13], Batch [340/938], Loss: 0.2795\n",
            "Epoch [13], Batch [350/938], Loss: 0.3665\n",
            "Epoch [13], Batch [360/938], Loss: 0.3769\n",
            "Epoch [13], Batch [370/938], Loss: 0.5364\n",
            "Epoch [13], Batch [380/938], Loss: 0.4172\n",
            "Epoch [13], Batch [390/938], Loss: 0.4311\n",
            "Epoch [13], Batch [400/938], Loss: 0.7331\n",
            "Epoch [13], Batch [410/938], Loss: 0.4812\n",
            "Epoch [13], Batch [420/938], Loss: 0.4054\n",
            "Epoch [13], Batch [430/938], Loss: 0.5126\n",
            "Epoch [13], Batch [440/938], Loss: 0.5580\n",
            "Epoch [13], Batch [450/938], Loss: 0.4448\n",
            "Epoch [13], Batch [460/938], Loss: 0.4326\n",
            "Epoch [13], Batch [470/938], Loss: 0.3452\n",
            "Epoch [13], Batch [480/938], Loss: 0.3796\n",
            "Epoch [13], Batch [490/938], Loss: 0.5713\n",
            "Epoch [13], Batch [500/938], Loss: 0.6419\n",
            "Epoch [13], Batch [510/938], Loss: 0.3549\n",
            "Epoch [13], Batch [520/938], Loss: 0.5459\n",
            "Epoch [13], Batch [530/938], Loss: 0.4267\n",
            "Epoch [13], Batch [540/938], Loss: 0.2634\n",
            "Epoch [13], Batch [550/938], Loss: 0.4126\n",
            "Epoch [13], Batch [560/938], Loss: 0.6646\n",
            "Epoch [13], Batch [570/938], Loss: 0.5784\n",
            "Epoch [13], Batch [580/938], Loss: 0.4774\n",
            "Epoch [13], Batch [590/938], Loss: 0.4051\n",
            "Epoch [13], Batch [600/938], Loss: 0.3831\n",
            "Epoch [13], Batch [610/938], Loss: 0.3955\n",
            "Epoch [13], Batch [620/938], Loss: 0.3840\n",
            "Epoch [13], Batch [630/938], Loss: 0.4828\n",
            "Epoch [13], Batch [640/938], Loss: 0.6507\n",
            "Epoch [13], Batch [650/938], Loss: 0.4166\n",
            "Epoch [13], Batch [660/938], Loss: 0.3942\n",
            "Epoch [13], Batch [670/938], Loss: 0.3937\n",
            "Epoch [13], Batch [680/938], Loss: 0.5217\n",
            "Epoch [13], Batch [690/938], Loss: 0.4816\n",
            "Epoch [13], Batch [700/938], Loss: 0.5142\n",
            "Epoch [13], Batch [710/938], Loss: 0.3121\n",
            "Epoch [13], Batch [720/938], Loss: 0.6738\n",
            "Epoch [13], Batch [730/938], Loss: 0.3992\n",
            "Epoch [13], Batch [740/938], Loss: 0.4229\n",
            "Epoch [13], Batch [750/938], Loss: 0.6138\n",
            "Epoch [13], Batch [760/938], Loss: 0.5720\n",
            "Epoch [13], Batch [770/938], Loss: 0.4273\n",
            "Epoch [13], Batch [780/938], Loss: 0.9093\n",
            "Epoch [13], Batch [790/938], Loss: 0.5513\n",
            "Epoch [13], Batch [800/938], Loss: 0.5217\n",
            "Epoch [13], Batch [810/938], Loss: 0.6517\n",
            "Epoch [13], Batch [820/938], Loss: 0.4771\n",
            "Epoch [13], Batch [830/938], Loss: 0.5537\n",
            "Epoch [13], Batch [840/938], Loss: 0.3187\n",
            "Epoch [13], Batch [850/938], Loss: 0.4614\n",
            "Epoch [13], Batch [860/938], Loss: 0.4507\n",
            "Epoch [13], Batch [870/938], Loss: 0.5137\n",
            "Epoch [13], Batch [880/938], Loss: 0.5727\n",
            "Epoch [13], Batch [890/938], Loss: 0.6218\n",
            "Epoch [13], Batch [900/938], Loss: 0.6115\n",
            "Epoch [13], Batch [910/938], Loss: 0.5478\n",
            "Epoch [13], Batch [920/938], Loss: 0.3908\n",
            "Epoch [13], Batch [930/938], Loss: 0.4440\n",
            "Epoch [13], Batch [938/938], Loss: 0.5524\n",
            "Accuracy of train set: 82.48%\n",
            "Epoch [13], Batch [1/157], test Loss: 0.3953\n",
            "Epoch [13], Batch [2/157], test Loss: 0.6590\n",
            "Epoch [13], Batch [3/157], test Loss: 0.5485\n",
            "Epoch [13], Batch [4/157], test Loss: 0.3187\n",
            "Epoch [13], Batch [5/157], test Loss: 0.4415\n",
            "Epoch [13], Batch [6/157], test Loss: 0.4640\n",
            "Epoch [13], Batch [7/157], test Loss: 0.5523\n",
            "Epoch [13], Batch [8/157], test Loss: 0.5430\n",
            "Epoch [13], Batch [9/157], test Loss: 0.4589\n",
            "Epoch [13], Batch [10/157], test Loss: 0.6384\n",
            "Epoch [13], Batch [11/157], test Loss: 0.4138\n",
            "Epoch [13], Batch [12/157], test Loss: 0.3761\n",
            "Epoch [13], Batch [13/157], test Loss: 0.5661\n",
            "Epoch [13], Batch [14/157], test Loss: 0.5501\n",
            "Epoch [13], Batch [15/157], test Loss: 0.6700\n",
            "Epoch [13], Batch [16/157], test Loss: 0.5559\n",
            "Epoch [13], Batch [17/157], test Loss: 0.4618\n",
            "Epoch [13], Batch [18/157], test Loss: 0.5673\n",
            "Epoch [13], Batch [19/157], test Loss: 0.4647\n",
            "Epoch [13], Batch [20/157], test Loss: 0.4668\n",
            "Epoch [13], Batch [21/157], test Loss: 0.4315\n",
            "Epoch [13], Batch [22/157], test Loss: 0.3676\n",
            "Epoch [13], Batch [23/157], test Loss: 0.6324\n",
            "Epoch [13], Batch [24/157], test Loss: 0.4847\n",
            "Epoch [13], Batch [25/157], test Loss: 0.5650\n",
            "Epoch [13], Batch [26/157], test Loss: 0.6214\n",
            "Epoch [13], Batch [27/157], test Loss: 0.5236\n",
            "Epoch [13], Batch [28/157], test Loss: 0.8293\n",
            "Epoch [13], Batch [29/157], test Loss: 0.5750\n",
            "Epoch [13], Batch [30/157], test Loss: 0.6360\n",
            "Epoch [13], Batch [31/157], test Loss: 0.6515\n",
            "Epoch [13], Batch [32/157], test Loss: 0.4912\n",
            "Epoch [13], Batch [33/157], test Loss: 0.5361\n",
            "Epoch [13], Batch [34/157], test Loss: 0.5244\n",
            "Epoch [13], Batch [35/157], test Loss: 0.3700\n",
            "Epoch [13], Batch [36/157], test Loss: 0.5495\n",
            "Epoch [13], Batch [37/157], test Loss: 0.5885\n",
            "Epoch [13], Batch [38/157], test Loss: 0.4233\n",
            "Epoch [13], Batch [39/157], test Loss: 0.7857\n",
            "Epoch [13], Batch [40/157], test Loss: 0.4400\n",
            "Epoch [13], Batch [41/157], test Loss: 0.5195\n",
            "Epoch [13], Batch [42/157], test Loss: 0.3850\n",
            "Epoch [13], Batch [43/157], test Loss: 0.4272\n",
            "Epoch [13], Batch [44/157], test Loss: 0.4820\n",
            "Epoch [13], Batch [45/157], test Loss: 0.5972\n",
            "Epoch [13], Batch [46/157], test Loss: 0.4948\n",
            "Epoch [13], Batch [47/157], test Loss: 0.7055\n",
            "Epoch [13], Batch [48/157], test Loss: 0.5995\n",
            "Epoch [13], Batch [49/157], test Loss: 0.6972\n",
            "Epoch [13], Batch [50/157], test Loss: 0.5478\n",
            "Epoch [13], Batch [51/157], test Loss: 0.4827\n",
            "Epoch [13], Batch [52/157], test Loss: 0.4074\n",
            "Epoch [13], Batch [53/157], test Loss: 0.5535\n",
            "Epoch [13], Batch [54/157], test Loss: 0.4250\n",
            "Epoch [13], Batch [55/157], test Loss: 0.6132\n",
            "Epoch [13], Batch [56/157], test Loss: 0.5368\n",
            "Epoch [13], Batch [57/157], test Loss: 0.6875\n",
            "Epoch [13], Batch [58/157], test Loss: 0.5114\n",
            "Epoch [13], Batch [59/157], test Loss: 0.5050\n",
            "Epoch [13], Batch [60/157], test Loss: 0.4489\n",
            "Epoch [13], Batch [61/157], test Loss: 0.4495\n",
            "Epoch [13], Batch [62/157], test Loss: 0.3605\n",
            "Epoch [13], Batch [63/157], test Loss: 0.4202\n",
            "Epoch [13], Batch [64/157], test Loss: 0.5031\n",
            "Epoch [13], Batch [65/157], test Loss: 0.5187\n",
            "Epoch [13], Batch [66/157], test Loss: 0.5580\n",
            "Epoch [13], Batch [67/157], test Loss: 0.4260\n",
            "Epoch [13], Batch [68/157], test Loss: 0.5330\n",
            "Epoch [13], Batch [69/157], test Loss: 0.6621\n",
            "Epoch [13], Batch [70/157], test Loss: 0.5161\n",
            "Epoch [13], Batch [71/157], test Loss: 0.5630\n",
            "Epoch [13], Batch [72/157], test Loss: 0.3870\n",
            "Epoch [13], Batch [73/157], test Loss: 0.4595\n",
            "Epoch [13], Batch [74/157], test Loss: 0.6149\n",
            "Epoch [13], Batch [75/157], test Loss: 0.4290\n",
            "Epoch [13], Batch [76/157], test Loss: 0.5118\n",
            "Epoch [13], Batch [77/157], test Loss: 0.4694\n",
            "Epoch [13], Batch [78/157], test Loss: 0.5724\n",
            "Epoch [13], Batch [79/157], test Loss: 0.6686\n",
            "Epoch [13], Batch [80/157], test Loss: 0.4876\n",
            "Epoch [13], Batch [81/157], test Loss: 0.5685\n",
            "Epoch [13], Batch [82/157], test Loss: 0.3594\n",
            "Epoch [13], Batch [83/157], test Loss: 0.3803\n",
            "Epoch [13], Batch [84/157], test Loss: 0.5188\n",
            "Epoch [13], Batch [85/157], test Loss: 0.5373\n",
            "Epoch [13], Batch [86/157], test Loss: 0.3973\n",
            "Epoch [13], Batch [87/157], test Loss: 0.2537\n",
            "Epoch [13], Batch [88/157], test Loss: 0.4198\n",
            "Epoch [13], Batch [89/157], test Loss: 0.4705\n",
            "Epoch [13], Batch [90/157], test Loss: 0.4263\n",
            "Epoch [13], Batch [91/157], test Loss: 0.6668\n",
            "Epoch [13], Batch [92/157], test Loss: 0.5449\n",
            "Epoch [13], Batch [93/157], test Loss: 0.4095\n",
            "Epoch [13], Batch [94/157], test Loss: 0.3626\n",
            "Epoch [13], Batch [95/157], test Loss: 0.6180\n",
            "Epoch [13], Batch [96/157], test Loss: 0.5608\n",
            "Epoch [13], Batch [97/157], test Loss: 0.3869\n",
            "Epoch [13], Batch [98/157], test Loss: 0.5872\n",
            "Epoch [13], Batch [99/157], test Loss: 0.4370\n",
            "Epoch [13], Batch [100/157], test Loss: 0.5157\n",
            "Epoch [13], Batch [101/157], test Loss: 0.6758\n",
            "Epoch [13], Batch [102/157], test Loss: 0.7303\n",
            "Epoch [13], Batch [103/157], test Loss: 0.6319\n",
            "Epoch [13], Batch [104/157], test Loss: 0.3581\n",
            "Epoch [13], Batch [105/157], test Loss: 0.5141\n",
            "Epoch [13], Batch [106/157], test Loss: 0.4304\n",
            "Epoch [13], Batch [107/157], test Loss: 0.4875\n",
            "Epoch [13], Batch [108/157], test Loss: 0.4598\n",
            "Epoch [13], Batch [109/157], test Loss: 0.3146\n",
            "Epoch [13], Batch [110/157], test Loss: 0.5528\n",
            "Epoch [13], Batch [111/157], test Loss: 0.4765\n",
            "Epoch [13], Batch [112/157], test Loss: 0.5122\n",
            "Epoch [13], Batch [113/157], test Loss: 0.4433\n",
            "Epoch [13], Batch [114/157], test Loss: 0.6431\n",
            "Epoch [13], Batch [115/157], test Loss: 0.4743\n",
            "Epoch [13], Batch [116/157], test Loss: 0.3828\n",
            "Epoch [13], Batch [117/157], test Loss: 0.5399\n",
            "Epoch [13], Batch [118/157], test Loss: 0.3591\n",
            "Epoch [13], Batch [119/157], test Loss: 0.5757\n",
            "Epoch [13], Batch [120/157], test Loss: 0.6685\n",
            "Epoch [13], Batch [121/157], test Loss: 0.3704\n",
            "Epoch [13], Batch [122/157], test Loss: 0.3438\n",
            "Epoch [13], Batch [123/157], test Loss: 0.3188\n",
            "Epoch [13], Batch [124/157], test Loss: 0.6226\n",
            "Epoch [13], Batch [125/157], test Loss: 0.6707\n",
            "Epoch [13], Batch [126/157], test Loss: 0.5785\n",
            "Epoch [13], Batch [127/157], test Loss: 0.5438\n",
            "Epoch [13], Batch [128/157], test Loss: 0.5857\n",
            "Epoch [13], Batch [129/157], test Loss: 0.4707\n",
            "Epoch [13], Batch [130/157], test Loss: 0.5343\n",
            "Epoch [13], Batch [131/157], test Loss: 0.5439\n",
            "Epoch [13], Batch [132/157], test Loss: 0.6555\n",
            "Epoch [13], Batch [133/157], test Loss: 0.5806\n",
            "Epoch [13], Batch [134/157], test Loss: 0.3345\n",
            "Epoch [13], Batch [135/157], test Loss: 0.5694\n",
            "Epoch [13], Batch [136/157], test Loss: 0.4830\n",
            "Epoch [13], Batch [137/157], test Loss: 0.5053\n",
            "Epoch [13], Batch [138/157], test Loss: 0.4157\n",
            "Epoch [13], Batch [139/157], test Loss: 0.3681\n",
            "Epoch [13], Batch [140/157], test Loss: 0.4927\n",
            "Epoch [13], Batch [141/157], test Loss: 0.4902\n",
            "Epoch [13], Batch [142/157], test Loss: 0.3673\n",
            "Epoch [13], Batch [143/157], test Loss: 0.5151\n",
            "Epoch [13], Batch [144/157], test Loss: 0.5099\n",
            "Epoch [13], Batch [145/157], test Loss: 0.4251\n",
            "Epoch [13], Batch [146/157], test Loss: 0.4258\n",
            "Epoch [13], Batch [147/157], test Loss: 0.7016\n",
            "Epoch [13], Batch [148/157], test Loss: 0.4495\n",
            "Epoch [13], Batch [149/157], test Loss: 0.5528\n",
            "Epoch [13], Batch [150/157], test Loss: 0.7041\n",
            "Epoch [13], Batch [151/157], test Loss: 0.6788\n",
            "Epoch [13], Batch [152/157], test Loss: 0.5621\n",
            "Epoch [13], Batch [153/157], test Loss: 0.3698\n",
            "Epoch [13], Batch [154/157], test Loss: 0.5829\n",
            "Epoch [13], Batch [155/157], test Loss: 0.4099\n",
            "Epoch [13], Batch [156/157], test Loss: 0.5477\n",
            "Epoch [13], Batch [157/157], test Loss: 1.0875\n",
            "Accuracy of test set: 82.06%\n",
            "Epoch [14/25] - Train Loss: 0.4978, Train Accuracy: 82.48% - Test Loss: 0.5153, Test Accuracy: 82.06%\n",
            "Epoch [14], Batch [10/938], Loss: 0.5393\n",
            "Epoch [14], Batch [20/938], Loss: 0.4346\n",
            "Epoch [14], Batch [30/938], Loss: 0.3462\n",
            "Epoch [14], Batch [40/938], Loss: 0.6098\n",
            "Epoch [14], Batch [50/938], Loss: 0.5277\n",
            "Epoch [14], Batch [60/938], Loss: 0.4945\n",
            "Epoch [14], Batch [70/938], Loss: 0.4428\n",
            "Epoch [14], Batch [80/938], Loss: 0.5904\n",
            "Epoch [14], Batch [90/938], Loss: 0.4461\n",
            "Epoch [14], Batch [100/938], Loss: 0.5087\n",
            "Epoch [14], Batch [110/938], Loss: 0.4377\n",
            "Epoch [14], Batch [120/938], Loss: 0.4073\n",
            "Epoch [14], Batch [130/938], Loss: 0.5769\n",
            "Epoch [14], Batch [140/938], Loss: 0.5624\n",
            "Epoch [14], Batch [150/938], Loss: 0.2991\n",
            "Epoch [14], Batch [160/938], Loss: 0.6020\n",
            "Epoch [14], Batch [170/938], Loss: 0.2408\n",
            "Epoch [14], Batch [180/938], Loss: 0.4178\n",
            "Epoch [14], Batch [190/938], Loss: 0.3204\n",
            "Epoch [14], Batch [200/938], Loss: 0.3376\n",
            "Epoch [14], Batch [210/938], Loss: 0.3596\n",
            "Epoch [14], Batch [220/938], Loss: 0.3769\n",
            "Epoch [14], Batch [230/938], Loss: 0.5757\n",
            "Epoch [14], Batch [240/938], Loss: 0.3681\n",
            "Epoch [14], Batch [250/938], Loss: 0.3835\n",
            "Epoch [14], Batch [260/938], Loss: 0.4426\n",
            "Epoch [14], Batch [270/938], Loss: 0.4046\n",
            "Epoch [14], Batch [280/938], Loss: 0.6266\n",
            "Epoch [14], Batch [290/938], Loss: 0.5054\n",
            "Epoch [14], Batch [300/938], Loss: 0.4679\n",
            "Epoch [14], Batch [310/938], Loss: 0.6558\n",
            "Epoch [14], Batch [320/938], Loss: 0.6213\n",
            "Epoch [14], Batch [330/938], Loss: 0.4778\n",
            "Epoch [14], Batch [340/938], Loss: 0.3792\n",
            "Epoch [14], Batch [350/938], Loss: 0.4302\n",
            "Epoch [14], Batch [360/938], Loss: 0.3445\n",
            "Epoch [14], Batch [370/938], Loss: 0.7409\n",
            "Epoch [14], Batch [380/938], Loss: 0.3817\n",
            "Epoch [14], Batch [390/938], Loss: 0.3163\n",
            "Epoch [14], Batch [400/938], Loss: 0.5299\n",
            "Epoch [14], Batch [410/938], Loss: 0.6449\n",
            "Epoch [14], Batch [420/938], Loss: 0.5768\n",
            "Epoch [14], Batch [430/938], Loss: 0.3750\n",
            "Epoch [14], Batch [440/938], Loss: 0.4781\n",
            "Epoch [14], Batch [450/938], Loss: 0.5182\n",
            "Epoch [14], Batch [460/938], Loss: 0.4314\n",
            "Epoch [14], Batch [470/938], Loss: 0.5743\n",
            "Epoch [14], Batch [480/938], Loss: 1.0950\n",
            "Epoch [14], Batch [490/938], Loss: 0.5811\n",
            "Epoch [14], Batch [500/938], Loss: 0.3622\n",
            "Epoch [14], Batch [510/938], Loss: 0.4682\n",
            "Epoch [14], Batch [520/938], Loss: 0.5331\n",
            "Epoch [14], Batch [530/938], Loss: 0.6296\n",
            "Epoch [14], Batch [540/938], Loss: 0.4816\n",
            "Epoch [14], Batch [550/938], Loss: 0.2930\n",
            "Epoch [14], Batch [560/938], Loss: 0.6584\n",
            "Epoch [14], Batch [570/938], Loss: 0.4973\n",
            "Epoch [14], Batch [580/938], Loss: 0.5716\n",
            "Epoch [14], Batch [590/938], Loss: 0.2932\n",
            "Epoch [14], Batch [600/938], Loss: 0.3994\n",
            "Epoch [14], Batch [610/938], Loss: 0.4555\n",
            "Epoch [14], Batch [620/938], Loss: 0.6384\n",
            "Epoch [14], Batch [630/938], Loss: 0.4384\n",
            "Epoch [14], Batch [640/938], Loss: 0.4483\n",
            "Epoch [14], Batch [650/938], Loss: 0.5536\n",
            "Epoch [14], Batch [660/938], Loss: 0.6121\n",
            "Epoch [14], Batch [670/938], Loss: 0.2676\n",
            "Epoch [14], Batch [680/938], Loss: 0.4307\n",
            "Epoch [14], Batch [690/938], Loss: 0.6767\n",
            "Epoch [14], Batch [700/938], Loss: 0.3426\n",
            "Epoch [14], Batch [710/938], Loss: 0.4789\n",
            "Epoch [14], Batch [720/938], Loss: 0.4753\n",
            "Epoch [14], Batch [730/938], Loss: 0.3040\n",
            "Epoch [14], Batch [740/938], Loss: 0.4664\n",
            "Epoch [14], Batch [750/938], Loss: 0.3238\n",
            "Epoch [14], Batch [760/938], Loss: 0.4016\n",
            "Epoch [14], Batch [770/938], Loss: 0.4844\n",
            "Epoch [14], Batch [780/938], Loss: 0.5276\n",
            "Epoch [14], Batch [790/938], Loss: 0.6743\n",
            "Epoch [14], Batch [800/938], Loss: 0.4397\n",
            "Epoch [14], Batch [810/938], Loss: 0.6458\n",
            "Epoch [14], Batch [820/938], Loss: 0.6801\n",
            "Epoch [14], Batch [830/938], Loss: 0.4507\n",
            "Epoch [14], Batch [840/938], Loss: 0.6069\n",
            "Epoch [14], Batch [850/938], Loss: 0.4264\n",
            "Epoch [14], Batch [860/938], Loss: 0.3541\n",
            "Epoch [14], Batch [870/938], Loss: 0.4467\n",
            "Epoch [14], Batch [880/938], Loss: 0.3915\n",
            "Epoch [14], Batch [890/938], Loss: 0.3826\n",
            "Epoch [14], Batch [900/938], Loss: 0.3950\n",
            "Epoch [14], Batch [910/938], Loss: 0.6469\n",
            "Epoch [14], Batch [920/938], Loss: 0.3373\n",
            "Epoch [14], Batch [930/938], Loss: 0.3185\n",
            "Epoch [14], Batch [938/938], Loss: 0.3443\n",
            "Accuracy of train set: 82.97%\n",
            "Epoch [14], Batch [1/157], test Loss: 0.4834\n",
            "Epoch [14], Batch [2/157], test Loss: 0.4698\n",
            "Epoch [14], Batch [3/157], test Loss: 0.5366\n",
            "Epoch [14], Batch [4/157], test Loss: 0.4740\n",
            "Epoch [14], Batch [5/157], test Loss: 0.3738\n",
            "Epoch [14], Batch [6/157], test Loss: 0.6287\n",
            "Epoch [14], Batch [7/157], test Loss: 0.4800\n",
            "Epoch [14], Batch [8/157], test Loss: 0.4850\n",
            "Epoch [14], Batch [9/157], test Loss: 0.5608\n",
            "Epoch [14], Batch [10/157], test Loss: 0.5136\n",
            "Epoch [14], Batch [11/157], test Loss: 0.4526\n",
            "Epoch [14], Batch [12/157], test Loss: 0.3426\n",
            "Epoch [14], Batch [13/157], test Loss: 0.5030\n",
            "Epoch [14], Batch [14/157], test Loss: 0.5830\n",
            "Epoch [14], Batch [15/157], test Loss: 0.6527\n",
            "Epoch [14], Batch [16/157], test Loss: 0.5082\n",
            "Epoch [14], Batch [17/157], test Loss: 0.3856\n",
            "Epoch [14], Batch [18/157], test Loss: 0.3999\n",
            "Epoch [14], Batch [19/157], test Loss: 0.5336\n",
            "Epoch [14], Batch [20/157], test Loss: 0.3461\n",
            "Epoch [14], Batch [21/157], test Loss: 0.3761\n",
            "Epoch [14], Batch [22/157], test Loss: 0.4680\n",
            "Epoch [14], Batch [23/157], test Loss: 0.6143\n",
            "Epoch [14], Batch [24/157], test Loss: 0.5577\n",
            "Epoch [14], Batch [25/157], test Loss: 0.4900\n",
            "Epoch [14], Batch [26/157], test Loss: 0.5327\n",
            "Epoch [14], Batch [27/157], test Loss: 0.5367\n",
            "Epoch [14], Batch [28/157], test Loss: 0.6515\n",
            "Epoch [14], Batch [29/157], test Loss: 0.4044\n",
            "Epoch [14], Batch [30/157], test Loss: 0.6002\n",
            "Epoch [14], Batch [31/157], test Loss: 0.4764\n",
            "Epoch [14], Batch [32/157], test Loss: 0.4766\n",
            "Epoch [14], Batch [33/157], test Loss: 0.6041\n",
            "Epoch [14], Batch [34/157], test Loss: 0.5511\n",
            "Epoch [14], Batch [35/157], test Loss: 0.3659\n",
            "Epoch [14], Batch [36/157], test Loss: 0.6739\n",
            "Epoch [14], Batch [37/157], test Loss: 0.4377\n",
            "Epoch [14], Batch [38/157], test Loss: 0.4177\n",
            "Epoch [14], Batch [39/157], test Loss: 0.2577\n",
            "Epoch [14], Batch [40/157], test Loss: 0.4608\n",
            "Epoch [14], Batch [41/157], test Loss: 0.8412\n",
            "Epoch [14], Batch [42/157], test Loss: 0.4346\n",
            "Epoch [14], Batch [43/157], test Loss: 0.5962\n",
            "Epoch [14], Batch [44/157], test Loss: 0.4714\n",
            "Epoch [14], Batch [45/157], test Loss: 0.5099\n",
            "Epoch [14], Batch [46/157], test Loss: 0.4917\n",
            "Epoch [14], Batch [47/157], test Loss: 0.5137\n",
            "Epoch [14], Batch [48/157], test Loss: 0.4345\n",
            "Epoch [14], Batch [49/157], test Loss: 0.7097\n",
            "Epoch [14], Batch [50/157], test Loss: 0.5060\n",
            "Epoch [14], Batch [51/157], test Loss: 0.5104\n",
            "Epoch [14], Batch [52/157], test Loss: 0.5848\n",
            "Epoch [14], Batch [53/157], test Loss: 0.4205\n",
            "Epoch [14], Batch [54/157], test Loss: 0.5000\n",
            "Epoch [14], Batch [55/157], test Loss: 0.4958\n",
            "Epoch [14], Batch [56/157], test Loss: 0.5345\n",
            "Epoch [14], Batch [57/157], test Loss: 0.4918\n",
            "Epoch [14], Batch [58/157], test Loss: 0.4369\n",
            "Epoch [14], Batch [59/157], test Loss: 0.8041\n",
            "Epoch [14], Batch [60/157], test Loss: 0.5035\n",
            "Epoch [14], Batch [61/157], test Loss: 0.3970\n",
            "Epoch [14], Batch [62/157], test Loss: 0.5678\n",
            "Epoch [14], Batch [63/157], test Loss: 0.5133\n",
            "Epoch [14], Batch [64/157], test Loss: 0.6172\n",
            "Epoch [14], Batch [65/157], test Loss: 0.3584\n",
            "Epoch [14], Batch [66/157], test Loss: 0.4013\n",
            "Epoch [14], Batch [67/157], test Loss: 0.5097\n",
            "Epoch [14], Batch [68/157], test Loss: 0.5149\n",
            "Epoch [14], Batch [69/157], test Loss: 0.6675\n",
            "Epoch [14], Batch [70/157], test Loss: 0.5575\n",
            "Epoch [14], Batch [71/157], test Loss: 0.5636\n",
            "Epoch [14], Batch [72/157], test Loss: 0.3039\n",
            "Epoch [14], Batch [73/157], test Loss: 0.6224\n",
            "Epoch [14], Batch [74/157], test Loss: 0.5590\n",
            "Epoch [14], Batch [75/157], test Loss: 0.7530\n",
            "Epoch [14], Batch [76/157], test Loss: 0.6082\n",
            "Epoch [14], Batch [77/157], test Loss: 0.5811\n",
            "Epoch [14], Batch [78/157], test Loss: 0.4228\n",
            "Epoch [14], Batch [79/157], test Loss: 0.4695\n",
            "Epoch [14], Batch [80/157], test Loss: 0.5323\n",
            "Epoch [14], Batch [81/157], test Loss: 0.3758\n",
            "Epoch [14], Batch [82/157], test Loss: 0.3372\n",
            "Epoch [14], Batch [83/157], test Loss: 0.5086\n",
            "Epoch [14], Batch [84/157], test Loss: 0.4806\n",
            "Epoch [14], Batch [85/157], test Loss: 0.6001\n",
            "Epoch [14], Batch [86/157], test Loss: 0.5864\n",
            "Epoch [14], Batch [87/157], test Loss: 0.5618\n",
            "Epoch [14], Batch [88/157], test Loss: 0.5567\n",
            "Epoch [14], Batch [89/157], test Loss: 0.6740\n",
            "Epoch [14], Batch [90/157], test Loss: 0.4408\n",
            "Epoch [14], Batch [91/157], test Loss: 0.6927\n",
            "Epoch [14], Batch [92/157], test Loss: 0.5778\n",
            "Epoch [14], Batch [93/157], test Loss: 0.6976\n",
            "Epoch [14], Batch [94/157], test Loss: 0.6333\n",
            "Epoch [14], Batch [95/157], test Loss: 0.4713\n",
            "Epoch [14], Batch [96/157], test Loss: 0.4950\n",
            "Epoch [14], Batch [97/157], test Loss: 0.3303\n",
            "Epoch [14], Batch [98/157], test Loss: 0.5739\n",
            "Epoch [14], Batch [99/157], test Loss: 0.4047\n",
            "Epoch [14], Batch [100/157], test Loss: 0.3522\n",
            "Epoch [14], Batch [101/157], test Loss: 0.7269\n",
            "Epoch [14], Batch [102/157], test Loss: 0.5303\n",
            "Epoch [14], Batch [103/157], test Loss: 0.3943\n",
            "Epoch [14], Batch [104/157], test Loss: 0.5651\n",
            "Epoch [14], Batch [105/157], test Loss: 0.6244\n",
            "Epoch [14], Batch [106/157], test Loss: 0.4752\n",
            "Epoch [14], Batch [107/157], test Loss: 0.6899\n",
            "Epoch [14], Batch [108/157], test Loss: 0.2905\n",
            "Epoch [14], Batch [109/157], test Loss: 0.6709\n",
            "Epoch [14], Batch [110/157], test Loss: 0.5000\n",
            "Epoch [14], Batch [111/157], test Loss: 0.5632\n",
            "Epoch [14], Batch [112/157], test Loss: 0.4508\n",
            "Epoch [14], Batch [113/157], test Loss: 0.4320\n",
            "Epoch [14], Batch [114/157], test Loss: 0.5931\n",
            "Epoch [14], Batch [115/157], test Loss: 0.5489\n",
            "Epoch [14], Batch [116/157], test Loss: 0.6344\n",
            "Epoch [14], Batch [117/157], test Loss: 0.3764\n",
            "Epoch [14], Batch [118/157], test Loss: 0.4286\n",
            "Epoch [14], Batch [119/157], test Loss: 0.5193\n",
            "Epoch [14], Batch [120/157], test Loss: 0.4533\n",
            "Epoch [14], Batch [121/157], test Loss: 0.5575\n",
            "Epoch [14], Batch [122/157], test Loss: 0.8388\n",
            "Epoch [14], Batch [123/157], test Loss: 0.5075\n",
            "Epoch [14], Batch [124/157], test Loss: 0.6924\n",
            "Epoch [14], Batch [125/157], test Loss: 0.4645\n",
            "Epoch [14], Batch [126/157], test Loss: 0.2914\n",
            "Epoch [14], Batch [127/157], test Loss: 0.5983\n",
            "Epoch [14], Batch [128/157], test Loss: 0.5189\n",
            "Epoch [14], Batch [129/157], test Loss: 0.5455\n",
            "Epoch [14], Batch [130/157], test Loss: 0.3550\n",
            "Epoch [14], Batch [131/157], test Loss: 0.2623\n",
            "Epoch [14], Batch [132/157], test Loss: 0.5906\n",
            "Epoch [14], Batch [133/157], test Loss: 0.3479\n",
            "Epoch [14], Batch [134/157], test Loss: 0.4964\n",
            "Epoch [14], Batch [135/157], test Loss: 0.6826\n",
            "Epoch [14], Batch [136/157], test Loss: 0.5051\n",
            "Epoch [14], Batch [137/157], test Loss: 0.7034\n",
            "Epoch [14], Batch [138/157], test Loss: 0.4128\n",
            "Epoch [14], Batch [139/157], test Loss: 0.4143\n",
            "Epoch [14], Batch [140/157], test Loss: 0.6346\n",
            "Epoch [14], Batch [141/157], test Loss: 0.5035\n",
            "Epoch [14], Batch [142/157], test Loss: 0.5769\n",
            "Epoch [14], Batch [143/157], test Loss: 0.6721\n",
            "Epoch [14], Batch [144/157], test Loss: 0.4125\n",
            "Epoch [14], Batch [145/157], test Loss: 0.4199\n",
            "Epoch [14], Batch [146/157], test Loss: 0.4983\n",
            "Epoch [14], Batch [147/157], test Loss: 0.5856\n",
            "Epoch [14], Batch [148/157], test Loss: 0.5034\n",
            "Epoch [14], Batch [149/157], test Loss: 0.4337\n",
            "Epoch [14], Batch [150/157], test Loss: 0.3618\n",
            "Epoch [14], Batch [151/157], test Loss: 0.4157\n",
            "Epoch [14], Batch [152/157], test Loss: 0.6294\n",
            "Epoch [14], Batch [153/157], test Loss: 0.4089\n",
            "Epoch [14], Batch [154/157], test Loss: 0.5261\n",
            "Epoch [14], Batch [155/157], test Loss: 0.4090\n",
            "Epoch [14], Batch [156/157], test Loss: 0.6263\n",
            "Epoch [14], Batch [157/157], test Loss: 0.2653\n",
            "Accuracy of test set: 81.93%\n",
            "Epoch [15/25] - Train Loss: 0.4847, Train Accuracy: 82.97% - Test Loss: 0.5125, Test Accuracy: 81.93%\n",
            "Epoch [15], Batch [10/938], Loss: 0.3560\n",
            "Epoch [15], Batch [20/938], Loss: 0.4588\n",
            "Epoch [15], Batch [30/938], Loss: 0.3514\n",
            "Epoch [15], Batch [40/938], Loss: 0.6533\n",
            "Epoch [15], Batch [50/938], Loss: 0.4667\n",
            "Epoch [15], Batch [60/938], Loss: 0.5584\n",
            "Epoch [15], Batch [70/938], Loss: 0.9409\n",
            "Epoch [15], Batch [80/938], Loss: 0.5564\n",
            "Epoch [15], Batch [90/938], Loss: 0.5604\n",
            "Epoch [15], Batch [100/938], Loss: 0.5253\n",
            "Epoch [15], Batch [110/938], Loss: 0.4844\n",
            "Epoch [15], Batch [120/938], Loss: 0.3396\n",
            "Epoch [15], Batch [130/938], Loss: 0.5608\n",
            "Epoch [15], Batch [140/938], Loss: 0.4485\n",
            "Epoch [15], Batch [150/938], Loss: 0.5000\n",
            "Epoch [15], Batch [160/938], Loss: 0.5886\n",
            "Epoch [15], Batch [170/938], Loss: 0.5132\n",
            "Epoch [15], Batch [180/938], Loss: 0.2891\n",
            "Epoch [15], Batch [190/938], Loss: 0.5131\n",
            "Epoch [15], Batch [200/938], Loss: 0.4812\n",
            "Epoch [15], Batch [210/938], Loss: 0.4050\n",
            "Epoch [15], Batch [220/938], Loss: 0.4213\n",
            "Epoch [15], Batch [230/938], Loss: 0.5456\n",
            "Epoch [15], Batch [240/938], Loss: 0.3324\n",
            "Epoch [15], Batch [250/938], Loss: 0.5922\n",
            "Epoch [15], Batch [260/938], Loss: 0.4911\n",
            "Epoch [15], Batch [270/938], Loss: 0.5252\n",
            "Epoch [15], Batch [280/938], Loss: 0.5290\n",
            "Epoch [15], Batch [290/938], Loss: 0.3921\n",
            "Epoch [15], Batch [300/938], Loss: 0.6098\n",
            "Epoch [15], Batch [310/938], Loss: 0.4229\n",
            "Epoch [15], Batch [320/938], Loss: 0.7840\n",
            "Epoch [15], Batch [330/938], Loss: 0.4811\n",
            "Epoch [15], Batch [340/938], Loss: 0.4867\n",
            "Epoch [15], Batch [350/938], Loss: 0.3713\n",
            "Epoch [15], Batch [360/938], Loss: 0.5567\n",
            "Epoch [15], Batch [370/938], Loss: 0.4241\n",
            "Epoch [15], Batch [380/938], Loss: 0.4174\n",
            "Epoch [15], Batch [390/938], Loss: 0.6041\n",
            "Epoch [15], Batch [400/938], Loss: 0.6232\n",
            "Epoch [15], Batch [410/938], Loss: 0.2851\n",
            "Epoch [15], Batch [420/938], Loss: 0.4755\n",
            "Epoch [15], Batch [430/938], Loss: 0.4407\n",
            "Epoch [15], Batch [440/938], Loss: 0.5121\n",
            "Epoch [15], Batch [450/938], Loss: 0.4356\n",
            "Epoch [15], Batch [460/938], Loss: 0.4562\n",
            "Epoch [15], Batch [470/938], Loss: 0.5085\n",
            "Epoch [15], Batch [480/938], Loss: 0.6384\n",
            "Epoch [15], Batch [490/938], Loss: 0.4656\n",
            "Epoch [15], Batch [500/938], Loss: 0.4591\n",
            "Epoch [15], Batch [510/938], Loss: 0.4039\n",
            "Epoch [15], Batch [520/938], Loss: 0.5866\n",
            "Epoch [15], Batch [530/938], Loss: 0.4473\n",
            "Epoch [15], Batch [540/938], Loss: 0.4273\n",
            "Epoch [15], Batch [550/938], Loss: 0.5244\n",
            "Epoch [15], Batch [560/938], Loss: 0.3987\n",
            "Epoch [15], Batch [570/938], Loss: 0.5625\n",
            "Epoch [15], Batch [580/938], Loss: 0.5131\n",
            "Epoch [15], Batch [590/938], Loss: 0.4717\n",
            "Epoch [15], Batch [600/938], Loss: 0.5890\n",
            "Epoch [15], Batch [610/938], Loss: 0.3321\n",
            "Epoch [15], Batch [620/938], Loss: 0.4816\n",
            "Epoch [15], Batch [630/938], Loss: 0.6256\n",
            "Epoch [15], Batch [640/938], Loss: 0.4465\n",
            "Epoch [15], Batch [650/938], Loss: 0.3436\n",
            "Epoch [15], Batch [660/938], Loss: 0.5524\n",
            "Epoch [15], Batch [670/938], Loss: 0.5969\n",
            "Epoch [15], Batch [680/938], Loss: 0.4773\n",
            "Epoch [15], Batch [690/938], Loss: 0.4024\n",
            "Epoch [15], Batch [700/938], Loss: 0.4526\n",
            "Epoch [15], Batch [710/938], Loss: 0.2925\n",
            "Epoch [15], Batch [720/938], Loss: 0.3788\n",
            "Epoch [15], Batch [730/938], Loss: 0.4409\n",
            "Epoch [15], Batch [740/938], Loss: 0.3294\n",
            "Epoch [15], Batch [750/938], Loss: 0.4975\n",
            "Epoch [15], Batch [760/938], Loss: 0.4191\n",
            "Epoch [15], Batch [770/938], Loss: 0.4409\n",
            "Epoch [15], Batch [780/938], Loss: 0.4223\n",
            "Epoch [15], Batch [790/938], Loss: 0.5624\n",
            "Epoch [15], Batch [800/938], Loss: 0.4669\n",
            "Epoch [15], Batch [810/938], Loss: 0.3353\n",
            "Epoch [15], Batch [820/938], Loss: 0.4226\n",
            "Epoch [15], Batch [830/938], Loss: 0.4370\n",
            "Epoch [15], Batch [840/938], Loss: 0.7148\n",
            "Epoch [15], Batch [850/938], Loss: 0.4591\n",
            "Epoch [15], Batch [860/938], Loss: 0.3482\n",
            "Epoch [15], Batch [870/938], Loss: 0.5297\n",
            "Epoch [15], Batch [880/938], Loss: 0.5481\n",
            "Epoch [15], Batch [890/938], Loss: 0.5349\n",
            "Epoch [15], Batch [900/938], Loss: 0.4861\n",
            "Epoch [15], Batch [910/938], Loss: 0.3926\n",
            "Epoch [15], Batch [920/938], Loss: 0.5083\n",
            "Epoch [15], Batch [930/938], Loss: 0.4570\n",
            "Epoch [15], Batch [938/938], Loss: 0.3781\n",
            "Accuracy of train set: 83.56%\n",
            "Epoch [15], Batch [1/157], test Loss: 0.6765\n",
            "Epoch [15], Batch [2/157], test Loss: 0.5032\n",
            "Epoch [15], Batch [3/157], test Loss: 0.5820\n",
            "Epoch [15], Batch [4/157], test Loss: 0.4582\n",
            "Epoch [15], Batch [5/157], test Loss: 0.8346\n",
            "Epoch [15], Batch [6/157], test Loss: 0.4829\n",
            "Epoch [15], Batch [7/157], test Loss: 0.3055\n",
            "Epoch [15], Batch [8/157], test Loss: 0.6924\n",
            "Epoch [15], Batch [9/157], test Loss: 0.4354\n",
            "Epoch [15], Batch [10/157], test Loss: 0.3868\n",
            "Epoch [15], Batch [11/157], test Loss: 0.3110\n",
            "Epoch [15], Batch [12/157], test Loss: 0.6382\n",
            "Epoch [15], Batch [13/157], test Loss: 0.3698\n",
            "Epoch [15], Batch [14/157], test Loss: 0.5152\n",
            "Epoch [15], Batch [15/157], test Loss: 0.3465\n",
            "Epoch [15], Batch [16/157], test Loss: 0.3324\n",
            "Epoch [15], Batch [17/157], test Loss: 0.3958\n",
            "Epoch [15], Batch [18/157], test Loss: 0.4926\n",
            "Epoch [15], Batch [19/157], test Loss: 0.7430\n",
            "Epoch [15], Batch [20/157], test Loss: 0.4726\n",
            "Epoch [15], Batch [21/157], test Loss: 0.4179\n",
            "Epoch [15], Batch [22/157], test Loss: 0.5577\n",
            "Epoch [15], Batch [23/157], test Loss: 0.5666\n",
            "Epoch [15], Batch [24/157], test Loss: 0.4016\n",
            "Epoch [15], Batch [25/157], test Loss: 0.4272\n",
            "Epoch [15], Batch [26/157], test Loss: 0.5096\n",
            "Epoch [15], Batch [27/157], test Loss: 0.4872\n",
            "Epoch [15], Batch [28/157], test Loss: 0.4324\n",
            "Epoch [15], Batch [29/157], test Loss: 0.5998\n",
            "Epoch [15], Batch [30/157], test Loss: 0.5075\n",
            "Epoch [15], Batch [31/157], test Loss: 0.6410\n",
            "Epoch [15], Batch [32/157], test Loss: 0.5608\n",
            "Epoch [15], Batch [33/157], test Loss: 0.2989\n",
            "Epoch [15], Batch [34/157], test Loss: 0.4829\n",
            "Epoch [15], Batch [35/157], test Loss: 0.4946\n",
            "Epoch [15], Batch [36/157], test Loss: 0.3516\n",
            "Epoch [15], Batch [37/157], test Loss: 0.3847\n",
            "Epoch [15], Batch [38/157], test Loss: 0.4619\n",
            "Epoch [15], Batch [39/157], test Loss: 0.5679\n",
            "Epoch [15], Batch [40/157], test Loss: 0.4373\n",
            "Epoch [15], Batch [41/157], test Loss: 0.3965\n",
            "Epoch [15], Batch [42/157], test Loss: 0.5171\n",
            "Epoch [15], Batch [43/157], test Loss: 0.7556\n",
            "Epoch [15], Batch [44/157], test Loss: 0.3181\n",
            "Epoch [15], Batch [45/157], test Loss: 0.5215\n",
            "Epoch [15], Batch [46/157], test Loss: 0.4952\n",
            "Epoch [15], Batch [47/157], test Loss: 0.4943\n",
            "Epoch [15], Batch [48/157], test Loss: 0.4900\n",
            "Epoch [15], Batch [49/157], test Loss: 0.4879\n",
            "Epoch [15], Batch [50/157], test Loss: 0.3467\n",
            "Epoch [15], Batch [51/157], test Loss: 0.5346\n",
            "Epoch [15], Batch [52/157], test Loss: 0.6229\n",
            "Epoch [15], Batch [53/157], test Loss: 0.4088\n",
            "Epoch [15], Batch [54/157], test Loss: 0.4930\n",
            "Epoch [15], Batch [55/157], test Loss: 0.4172\n",
            "Epoch [15], Batch [56/157], test Loss: 0.6082\n",
            "Epoch [15], Batch [57/157], test Loss: 0.5208\n",
            "Epoch [15], Batch [58/157], test Loss: 0.4737\n",
            "Epoch [15], Batch [59/157], test Loss: 0.5745\n",
            "Epoch [15], Batch [60/157], test Loss: 0.4157\n",
            "Epoch [15], Batch [61/157], test Loss: 0.4394\n",
            "Epoch [15], Batch [62/157], test Loss: 0.5548\n",
            "Epoch [15], Batch [63/157], test Loss: 0.5082\n",
            "Epoch [15], Batch [64/157], test Loss: 0.6665\n",
            "Epoch [15], Batch [65/157], test Loss: 0.5238\n",
            "Epoch [15], Batch [66/157], test Loss: 0.5396\n",
            "Epoch [15], Batch [67/157], test Loss: 0.5292\n",
            "Epoch [15], Batch [68/157], test Loss: 0.5787\n",
            "Epoch [15], Batch [69/157], test Loss: 0.6507\n",
            "Epoch [15], Batch [70/157], test Loss: 0.3495\n",
            "Epoch [15], Batch [71/157], test Loss: 0.4801\n",
            "Epoch [15], Batch [72/157], test Loss: 0.4386\n",
            "Epoch [15], Batch [73/157], test Loss: 0.5108\n",
            "Epoch [15], Batch [74/157], test Loss: 0.3729\n",
            "Epoch [15], Batch [75/157], test Loss: 0.4407\n",
            "Epoch [15], Batch [76/157], test Loss: 0.5235\n",
            "Epoch [15], Batch [77/157], test Loss: 0.5628\n",
            "Epoch [15], Batch [78/157], test Loss: 0.5412\n",
            "Epoch [15], Batch [79/157], test Loss: 0.4882\n",
            "Epoch [15], Batch [80/157], test Loss: 0.6875\n",
            "Epoch [15], Batch [81/157], test Loss: 0.4085\n",
            "Epoch [15], Batch [82/157], test Loss: 0.3898\n",
            "Epoch [15], Batch [83/157], test Loss: 0.5473\n",
            "Epoch [15], Batch [84/157], test Loss: 0.3923\n",
            "Epoch [15], Batch [85/157], test Loss: 0.5420\n",
            "Epoch [15], Batch [86/157], test Loss: 0.4966\n",
            "Epoch [15], Batch [87/157], test Loss: 0.5278\n",
            "Epoch [15], Batch [88/157], test Loss: 0.5437\n",
            "Epoch [15], Batch [89/157], test Loss: 0.4613\n",
            "Epoch [15], Batch [90/157], test Loss: 0.5528\n",
            "Epoch [15], Batch [91/157], test Loss: 0.3965\n",
            "Epoch [15], Batch [92/157], test Loss: 0.6048\n",
            "Epoch [15], Batch [93/157], test Loss: 0.5654\n",
            "Epoch [15], Batch [94/157], test Loss: 0.5499\n",
            "Epoch [15], Batch [95/157], test Loss: 0.5568\n",
            "Epoch [15], Batch [96/157], test Loss: 0.7747\n",
            "Epoch [15], Batch [97/157], test Loss: 0.4401\n",
            "Epoch [15], Batch [98/157], test Loss: 0.5034\n",
            "Epoch [15], Batch [99/157], test Loss: 0.5552\n",
            "Epoch [15], Batch [100/157], test Loss: 0.4713\n",
            "Epoch [15], Batch [101/157], test Loss: 0.4460\n",
            "Epoch [15], Batch [102/157], test Loss: 0.8312\n",
            "Epoch [15], Batch [103/157], test Loss: 0.4626\n",
            "Epoch [15], Batch [104/157], test Loss: 0.5184\n",
            "Epoch [15], Batch [105/157], test Loss: 0.4434\n",
            "Epoch [15], Batch [106/157], test Loss: 0.4764\n",
            "Epoch [15], Batch [107/157], test Loss: 0.6059\n",
            "Epoch [15], Batch [108/157], test Loss: 0.5413\n",
            "Epoch [15], Batch [109/157], test Loss: 0.6100\n",
            "Epoch [15], Batch [110/157], test Loss: 0.3732\n",
            "Epoch [15], Batch [111/157], test Loss: 0.6188\n",
            "Epoch [15], Batch [112/157], test Loss: 0.4274\n",
            "Epoch [15], Batch [113/157], test Loss: 0.5167\n",
            "Epoch [15], Batch [114/157], test Loss: 0.4648\n",
            "Epoch [15], Batch [115/157], test Loss: 0.4422\n",
            "Epoch [15], Batch [116/157], test Loss: 0.5927\n",
            "Epoch [15], Batch [117/157], test Loss: 0.5119\n",
            "Epoch [15], Batch [118/157], test Loss: 0.4496\n",
            "Epoch [15], Batch [119/157], test Loss: 0.5419\n",
            "Epoch [15], Batch [120/157], test Loss: 0.4403\n",
            "Epoch [15], Batch [121/157], test Loss: 0.5865\n",
            "Epoch [15], Batch [122/157], test Loss: 0.6202\n",
            "Epoch [15], Batch [123/157], test Loss: 0.5716\n",
            "Epoch [15], Batch [124/157], test Loss: 0.8309\n",
            "Epoch [15], Batch [125/157], test Loss: 0.5750\n",
            "Epoch [15], Batch [126/157], test Loss: 0.6631\n",
            "Epoch [15], Batch [127/157], test Loss: 0.3740\n",
            "Epoch [15], Batch [128/157], test Loss: 0.4007\n",
            "Epoch [15], Batch [129/157], test Loss: 0.4915\n",
            "Epoch [15], Batch [130/157], test Loss: 0.4868\n",
            "Epoch [15], Batch [131/157], test Loss: 0.4358\n",
            "Epoch [15], Batch [132/157], test Loss: 0.5021\n",
            "Epoch [15], Batch [133/157], test Loss: 0.6509\n",
            "Epoch [15], Batch [134/157], test Loss: 0.5559\n",
            "Epoch [15], Batch [135/157], test Loss: 0.3882\n",
            "Epoch [15], Batch [136/157], test Loss: 0.5710\n",
            "Epoch [15], Batch [137/157], test Loss: 0.3418\n",
            "Epoch [15], Batch [138/157], test Loss: 0.4942\n",
            "Epoch [15], Batch [139/157], test Loss: 0.4854\n",
            "Epoch [15], Batch [140/157], test Loss: 0.7129\n",
            "Epoch [15], Batch [141/157], test Loss: 0.4075\n",
            "Epoch [15], Batch [142/157], test Loss: 0.5278\n",
            "Epoch [15], Batch [143/157], test Loss: 0.5522\n",
            "Epoch [15], Batch [144/157], test Loss: 0.4942\n",
            "Epoch [15], Batch [145/157], test Loss: 0.3684\n",
            "Epoch [15], Batch [146/157], test Loss: 0.6271\n",
            "Epoch [15], Batch [147/157], test Loss: 0.3833\n",
            "Epoch [15], Batch [148/157], test Loss: 0.5985\n",
            "Epoch [15], Batch [149/157], test Loss: 0.6774\n",
            "Epoch [15], Batch [150/157], test Loss: 0.2851\n",
            "Epoch [15], Batch [151/157], test Loss: 0.3331\n",
            "Epoch [15], Batch [152/157], test Loss: 0.4328\n",
            "Epoch [15], Batch [153/157], test Loss: 0.2990\n",
            "Epoch [15], Batch [154/157], test Loss: 0.4090\n",
            "Epoch [15], Batch [155/157], test Loss: 0.4869\n",
            "Epoch [15], Batch [156/157], test Loss: 0.5344\n",
            "Epoch [15], Batch [157/157], test Loss: 0.7543\n",
            "Accuracy of test set: 82.01%\n",
            "Epoch [16/25] - Train Loss: 0.4713, Train Accuracy: 83.56% - Test Loss: 0.5054, Test Accuracy: 82.01%\n",
            "Epoch [16], Batch [10/938], Loss: 0.5675\n",
            "Epoch [16], Batch [20/938], Loss: 0.4127\n",
            "Epoch [16], Batch [30/938], Loss: 0.5284\n",
            "Epoch [16], Batch [40/938], Loss: 0.4171\n",
            "Epoch [16], Batch [50/938], Loss: 0.4223\n",
            "Epoch [16], Batch [60/938], Loss: 0.3687\n",
            "Epoch [16], Batch [70/938], Loss: 0.3802\n",
            "Epoch [16], Batch [80/938], Loss: 0.4749\n",
            "Epoch [16], Batch [90/938], Loss: 0.4674\n",
            "Epoch [16], Batch [100/938], Loss: 0.4819\n",
            "Epoch [16], Batch [110/938], Loss: 0.4214\n",
            "Epoch [16], Batch [120/938], Loss: 0.3231\n",
            "Epoch [16], Batch [130/938], Loss: 0.4578\n",
            "Epoch [16], Batch [140/938], Loss: 0.4152\n",
            "Epoch [16], Batch [150/938], Loss: 0.7843\n",
            "Epoch [16], Batch [160/938], Loss: 0.5066\n",
            "Epoch [16], Batch [170/938], Loss: 0.4853\n",
            "Epoch [16], Batch [180/938], Loss: 0.4933\n",
            "Epoch [16], Batch [190/938], Loss: 0.3535\n",
            "Epoch [16], Batch [200/938], Loss: 0.4754\n",
            "Epoch [16], Batch [210/938], Loss: 0.3277\n",
            "Epoch [16], Batch [220/938], Loss: 0.5076\n",
            "Epoch [16], Batch [230/938], Loss: 0.4843\n",
            "Epoch [16], Batch [240/938], Loss: 0.4819\n",
            "Epoch [16], Batch [250/938], Loss: 0.3297\n",
            "Epoch [16], Batch [260/938], Loss: 0.5235\n",
            "Epoch [16], Batch [270/938], Loss: 0.4401\n",
            "Epoch [16], Batch [280/938], Loss: 0.3850\n",
            "Epoch [16], Batch [290/938], Loss: 0.4119\n",
            "Epoch [16], Batch [300/938], Loss: 0.4423\n",
            "Epoch [16], Batch [310/938], Loss: 0.4312\n",
            "Epoch [16], Batch [320/938], Loss: 0.5001\n",
            "Epoch [16], Batch [330/938], Loss: 0.3198\n",
            "Epoch [16], Batch [340/938], Loss: 0.5598\n",
            "Epoch [16], Batch [350/938], Loss: 0.3330\n",
            "Epoch [16], Batch [360/938], Loss: 0.7056\n",
            "Epoch [16], Batch [370/938], Loss: 0.5296\n",
            "Epoch [16], Batch [380/938], Loss: 0.5429\n",
            "Epoch [16], Batch [390/938], Loss: 0.4130\n",
            "Epoch [16], Batch [400/938], Loss: 0.4834\n",
            "Epoch [16], Batch [410/938], Loss: 0.2747\n",
            "Epoch [16], Batch [420/938], Loss: 0.4512\n",
            "Epoch [16], Batch [430/938], Loss: 0.4413\n",
            "Epoch [16], Batch [440/938], Loss: 0.5268\n",
            "Epoch [16], Batch [450/938], Loss: 0.6012\n",
            "Epoch [16], Batch [460/938], Loss: 0.3294\n",
            "Epoch [16], Batch [470/938], Loss: 0.4880\n",
            "Epoch [16], Batch [480/938], Loss: 0.4606\n",
            "Epoch [16], Batch [490/938], Loss: 0.6117\n",
            "Epoch [16], Batch [500/938], Loss: 0.4319\n",
            "Epoch [16], Batch [510/938], Loss: 0.3367\n",
            "Epoch [16], Batch [520/938], Loss: 0.5266\n",
            "Epoch [16], Batch [530/938], Loss: 0.5990\n",
            "Epoch [16], Batch [540/938], Loss: 0.5085\n",
            "Epoch [16], Batch [550/938], Loss: 0.4689\n",
            "Epoch [16], Batch [560/938], Loss: 0.4791\n",
            "Epoch [16], Batch [570/938], Loss: 0.5765\n",
            "Epoch [16], Batch [580/938], Loss: 0.3986\n",
            "Epoch [16], Batch [590/938], Loss: 0.7386\n",
            "Epoch [16], Batch [600/938], Loss: 0.5350\n",
            "Epoch [16], Batch [610/938], Loss: 0.5198\n",
            "Epoch [16], Batch [620/938], Loss: 0.3661\n",
            "Epoch [16], Batch [630/938], Loss: 0.5958\n",
            "Epoch [16], Batch [640/938], Loss: 0.5015\n",
            "Epoch [16], Batch [650/938], Loss: 0.3774\n",
            "Epoch [16], Batch [660/938], Loss: 0.4817\n",
            "Epoch [16], Batch [670/938], Loss: 0.5904\n",
            "Epoch [16], Batch [680/938], Loss: 0.6796\n",
            "Epoch [16], Batch [690/938], Loss: 0.2431\n",
            "Epoch [16], Batch [700/938], Loss: 0.2032\n",
            "Epoch [16], Batch [710/938], Loss: 0.3531\n",
            "Epoch [16], Batch [720/938], Loss: 0.4371\n",
            "Epoch [16], Batch [730/938], Loss: 0.3574\n",
            "Epoch [16], Batch [740/938], Loss: 0.4855\n",
            "Epoch [16], Batch [750/938], Loss: 0.5807\n",
            "Epoch [16], Batch [760/938], Loss: 0.3735\n",
            "Epoch [16], Batch [770/938], Loss: 0.4671\n",
            "Epoch [16], Batch [780/938], Loss: 0.4060\n",
            "Epoch [16], Batch [790/938], Loss: 0.4344\n",
            "Epoch [16], Batch [800/938], Loss: 0.4962\n",
            "Epoch [16], Batch [810/938], Loss: 0.4325\n",
            "Epoch [16], Batch [820/938], Loss: 0.6358\n",
            "Epoch [16], Batch [830/938], Loss: 0.4787\n",
            "Epoch [16], Batch [840/938], Loss: 0.4403\n",
            "Epoch [16], Batch [850/938], Loss: 0.5083\n",
            "Epoch [16], Batch [860/938], Loss: 0.4105\n",
            "Epoch [16], Batch [870/938], Loss: 0.3950\n",
            "Epoch [16], Batch [880/938], Loss: 0.4667\n",
            "Epoch [16], Batch [890/938], Loss: 0.4418\n",
            "Epoch [16], Batch [900/938], Loss: 0.3929\n",
            "Epoch [16], Batch [910/938], Loss: 0.4641\n",
            "Epoch [16], Batch [920/938], Loss: 0.4077\n",
            "Epoch [16], Batch [930/938], Loss: 0.3473\n",
            "Epoch [16], Batch [938/938], Loss: 0.5727\n",
            "Accuracy of train set: 83.84%\n",
            "Epoch [16], Batch [1/157], test Loss: 0.5607\n",
            "Epoch [16], Batch [2/157], test Loss: 0.4858\n",
            "Epoch [16], Batch [3/157], test Loss: 0.4080\n",
            "Epoch [16], Batch [4/157], test Loss: 0.3739\n",
            "Epoch [16], Batch [5/157], test Loss: 0.4553\n",
            "Epoch [16], Batch [6/157], test Loss: 0.5608\n",
            "Epoch [16], Batch [7/157], test Loss: 0.6248\n",
            "Epoch [16], Batch [8/157], test Loss: 0.4513\n",
            "Epoch [16], Batch [9/157], test Loss: 0.3282\n",
            "Epoch [16], Batch [10/157], test Loss: 0.7601\n",
            "Epoch [16], Batch [11/157], test Loss: 0.4982\n",
            "Epoch [16], Batch [12/157], test Loss: 0.4600\n",
            "Epoch [16], Batch [13/157], test Loss: 0.4991\n",
            "Epoch [16], Batch [14/157], test Loss: 0.5288\n",
            "Epoch [16], Batch [15/157], test Loss: 0.7354\n",
            "Epoch [16], Batch [16/157], test Loss: 0.4319\n",
            "Epoch [16], Batch [17/157], test Loss: 0.5086\n",
            "Epoch [16], Batch [18/157], test Loss: 0.3886\n",
            "Epoch [16], Batch [19/157], test Loss: 0.6001\n",
            "Epoch [16], Batch [20/157], test Loss: 0.2686\n",
            "Epoch [16], Batch [21/157], test Loss: 0.5682\n",
            "Epoch [16], Batch [22/157], test Loss: 0.4180\n",
            "Epoch [16], Batch [23/157], test Loss: 0.5495\n",
            "Epoch [16], Batch [24/157], test Loss: 0.4621\n",
            "Epoch [16], Batch [25/157], test Loss: 0.4800\n",
            "Epoch [16], Batch [26/157], test Loss: 0.6528\n",
            "Epoch [16], Batch [27/157], test Loss: 0.4556\n",
            "Epoch [16], Batch [28/157], test Loss: 0.5992\n",
            "Epoch [16], Batch [29/157], test Loss: 0.4981\n",
            "Epoch [16], Batch [30/157], test Loss: 0.4905\n",
            "Epoch [16], Batch [31/157], test Loss: 0.4822\n",
            "Epoch [16], Batch [32/157], test Loss: 0.6047\n",
            "Epoch [16], Batch [33/157], test Loss: 0.4090\n",
            "Epoch [16], Batch [34/157], test Loss: 0.7183\n",
            "Epoch [16], Batch [35/157], test Loss: 0.5307\n",
            "Epoch [16], Batch [36/157], test Loss: 0.4416\n",
            "Epoch [16], Batch [37/157], test Loss: 0.4182\n",
            "Epoch [16], Batch [38/157], test Loss: 0.3850\n",
            "Epoch [16], Batch [39/157], test Loss: 0.4155\n",
            "Epoch [16], Batch [40/157], test Loss: 0.2312\n",
            "Epoch [16], Batch [41/157], test Loss: 0.5204\n",
            "Epoch [16], Batch [42/157], test Loss: 0.4482\n",
            "Epoch [16], Batch [43/157], test Loss: 0.4131\n",
            "Epoch [16], Batch [44/157], test Loss: 0.6208\n",
            "Epoch [16], Batch [45/157], test Loss: 0.3747\n",
            "Epoch [16], Batch [46/157], test Loss: 0.5700\n",
            "Epoch [16], Batch [47/157], test Loss: 0.6491\n",
            "Epoch [16], Batch [48/157], test Loss: 0.4519\n",
            "Epoch [16], Batch [49/157], test Loss: 0.4046\n",
            "Epoch [16], Batch [50/157], test Loss: 0.5347\n",
            "Epoch [16], Batch [51/157], test Loss: 0.5116\n",
            "Epoch [16], Batch [52/157], test Loss: 0.5331\n",
            "Epoch [16], Batch [53/157], test Loss: 0.3926\n",
            "Epoch [16], Batch [54/157], test Loss: 0.4667\n",
            "Epoch [16], Batch [55/157], test Loss: 0.5895\n",
            "Epoch [16], Batch [56/157], test Loss: 0.4291\n",
            "Epoch [16], Batch [57/157], test Loss: 0.6644\n",
            "Epoch [16], Batch [58/157], test Loss: 0.3296\n",
            "Epoch [16], Batch [59/157], test Loss: 0.5974\n",
            "Epoch [16], Batch [60/157], test Loss: 0.3330\n",
            "Epoch [16], Batch [61/157], test Loss: 0.8290\n",
            "Epoch [16], Batch [62/157], test Loss: 0.4915\n",
            "Epoch [16], Batch [63/157], test Loss: 0.4231\n",
            "Epoch [16], Batch [64/157], test Loss: 0.4270\n",
            "Epoch [16], Batch [65/157], test Loss: 0.4103\n",
            "Epoch [16], Batch [66/157], test Loss: 0.3395\n",
            "Epoch [16], Batch [67/157], test Loss: 0.7678\n",
            "Epoch [16], Batch [68/157], test Loss: 0.4619\n",
            "Epoch [16], Batch [69/157], test Loss: 0.5112\n",
            "Epoch [16], Batch [70/157], test Loss: 0.5230\n",
            "Epoch [16], Batch [71/157], test Loss: 0.3881\n",
            "Epoch [16], Batch [72/157], test Loss: 0.3151\n",
            "Epoch [16], Batch [73/157], test Loss: 0.5222\n",
            "Epoch [16], Batch [74/157], test Loss: 0.6277\n",
            "Epoch [16], Batch [75/157], test Loss: 0.4761\n",
            "Epoch [16], Batch [76/157], test Loss: 0.4278\n",
            "Epoch [16], Batch [77/157], test Loss: 0.4227\n",
            "Epoch [16], Batch [78/157], test Loss: 0.6905\n",
            "Epoch [16], Batch [79/157], test Loss: 0.2457\n",
            "Epoch [16], Batch [80/157], test Loss: 0.4747\n",
            "Epoch [16], Batch [81/157], test Loss: 0.4018\n",
            "Epoch [16], Batch [82/157], test Loss: 0.3345\n",
            "Epoch [16], Batch [83/157], test Loss: 0.4721\n",
            "Epoch [16], Batch [84/157], test Loss: 0.5026\n",
            "Epoch [16], Batch [85/157], test Loss: 0.4863\n",
            "Epoch [16], Batch [86/157], test Loss: 0.4007\n",
            "Epoch [16], Batch [87/157], test Loss: 0.6184\n",
            "Epoch [16], Batch [88/157], test Loss: 0.4248\n",
            "Epoch [16], Batch [89/157], test Loss: 0.4432\n",
            "Epoch [16], Batch [90/157], test Loss: 0.5886\n",
            "Epoch [16], Batch [91/157], test Loss: 0.5034\n",
            "Epoch [16], Batch [92/157], test Loss: 0.5210\n",
            "Epoch [16], Batch [93/157], test Loss: 0.3169\n",
            "Epoch [16], Batch [94/157], test Loss: 0.4347\n",
            "Epoch [16], Batch [95/157], test Loss: 0.7434\n",
            "Epoch [16], Batch [96/157], test Loss: 0.5590\n",
            "Epoch [16], Batch [97/157], test Loss: 0.3959\n",
            "Epoch [16], Batch [98/157], test Loss: 0.3754\n",
            "Epoch [16], Batch [99/157], test Loss: 0.2809\n",
            "Epoch [16], Batch [100/157], test Loss: 0.4047\n",
            "Epoch [16], Batch [101/157], test Loss: 0.3538\n",
            "Epoch [16], Batch [102/157], test Loss: 0.5051\n",
            "Epoch [16], Batch [103/157], test Loss: 0.5585\n",
            "Epoch [16], Batch [104/157], test Loss: 0.3753\n",
            "Epoch [16], Batch [105/157], test Loss: 0.4964\n",
            "Epoch [16], Batch [106/157], test Loss: 0.4260\n",
            "Epoch [16], Batch [107/157], test Loss: 0.4003\n",
            "Epoch [16], Batch [108/157], test Loss: 0.4923\n",
            "Epoch [16], Batch [109/157], test Loss: 0.4900\n",
            "Epoch [16], Batch [110/157], test Loss: 0.3654\n",
            "Epoch [16], Batch [111/157], test Loss: 0.7085\n",
            "Epoch [16], Batch [112/157], test Loss: 0.4636\n",
            "Epoch [16], Batch [113/157], test Loss: 0.4509\n",
            "Epoch [16], Batch [114/157], test Loss: 0.6657\n",
            "Epoch [16], Batch [115/157], test Loss: 0.3267\n",
            "Epoch [16], Batch [116/157], test Loss: 0.4073\n",
            "Epoch [16], Batch [117/157], test Loss: 0.4940\n",
            "Epoch [16], Batch [118/157], test Loss: 0.5680\n",
            "Epoch [16], Batch [119/157], test Loss: 0.6830\n",
            "Epoch [16], Batch [120/157], test Loss: 0.5576\n",
            "Epoch [16], Batch [121/157], test Loss: 0.5625\n",
            "Epoch [16], Batch [122/157], test Loss: 0.2500\n",
            "Epoch [16], Batch [123/157], test Loss: 0.2883\n",
            "Epoch [16], Batch [124/157], test Loss: 0.5195\n",
            "Epoch [16], Batch [125/157], test Loss: 0.5318\n",
            "Epoch [16], Batch [126/157], test Loss: 0.7394\n",
            "Epoch [16], Batch [127/157], test Loss: 0.4534\n",
            "Epoch [16], Batch [128/157], test Loss: 0.4583\n",
            "Epoch [16], Batch [129/157], test Loss: 0.3002\n",
            "Epoch [16], Batch [130/157], test Loss: 0.3515\n",
            "Epoch [16], Batch [131/157], test Loss: 0.5212\n",
            "Epoch [16], Batch [132/157], test Loss: 0.6689\n",
            "Epoch [16], Batch [133/157], test Loss: 0.2887\n",
            "Epoch [16], Batch [134/157], test Loss: 0.3828\n",
            "Epoch [16], Batch [135/157], test Loss: 0.4548\n",
            "Epoch [16], Batch [136/157], test Loss: 0.4161\n",
            "Epoch [16], Batch [137/157], test Loss: 0.5631\n",
            "Epoch [16], Batch [138/157], test Loss: 0.5003\n",
            "Epoch [16], Batch [139/157], test Loss: 0.4209\n",
            "Epoch [16], Batch [140/157], test Loss: 0.4238\n",
            "Epoch [16], Batch [141/157], test Loss: 0.5694\n",
            "Epoch [16], Batch [142/157], test Loss: 0.5723\n",
            "Epoch [16], Batch [143/157], test Loss: 0.4265\n",
            "Epoch [16], Batch [144/157], test Loss: 0.4073\n",
            "Epoch [16], Batch [145/157], test Loss: 0.5156\n",
            "Epoch [16], Batch [146/157], test Loss: 0.4916\n",
            "Epoch [16], Batch [147/157], test Loss: 0.8024\n",
            "Epoch [16], Batch [148/157], test Loss: 0.7766\n",
            "Epoch [16], Batch [149/157], test Loss: 0.6196\n",
            "Epoch [16], Batch [150/157], test Loss: 0.4412\n",
            "Epoch [16], Batch [151/157], test Loss: 0.3444\n",
            "Epoch [16], Batch [152/157], test Loss: 0.5885\n",
            "Epoch [16], Batch [153/157], test Loss: 0.4342\n",
            "Epoch [16], Batch [154/157], test Loss: 0.5939\n",
            "Epoch [16], Batch [155/157], test Loss: 0.5962\n",
            "Epoch [16], Batch [156/157], test Loss: 0.5974\n",
            "Epoch [16], Batch [157/157], test Loss: 0.7388\n",
            "Accuracy of test set: 82.73%\n",
            "Epoch [17/25] - Train Loss: 0.4620, Train Accuracy: 83.84% - Test Loss: 0.4902, Test Accuracy: 82.73%\n",
            "Epoch [17], Batch [10/938], Loss: 0.3368\n",
            "Epoch [17], Batch [20/938], Loss: 0.2774\n",
            "Epoch [17], Batch [30/938], Loss: 0.6710\n",
            "Epoch [17], Batch [40/938], Loss: 0.4595\n",
            "Epoch [17], Batch [50/938], Loss: 0.3422\n",
            "Epoch [17], Batch [60/938], Loss: 0.5008\n",
            "Epoch [17], Batch [70/938], Loss: 0.4777\n",
            "Epoch [17], Batch [80/938], Loss: 0.5863\n",
            "Epoch [17], Batch [90/938], Loss: 0.3554\n",
            "Epoch [17], Batch [100/938], Loss: 0.4329\n",
            "Epoch [17], Batch [110/938], Loss: 0.3711\n",
            "Epoch [17], Batch [120/938], Loss: 0.4671\n",
            "Epoch [17], Batch [130/938], Loss: 0.5902\n",
            "Epoch [17], Batch [140/938], Loss: 0.3538\n",
            "Epoch [17], Batch [150/938], Loss: 0.3770\n",
            "Epoch [17], Batch [160/938], Loss: 0.3931\n",
            "Epoch [17], Batch [170/938], Loss: 0.5299\n",
            "Epoch [17], Batch [180/938], Loss: 0.4440\n",
            "Epoch [17], Batch [190/938], Loss: 0.6843\n",
            "Epoch [17], Batch [200/938], Loss: 0.3052\n",
            "Epoch [17], Batch [210/938], Loss: 0.6947\n",
            "Epoch [17], Batch [220/938], Loss: 0.4740\n",
            "Epoch [17], Batch [230/938], Loss: 0.5538\n",
            "Epoch [17], Batch [240/938], Loss: 0.6252\n",
            "Epoch [17], Batch [250/938], Loss: 0.5168\n",
            "Epoch [17], Batch [260/938], Loss: 0.5056\n",
            "Epoch [17], Batch [270/938], Loss: 0.3373\n",
            "Epoch [17], Batch [280/938], Loss: 0.3913\n",
            "Epoch [17], Batch [290/938], Loss: 0.6630\n",
            "Epoch [17], Batch [300/938], Loss: 0.2800\n",
            "Epoch [17], Batch [310/938], Loss: 0.3614\n",
            "Epoch [17], Batch [320/938], Loss: 0.6883\n",
            "Epoch [17], Batch [330/938], Loss: 0.4536\n",
            "Epoch [17], Batch [340/938], Loss: 0.5153\n",
            "Epoch [17], Batch [350/938], Loss: 0.4116\n",
            "Epoch [17], Batch [360/938], Loss: 0.4183\n",
            "Epoch [17], Batch [370/938], Loss: 0.4578\n",
            "Epoch [17], Batch [380/938], Loss: 0.3975\n",
            "Epoch [17], Batch [390/938], Loss: 0.3043\n",
            "Epoch [17], Batch [400/938], Loss: 0.5868\n",
            "Epoch [17], Batch [410/938], Loss: 0.2844\n",
            "Epoch [17], Batch [420/938], Loss: 0.4153\n",
            "Epoch [17], Batch [430/938], Loss: 0.4964\n",
            "Epoch [17], Batch [440/938], Loss: 0.4778\n",
            "Epoch [17], Batch [450/938], Loss: 0.3026\n",
            "Epoch [17], Batch [460/938], Loss: 0.4286\n",
            "Epoch [17], Batch [470/938], Loss: 0.3288\n",
            "Epoch [17], Batch [480/938], Loss: 0.2387\n",
            "Epoch [17], Batch [490/938], Loss: 0.5549\n",
            "Epoch [17], Batch [500/938], Loss: 0.6425\n",
            "Epoch [17], Batch [510/938], Loss: 0.2886\n",
            "Epoch [17], Batch [520/938], Loss: 0.5004\n",
            "Epoch [17], Batch [530/938], Loss: 0.4721\n",
            "Epoch [17], Batch [540/938], Loss: 0.4167\n",
            "Epoch [17], Batch [550/938], Loss: 0.3926\n",
            "Epoch [17], Batch [560/938], Loss: 0.8490\n",
            "Epoch [17], Batch [570/938], Loss: 0.7484\n",
            "Epoch [17], Batch [580/938], Loss: 0.4035\n",
            "Epoch [17], Batch [590/938], Loss: 0.2667\n",
            "Epoch [17], Batch [600/938], Loss: 0.4415\n",
            "Epoch [17], Batch [610/938], Loss: 0.4742\n",
            "Epoch [17], Batch [620/938], Loss: 0.4508\n",
            "Epoch [17], Batch [630/938], Loss: 0.5115\n",
            "Epoch [17], Batch [640/938], Loss: 0.5099\n",
            "Epoch [17], Batch [650/938], Loss: 0.4766\n",
            "Epoch [17], Batch [660/938], Loss: 0.3842\n",
            "Epoch [17], Batch [670/938], Loss: 0.3737\n",
            "Epoch [17], Batch [680/938], Loss: 0.5177\n",
            "Epoch [17], Batch [690/938], Loss: 0.3263\n",
            "Epoch [17], Batch [700/938], Loss: 0.4314\n",
            "Epoch [17], Batch [710/938], Loss: 0.3599\n",
            "Epoch [17], Batch [720/938], Loss: 0.3635\n",
            "Epoch [17], Batch [730/938], Loss: 0.5478\n",
            "Epoch [17], Batch [740/938], Loss: 0.3058\n",
            "Epoch [17], Batch [750/938], Loss: 0.5209\n",
            "Epoch [17], Batch [760/938], Loss: 0.4209\n",
            "Epoch [17], Batch [770/938], Loss: 0.4949\n",
            "Epoch [17], Batch [780/938], Loss: 0.3955\n",
            "Epoch [17], Batch [790/938], Loss: 0.5477\n",
            "Epoch [17], Batch [800/938], Loss: 0.4346\n",
            "Epoch [17], Batch [810/938], Loss: 0.3986\n",
            "Epoch [17], Batch [820/938], Loss: 0.3451\n",
            "Epoch [17], Batch [830/938], Loss: 0.3814\n",
            "Epoch [17], Batch [840/938], Loss: 0.5293\n",
            "Epoch [17], Batch [850/938], Loss: 0.3524\n",
            "Epoch [17], Batch [860/938], Loss: 0.4345\n",
            "Epoch [17], Batch [870/938], Loss: 0.4402\n",
            "Epoch [17], Batch [880/938], Loss: 0.5011\n",
            "Epoch [17], Batch [890/938], Loss: 0.2924\n",
            "Epoch [17], Batch [900/938], Loss: 0.5543\n",
            "Epoch [17], Batch [910/938], Loss: 0.4457\n",
            "Epoch [17], Batch [920/938], Loss: 0.3650\n",
            "Epoch [17], Batch [930/938], Loss: 0.4758\n",
            "Epoch [17], Batch [938/938], Loss: 0.6615\n",
            "Accuracy of train set: 84.23%\n",
            "Epoch [17], Batch [1/157], test Loss: 0.3246\n",
            "Epoch [17], Batch [2/157], test Loss: 0.3735\n",
            "Epoch [17], Batch [3/157], test Loss: 0.4735\n",
            "Epoch [17], Batch [4/157], test Loss: 0.5203\n",
            "Epoch [17], Batch [5/157], test Loss: 0.6163\n",
            "Epoch [17], Batch [6/157], test Loss: 0.4284\n",
            "Epoch [17], Batch [7/157], test Loss: 0.5087\n",
            "Epoch [17], Batch [8/157], test Loss: 0.5821\n",
            "Epoch [17], Batch [9/157], test Loss: 0.3424\n",
            "Epoch [17], Batch [10/157], test Loss: 0.5171\n",
            "Epoch [17], Batch [11/157], test Loss: 0.5670\n",
            "Epoch [17], Batch [12/157], test Loss: 0.4850\n",
            "Epoch [17], Batch [13/157], test Loss: 0.3747\n",
            "Epoch [17], Batch [14/157], test Loss: 0.5236\n",
            "Epoch [17], Batch [15/157], test Loss: 0.5342\n",
            "Epoch [17], Batch [16/157], test Loss: 0.5226\n",
            "Epoch [17], Batch [17/157], test Loss: 0.4288\n",
            "Epoch [17], Batch [18/157], test Loss: 0.3479\n",
            "Epoch [17], Batch [19/157], test Loss: 0.7249\n",
            "Epoch [17], Batch [20/157], test Loss: 0.4554\n",
            "Epoch [17], Batch [21/157], test Loss: 0.5108\n",
            "Epoch [17], Batch [22/157], test Loss: 0.4373\n",
            "Epoch [17], Batch [23/157], test Loss: 0.4873\n",
            "Epoch [17], Batch [24/157], test Loss: 0.3840\n",
            "Epoch [17], Batch [25/157], test Loss: 0.2550\n",
            "Epoch [17], Batch [26/157], test Loss: 0.5075\n",
            "Epoch [17], Batch [27/157], test Loss: 0.6457\n",
            "Epoch [17], Batch [28/157], test Loss: 0.3420\n",
            "Epoch [17], Batch [29/157], test Loss: 0.4554\n",
            "Epoch [17], Batch [30/157], test Loss: 0.3107\n",
            "Epoch [17], Batch [31/157], test Loss: 0.8874\n",
            "Epoch [17], Batch [32/157], test Loss: 0.5194\n",
            "Epoch [17], Batch [33/157], test Loss: 0.4231\n",
            "Epoch [17], Batch [34/157], test Loss: 0.6765\n",
            "Epoch [17], Batch [35/157], test Loss: 0.4492\n",
            "Epoch [17], Batch [36/157], test Loss: 0.4776\n",
            "Epoch [17], Batch [37/157], test Loss: 0.4768\n",
            "Epoch [17], Batch [38/157], test Loss: 0.5009\n",
            "Epoch [17], Batch [39/157], test Loss: 0.4559\n",
            "Epoch [17], Batch [40/157], test Loss: 0.4883\n",
            "Epoch [17], Batch [41/157], test Loss: 0.4614\n",
            "Epoch [17], Batch [42/157], test Loss: 0.4815\n",
            "Epoch [17], Batch [43/157], test Loss: 0.3917\n",
            "Epoch [17], Batch [44/157], test Loss: 0.3544\n",
            "Epoch [17], Batch [45/157], test Loss: 0.4543\n",
            "Epoch [17], Batch [46/157], test Loss: 0.4799\n",
            "Epoch [17], Batch [47/157], test Loss: 0.5982\n",
            "Epoch [17], Batch [48/157], test Loss: 0.6132\n",
            "Epoch [17], Batch [49/157], test Loss: 0.4750\n",
            "Epoch [17], Batch [50/157], test Loss: 0.7306\n",
            "Epoch [17], Batch [51/157], test Loss: 0.4633\n",
            "Epoch [17], Batch [52/157], test Loss: 0.6064\n",
            "Epoch [17], Batch [53/157], test Loss: 0.5725\n",
            "Epoch [17], Batch [54/157], test Loss: 0.6525\n",
            "Epoch [17], Batch [55/157], test Loss: 0.4580\n",
            "Epoch [17], Batch [56/157], test Loss: 0.4530\n",
            "Epoch [17], Batch [57/157], test Loss: 0.4036\n",
            "Epoch [17], Batch [58/157], test Loss: 0.5921\n",
            "Epoch [17], Batch [59/157], test Loss: 0.3938\n",
            "Epoch [17], Batch [60/157], test Loss: 0.4847\n",
            "Epoch [17], Batch [61/157], test Loss: 0.7224\n",
            "Epoch [17], Batch [62/157], test Loss: 0.6412\n",
            "Epoch [17], Batch [63/157], test Loss: 0.5834\n",
            "Epoch [17], Batch [64/157], test Loss: 0.4918\n",
            "Epoch [17], Batch [65/157], test Loss: 0.4456\n",
            "Epoch [17], Batch [66/157], test Loss: 0.6019\n",
            "Epoch [17], Batch [67/157], test Loss: 0.3120\n",
            "Epoch [17], Batch [68/157], test Loss: 0.6980\n",
            "Epoch [17], Batch [69/157], test Loss: 0.3954\n",
            "Epoch [17], Batch [70/157], test Loss: 0.4140\n",
            "Epoch [17], Batch [71/157], test Loss: 0.6827\n",
            "Epoch [17], Batch [72/157], test Loss: 0.4628\n",
            "Epoch [17], Batch [73/157], test Loss: 0.5405\n",
            "Epoch [17], Batch [74/157], test Loss: 0.4977\n",
            "Epoch [17], Batch [75/157], test Loss: 0.3704\n",
            "Epoch [17], Batch [76/157], test Loss: 0.4017\n",
            "Epoch [17], Batch [77/157], test Loss: 0.8474\n",
            "Epoch [17], Batch [78/157], test Loss: 0.5502\n",
            "Epoch [17], Batch [79/157], test Loss: 0.3858\n",
            "Epoch [17], Batch [80/157], test Loss: 0.3500\n",
            "Epoch [17], Batch [81/157], test Loss: 0.4326\n",
            "Epoch [17], Batch [82/157], test Loss: 0.3418\n",
            "Epoch [17], Batch [83/157], test Loss: 0.6392\n",
            "Epoch [17], Batch [84/157], test Loss: 0.3970\n",
            "Epoch [17], Batch [85/157], test Loss: 0.4417\n",
            "Epoch [17], Batch [86/157], test Loss: 0.5605\n",
            "Epoch [17], Batch [87/157], test Loss: 0.6074\n",
            "Epoch [17], Batch [88/157], test Loss: 0.4673\n",
            "Epoch [17], Batch [89/157], test Loss: 0.4003\n",
            "Epoch [17], Batch [90/157], test Loss: 0.3395\n",
            "Epoch [17], Batch [91/157], test Loss: 0.4606\n",
            "Epoch [17], Batch [92/157], test Loss: 0.5132\n",
            "Epoch [17], Batch [93/157], test Loss: 0.4380\n",
            "Epoch [17], Batch [94/157], test Loss: 0.4968\n",
            "Epoch [17], Batch [95/157], test Loss: 0.6571\n",
            "Epoch [17], Batch [96/157], test Loss: 0.4986\n",
            "Epoch [17], Batch [97/157], test Loss: 0.5448\n",
            "Epoch [17], Batch [98/157], test Loss: 0.4022\n",
            "Epoch [17], Batch [99/157], test Loss: 0.5000\n",
            "Epoch [17], Batch [100/157], test Loss: 0.6018\n",
            "Epoch [17], Batch [101/157], test Loss: 0.5748\n",
            "Epoch [17], Batch [102/157], test Loss: 0.3968\n",
            "Epoch [17], Batch [103/157], test Loss: 0.3494\n",
            "Epoch [17], Batch [104/157], test Loss: 0.5587\n",
            "Epoch [17], Batch [105/157], test Loss: 0.4627\n",
            "Epoch [17], Batch [106/157], test Loss: 0.3969\n",
            "Epoch [17], Batch [107/157], test Loss: 0.3608\n",
            "Epoch [17], Batch [108/157], test Loss: 0.3713\n",
            "Epoch [17], Batch [109/157], test Loss: 0.4152\n",
            "Epoch [17], Batch [110/157], test Loss: 0.4130\n",
            "Epoch [17], Batch [111/157], test Loss: 0.5251\n",
            "Epoch [17], Batch [112/157], test Loss: 0.6055\n",
            "Epoch [17], Batch [113/157], test Loss: 0.4462\n",
            "Epoch [17], Batch [114/157], test Loss: 0.3975\n",
            "Epoch [17], Batch [115/157], test Loss: 0.7197\n",
            "Epoch [17], Batch [116/157], test Loss: 0.6403\n",
            "Epoch [17], Batch [117/157], test Loss: 0.4260\n",
            "Epoch [17], Batch [118/157], test Loss: 0.4865\n",
            "Epoch [17], Batch [119/157], test Loss: 0.5216\n",
            "Epoch [17], Batch [120/157], test Loss: 0.4551\n",
            "Epoch [17], Batch [121/157], test Loss: 0.3565\n",
            "Epoch [17], Batch [122/157], test Loss: 0.4959\n",
            "Epoch [17], Batch [123/157], test Loss: 0.4109\n",
            "Epoch [17], Batch [124/157], test Loss: 0.5022\n",
            "Epoch [17], Batch [125/157], test Loss: 0.5359\n",
            "Epoch [17], Batch [126/157], test Loss: 0.5324\n",
            "Epoch [17], Batch [127/157], test Loss: 0.4445\n",
            "Epoch [17], Batch [128/157], test Loss: 0.4723\n",
            "Epoch [17], Batch [129/157], test Loss: 0.6415\n",
            "Epoch [17], Batch [130/157], test Loss: 0.7168\n",
            "Epoch [17], Batch [131/157], test Loss: 0.5622\n",
            "Epoch [17], Batch [132/157], test Loss: 0.6588\n",
            "Epoch [17], Batch [133/157], test Loss: 0.2059\n",
            "Epoch [17], Batch [134/157], test Loss: 0.5621\n",
            "Epoch [17], Batch [135/157], test Loss: 0.4288\n",
            "Epoch [17], Batch [136/157], test Loss: 0.5820\n",
            "Epoch [17], Batch [137/157], test Loss: 0.5064\n",
            "Epoch [17], Batch [138/157], test Loss: 0.5964\n",
            "Epoch [17], Batch [139/157], test Loss: 0.5587\n",
            "Epoch [17], Batch [140/157], test Loss: 0.7772\n",
            "Epoch [17], Batch [141/157], test Loss: 0.4903\n",
            "Epoch [17], Batch [142/157], test Loss: 0.4446\n",
            "Epoch [17], Batch [143/157], test Loss: 0.4457\n",
            "Epoch [17], Batch [144/157], test Loss: 0.5594\n",
            "Epoch [17], Batch [145/157], test Loss: 0.5782\n",
            "Epoch [17], Batch [146/157], test Loss: 0.5554\n",
            "Epoch [17], Batch [147/157], test Loss: 0.3847\n",
            "Epoch [17], Batch [148/157], test Loss: 0.3315\n",
            "Epoch [17], Batch [149/157], test Loss: 0.4691\n",
            "Epoch [17], Batch [150/157], test Loss: 0.2797\n",
            "Epoch [17], Batch [151/157], test Loss: 0.5126\n",
            "Epoch [17], Batch [152/157], test Loss: 0.6622\n",
            "Epoch [17], Batch [153/157], test Loss: 0.3941\n",
            "Epoch [17], Batch [154/157], test Loss: 0.3210\n",
            "Epoch [17], Batch [155/157], test Loss: 0.4331\n",
            "Epoch [17], Batch [156/157], test Loss: 0.4723\n",
            "Epoch [17], Batch [157/157], test Loss: 0.2118\n",
            "Accuracy of test set: 82.69%\n",
            "Epoch [18/25] - Train Loss: 0.4527, Train Accuracy: 84.23% - Test Loss: 0.4918, Test Accuracy: 82.69%\n",
            "Epoch [18], Batch [10/938], Loss: 0.3582\n",
            "Epoch [18], Batch [20/938], Loss: 0.6920\n",
            "Epoch [18], Batch [30/938], Loss: 0.3629\n",
            "Epoch [18], Batch [40/938], Loss: 0.2101\n",
            "Epoch [18], Batch [50/938], Loss: 0.3426\n",
            "Epoch [18], Batch [60/938], Loss: 0.4600\n",
            "Epoch [18], Batch [70/938], Loss: 0.4907\n",
            "Epoch [18], Batch [80/938], Loss: 0.4669\n",
            "Epoch [18], Batch [90/938], Loss: 0.4129\n",
            "Epoch [18], Batch [100/938], Loss: 0.4149\n",
            "Epoch [18], Batch [110/938], Loss: 0.4078\n",
            "Epoch [18], Batch [120/938], Loss: 0.5248\n",
            "Epoch [18], Batch [130/938], Loss: 0.3909\n",
            "Epoch [18], Batch [140/938], Loss: 0.7697\n",
            "Epoch [18], Batch [150/938], Loss: 0.3735\n",
            "Epoch [18], Batch [160/938], Loss: 0.3842\n",
            "Epoch [18], Batch [170/938], Loss: 0.3087\n",
            "Epoch [18], Batch [180/938], Loss: 0.5190\n",
            "Epoch [18], Batch [190/938], Loss: 0.3047\n",
            "Epoch [18], Batch [200/938], Loss: 0.5164\n",
            "Epoch [18], Batch [210/938], Loss: 0.6240\n",
            "Epoch [18], Batch [220/938], Loss: 0.6180\n",
            "Epoch [18], Batch [230/938], Loss: 0.3434\n",
            "Epoch [18], Batch [240/938], Loss: 0.5096\n",
            "Epoch [18], Batch [250/938], Loss: 0.3520\n",
            "Epoch [18], Batch [260/938], Loss: 0.4108\n",
            "Epoch [18], Batch [270/938], Loss: 0.6172\n",
            "Epoch [18], Batch [280/938], Loss: 0.2627\n",
            "Epoch [18], Batch [290/938], Loss: 0.4467\n",
            "Epoch [18], Batch [300/938], Loss: 0.2564\n",
            "Epoch [18], Batch [310/938], Loss: 0.5956\n",
            "Epoch [18], Batch [320/938], Loss: 0.4126\n",
            "Epoch [18], Batch [330/938], Loss: 0.4140\n",
            "Epoch [18], Batch [340/938], Loss: 0.4062\n",
            "Epoch [18], Batch [350/938], Loss: 0.5219\n",
            "Epoch [18], Batch [360/938], Loss: 0.4575\n",
            "Epoch [18], Batch [370/938], Loss: 0.4693\n",
            "Epoch [18], Batch [380/938], Loss: 0.3682\n",
            "Epoch [18], Batch [390/938], Loss: 0.4594\n",
            "Epoch [18], Batch [400/938], Loss: 0.5052\n",
            "Epoch [18], Batch [410/938], Loss: 0.3178\n",
            "Epoch [18], Batch [420/938], Loss: 0.5551\n",
            "Epoch [18], Batch [430/938], Loss: 0.3902\n",
            "Epoch [18], Batch [440/938], Loss: 0.2960\n",
            "Epoch [18], Batch [450/938], Loss: 0.5688\n",
            "Epoch [18], Batch [460/938], Loss: 0.5699\n",
            "Epoch [18], Batch [470/938], Loss: 0.3420\n",
            "Epoch [18], Batch [480/938], Loss: 0.3699\n",
            "Epoch [18], Batch [490/938], Loss: 0.4063\n",
            "Epoch [18], Batch [500/938], Loss: 0.2385\n",
            "Epoch [18], Batch [510/938], Loss: 0.4344\n",
            "Epoch [18], Batch [520/938], Loss: 0.3797\n",
            "Epoch [18], Batch [530/938], Loss: 0.4443\n",
            "Epoch [18], Batch [540/938], Loss: 0.3960\n",
            "Epoch [18], Batch [550/938], Loss: 0.3371\n",
            "Epoch [18], Batch [560/938], Loss: 0.3663\n",
            "Epoch [18], Batch [570/938], Loss: 0.3601\n",
            "Epoch [18], Batch [580/938], Loss: 0.4010\n",
            "Epoch [18], Batch [590/938], Loss: 0.3551\n",
            "Epoch [18], Batch [600/938], Loss: 0.5756\n",
            "Epoch [18], Batch [610/938], Loss: 0.3503\n",
            "Epoch [18], Batch [620/938], Loss: 0.5681\n",
            "Epoch [18], Batch [630/938], Loss: 0.4611\n",
            "Epoch [18], Batch [640/938], Loss: 0.5744\n",
            "Epoch [18], Batch [650/938], Loss: 0.4876\n",
            "Epoch [18], Batch [660/938], Loss: 0.4431\n",
            "Epoch [18], Batch [670/938], Loss: 0.5990\n",
            "Epoch [18], Batch [680/938], Loss: 0.4232\n",
            "Epoch [18], Batch [690/938], Loss: 0.4123\n",
            "Epoch [18], Batch [700/938], Loss: 0.2715\n",
            "Epoch [18], Batch [710/938], Loss: 0.5457\n",
            "Epoch [18], Batch [720/938], Loss: 0.4295\n",
            "Epoch [18], Batch [730/938], Loss: 0.5842\n",
            "Epoch [18], Batch [740/938], Loss: 0.4955\n",
            "Epoch [18], Batch [750/938], Loss: 0.3842\n",
            "Epoch [18], Batch [760/938], Loss: 0.5602\n",
            "Epoch [18], Batch [770/938], Loss: 0.4846\n",
            "Epoch [18], Batch [780/938], Loss: 0.4396\n",
            "Epoch [18], Batch [790/938], Loss: 0.3321\n",
            "Epoch [18], Batch [800/938], Loss: 0.6669\n",
            "Epoch [18], Batch [810/938], Loss: 0.5606\n",
            "Epoch [18], Batch [820/938], Loss: 0.4107\n",
            "Epoch [18], Batch [830/938], Loss: 0.4858\n",
            "Epoch [18], Batch [840/938], Loss: 0.4776\n",
            "Epoch [18], Batch [850/938], Loss: 0.3382\n",
            "Epoch [18], Batch [860/938], Loss: 0.3929\n",
            "Epoch [18], Batch [870/938], Loss: 0.4346\n",
            "Epoch [18], Batch [880/938], Loss: 0.4296\n",
            "Epoch [18], Batch [890/938], Loss: 0.5723\n",
            "Epoch [18], Batch [900/938], Loss: 0.5590\n",
            "Epoch [18], Batch [910/938], Loss: 0.4742\n",
            "Epoch [18], Batch [920/938], Loss: 0.3645\n",
            "Epoch [18], Batch [930/938], Loss: 0.5062\n",
            "Epoch [18], Batch [938/938], Loss: 0.6341\n",
            "Accuracy of train set: 84.54%\n",
            "Epoch [18], Batch [1/157], test Loss: 0.7892\n",
            "Epoch [18], Batch [2/157], test Loss: 0.5443\n",
            "Epoch [18], Batch [3/157], test Loss: 0.5072\n",
            "Epoch [18], Batch [4/157], test Loss: 0.5385\n",
            "Epoch [18], Batch [5/157], test Loss: 0.5591\n",
            "Epoch [18], Batch [6/157], test Loss: 0.3705\n",
            "Epoch [18], Batch [7/157], test Loss: 0.4141\n",
            "Epoch [18], Batch [8/157], test Loss: 0.4380\n",
            "Epoch [18], Batch [9/157], test Loss: 0.7281\n",
            "Epoch [18], Batch [10/157], test Loss: 0.4037\n",
            "Epoch [18], Batch [11/157], test Loss: 0.3680\n",
            "Epoch [18], Batch [12/157], test Loss: 0.3539\n",
            "Epoch [18], Batch [13/157], test Loss: 0.5179\n",
            "Epoch [18], Batch [14/157], test Loss: 0.4745\n",
            "Epoch [18], Batch [15/157], test Loss: 0.4399\n",
            "Epoch [18], Batch [16/157], test Loss: 0.5513\n",
            "Epoch [18], Batch [17/157], test Loss: 0.4098\n",
            "Epoch [18], Batch [18/157], test Loss: 0.5329\n",
            "Epoch [18], Batch [19/157], test Loss: 0.6405\n",
            "Epoch [18], Batch [20/157], test Loss: 0.3241\n",
            "Epoch [18], Batch [21/157], test Loss: 0.4508\n",
            "Epoch [18], Batch [22/157], test Loss: 0.4032\n",
            "Epoch [18], Batch [23/157], test Loss: 0.3646\n",
            "Epoch [18], Batch [24/157], test Loss: 0.4346\n",
            "Epoch [18], Batch [25/157], test Loss: 0.4369\n",
            "Epoch [18], Batch [26/157], test Loss: 0.4677\n",
            "Epoch [18], Batch [27/157], test Loss: 0.5807\n",
            "Epoch [18], Batch [28/157], test Loss: 0.3460\n",
            "Epoch [18], Batch [29/157], test Loss: 0.6840\n",
            "Epoch [18], Batch [30/157], test Loss: 0.3652\n",
            "Epoch [18], Batch [31/157], test Loss: 0.8190\n",
            "Epoch [18], Batch [32/157], test Loss: 0.7851\n",
            "Epoch [18], Batch [33/157], test Loss: 0.4362\n",
            "Epoch [18], Batch [34/157], test Loss: 0.4372\n",
            "Epoch [18], Batch [35/157], test Loss: 0.6408\n",
            "Epoch [18], Batch [36/157], test Loss: 0.5838\n",
            "Epoch [18], Batch [37/157], test Loss: 0.5395\n",
            "Epoch [18], Batch [38/157], test Loss: 0.8914\n",
            "Epoch [18], Batch [39/157], test Loss: 0.4714\n",
            "Epoch [18], Batch [40/157], test Loss: 0.4313\n",
            "Epoch [18], Batch [41/157], test Loss: 0.4877\n",
            "Epoch [18], Batch [42/157], test Loss: 0.4142\n",
            "Epoch [18], Batch [43/157], test Loss: 0.4707\n",
            "Epoch [18], Batch [44/157], test Loss: 0.4257\n",
            "Epoch [18], Batch [45/157], test Loss: 0.5175\n",
            "Epoch [18], Batch [46/157], test Loss: 0.3373\n",
            "Epoch [18], Batch [47/157], test Loss: 0.5020\n",
            "Epoch [18], Batch [48/157], test Loss: 0.4729\n",
            "Epoch [18], Batch [49/157], test Loss: 0.5286\n",
            "Epoch [18], Batch [50/157], test Loss: 0.4928\n",
            "Epoch [18], Batch [51/157], test Loss: 0.5402\n",
            "Epoch [18], Batch [52/157], test Loss: 0.3222\n",
            "Epoch [18], Batch [53/157], test Loss: 0.5540\n",
            "Epoch [18], Batch [54/157], test Loss: 0.4275\n",
            "Epoch [18], Batch [55/157], test Loss: 0.3430\n",
            "Epoch [18], Batch [56/157], test Loss: 0.6189\n",
            "Epoch [18], Batch [57/157], test Loss: 0.3843\n",
            "Epoch [18], Batch [58/157], test Loss: 0.5112\n",
            "Epoch [18], Batch [59/157], test Loss: 0.6181\n",
            "Epoch [18], Batch [60/157], test Loss: 0.2753\n",
            "Epoch [18], Batch [61/157], test Loss: 0.4432\n",
            "Epoch [18], Batch [62/157], test Loss: 0.3772\n",
            "Epoch [18], Batch [63/157], test Loss: 0.4457\n",
            "Epoch [18], Batch [64/157], test Loss: 0.5857\n",
            "Epoch [18], Batch [65/157], test Loss: 0.4593\n",
            "Epoch [18], Batch [66/157], test Loss: 0.3858\n",
            "Epoch [18], Batch [67/157], test Loss: 0.4929\n",
            "Epoch [18], Batch [68/157], test Loss: 0.3850\n",
            "Epoch [18], Batch [69/157], test Loss: 0.5978\n",
            "Epoch [18], Batch [70/157], test Loss: 0.4005\n",
            "Epoch [18], Batch [71/157], test Loss: 0.5556\n",
            "Epoch [18], Batch [72/157], test Loss: 0.5397\n",
            "Epoch [18], Batch [73/157], test Loss: 0.4336\n",
            "Epoch [18], Batch [74/157], test Loss: 0.5665\n",
            "Epoch [18], Batch [75/157], test Loss: 0.3503\n",
            "Epoch [18], Batch [76/157], test Loss: 0.6032\n",
            "Epoch [18], Batch [77/157], test Loss: 0.4100\n",
            "Epoch [18], Batch [78/157], test Loss: 0.3775\n",
            "Epoch [18], Batch [79/157], test Loss: 0.5561\n",
            "Epoch [18], Batch [80/157], test Loss: 0.3324\n",
            "Epoch [18], Batch [81/157], test Loss: 0.4885\n",
            "Epoch [18], Batch [82/157], test Loss: 0.4820\n",
            "Epoch [18], Batch [83/157], test Loss: 0.7252\n",
            "Epoch [18], Batch [84/157], test Loss: 0.7450\n",
            "Epoch [18], Batch [85/157], test Loss: 0.4320\n",
            "Epoch [18], Batch [86/157], test Loss: 0.6083\n",
            "Epoch [18], Batch [87/157], test Loss: 0.5148\n",
            "Epoch [18], Batch [88/157], test Loss: 0.6146\n",
            "Epoch [18], Batch [89/157], test Loss: 0.4799\n",
            "Epoch [18], Batch [90/157], test Loss: 0.6557\n",
            "Epoch [18], Batch [91/157], test Loss: 0.4671\n",
            "Epoch [18], Batch [92/157], test Loss: 0.5914\n",
            "Epoch [18], Batch [93/157], test Loss: 0.5268\n",
            "Epoch [18], Batch [94/157], test Loss: 0.3953\n",
            "Epoch [18], Batch [95/157], test Loss: 0.3833\n",
            "Epoch [18], Batch [96/157], test Loss: 0.4683\n",
            "Epoch [18], Batch [97/157], test Loss: 0.6222\n",
            "Epoch [18], Batch [98/157], test Loss: 0.3603\n",
            "Epoch [18], Batch [99/157], test Loss: 0.8030\n",
            "Epoch [18], Batch [100/157], test Loss: 0.4617\n",
            "Epoch [18], Batch [101/157], test Loss: 0.4478\n",
            "Epoch [18], Batch [102/157], test Loss: 0.4924\n",
            "Epoch [18], Batch [103/157], test Loss: 0.5378\n",
            "Epoch [18], Batch [104/157], test Loss: 0.6248\n",
            "Epoch [18], Batch [105/157], test Loss: 0.4170\n",
            "Epoch [18], Batch [106/157], test Loss: 0.6038\n",
            "Epoch [18], Batch [107/157], test Loss: 0.3891\n",
            "Epoch [18], Batch [108/157], test Loss: 0.5339\n",
            "Epoch [18], Batch [109/157], test Loss: 0.4920\n",
            "Epoch [18], Batch [110/157], test Loss: 0.5373\n",
            "Epoch [18], Batch [111/157], test Loss: 0.5847\n",
            "Epoch [18], Batch [112/157], test Loss: 0.5330\n",
            "Epoch [18], Batch [113/157], test Loss: 0.6181\n",
            "Epoch [18], Batch [114/157], test Loss: 0.5547\n",
            "Epoch [18], Batch [115/157], test Loss: 0.4669\n",
            "Epoch [18], Batch [116/157], test Loss: 0.5065\n",
            "Epoch [18], Batch [117/157], test Loss: 0.3250\n",
            "Epoch [18], Batch [118/157], test Loss: 0.4188\n",
            "Epoch [18], Batch [119/157], test Loss: 0.3565\n",
            "Epoch [18], Batch [120/157], test Loss: 0.6272\n",
            "Epoch [18], Batch [121/157], test Loss: 0.4216\n",
            "Epoch [18], Batch [122/157], test Loss: 0.4661\n",
            "Epoch [18], Batch [123/157], test Loss: 0.5261\n",
            "Epoch [18], Batch [124/157], test Loss: 0.7176\n",
            "Epoch [18], Batch [125/157], test Loss: 0.6041\n",
            "Epoch [18], Batch [126/157], test Loss: 0.3340\n",
            "Epoch [18], Batch [127/157], test Loss: 0.6266\n",
            "Epoch [18], Batch [128/157], test Loss: 0.6077\n",
            "Epoch [18], Batch [129/157], test Loss: 0.3526\n",
            "Epoch [18], Batch [130/157], test Loss: 0.4325\n",
            "Epoch [18], Batch [131/157], test Loss: 0.6998\n",
            "Epoch [18], Batch [132/157], test Loss: 0.3490\n",
            "Epoch [18], Batch [133/157], test Loss: 0.3378\n",
            "Epoch [18], Batch [134/157], test Loss: 0.3956\n",
            "Epoch [18], Batch [135/157], test Loss: 0.4905\n",
            "Epoch [18], Batch [136/157], test Loss: 0.4227\n",
            "Epoch [18], Batch [137/157], test Loss: 0.4094\n",
            "Epoch [18], Batch [138/157], test Loss: 0.3088\n",
            "Epoch [18], Batch [139/157], test Loss: 0.4091\n",
            "Epoch [18], Batch [140/157], test Loss: 0.4949\n",
            "Epoch [18], Batch [141/157], test Loss: 0.8105\n",
            "Epoch [18], Batch [142/157], test Loss: 0.4351\n",
            "Epoch [18], Batch [143/157], test Loss: 0.3355\n",
            "Epoch [18], Batch [144/157], test Loss: 0.4917\n",
            "Epoch [18], Batch [145/157], test Loss: 0.4642\n",
            "Epoch [18], Batch [146/157], test Loss: 0.3946\n",
            "Epoch [18], Batch [147/157], test Loss: 0.5461\n",
            "Epoch [18], Batch [148/157], test Loss: 0.4326\n",
            "Epoch [18], Batch [149/157], test Loss: 0.4513\n",
            "Epoch [18], Batch [150/157], test Loss: 0.6573\n",
            "Epoch [18], Batch [151/157], test Loss: 0.5573\n",
            "Epoch [18], Batch [152/157], test Loss: 0.3379\n",
            "Epoch [18], Batch [153/157], test Loss: 0.3518\n",
            "Epoch [18], Batch [154/157], test Loss: 0.4240\n",
            "Epoch [18], Batch [155/157], test Loss: 0.4827\n",
            "Epoch [18], Batch [156/157], test Loss: 0.4933\n",
            "Epoch [18], Batch [157/157], test Loss: 0.4564\n",
            "Accuracy of test set: 82.48%\n",
            "Epoch [19/25] - Train Loss: 0.4426, Train Accuracy: 84.54% - Test Loss: 0.4932, Test Accuracy: 82.48%\n",
            "Epoch [19], Batch [10/938], Loss: 0.4783\n",
            "Epoch [19], Batch [20/938], Loss: 0.3879\n",
            "Epoch [19], Batch [30/938], Loss: 0.4776\n",
            "Epoch [19], Batch [40/938], Loss: 0.3505\n",
            "Epoch [19], Batch [50/938], Loss: 0.3611\n",
            "Epoch [19], Batch [60/938], Loss: 0.4203\n",
            "Epoch [19], Batch [70/938], Loss: 0.3946\n",
            "Epoch [19], Batch [80/938], Loss: 0.3172\n",
            "Epoch [19], Batch [90/938], Loss: 0.2910\n",
            "Epoch [19], Batch [100/938], Loss: 0.3724\n",
            "Epoch [19], Batch [110/938], Loss: 0.6172\n",
            "Epoch [19], Batch [120/938], Loss: 0.3917\n",
            "Epoch [19], Batch [130/938], Loss: 0.4707\n",
            "Epoch [19], Batch [140/938], Loss: 0.4733\n",
            "Epoch [19], Batch [150/938], Loss: 0.5047\n",
            "Epoch [19], Batch [160/938], Loss: 0.4895\n",
            "Epoch [19], Batch [170/938], Loss: 0.5831\n",
            "Epoch [19], Batch [180/938], Loss: 0.4280\n",
            "Epoch [19], Batch [190/938], Loss: 0.2891\n",
            "Epoch [19], Batch [200/938], Loss: 0.5628\n",
            "Epoch [19], Batch [210/938], Loss: 0.4090\n",
            "Epoch [19], Batch [220/938], Loss: 0.4349\n",
            "Epoch [19], Batch [230/938], Loss: 0.4182\n",
            "Epoch [19], Batch [240/938], Loss: 0.4526\n",
            "Epoch [19], Batch [250/938], Loss: 0.5759\n",
            "Epoch [19], Batch [260/938], Loss: 0.4733\n",
            "Epoch [19], Batch [270/938], Loss: 0.2893\n",
            "Epoch [19], Batch [280/938], Loss: 0.4908\n",
            "Epoch [19], Batch [290/938], Loss: 0.5440\n",
            "Epoch [19], Batch [300/938], Loss: 0.5046\n",
            "Epoch [19], Batch [310/938], Loss: 0.4363\n",
            "Epoch [19], Batch [320/938], Loss: 0.5868\n",
            "Epoch [19], Batch [330/938], Loss: 0.4549\n",
            "Epoch [19], Batch [340/938], Loss: 0.3521\n",
            "Epoch [19], Batch [350/938], Loss: 0.4529\n",
            "Epoch [19], Batch [360/938], Loss: 0.4435\n",
            "Epoch [19], Batch [370/938], Loss: 0.2440\n",
            "Epoch [19], Batch [380/938], Loss: 0.3917\n",
            "Epoch [19], Batch [390/938], Loss: 0.5246\n",
            "Epoch [19], Batch [400/938], Loss: 0.4997\n",
            "Epoch [19], Batch [410/938], Loss: 0.3363\n",
            "Epoch [19], Batch [420/938], Loss: 0.4770\n",
            "Epoch [19], Batch [430/938], Loss: 0.6009\n",
            "Epoch [19], Batch [440/938], Loss: 0.4244\n",
            "Epoch [19], Batch [450/938], Loss: 0.5675\n",
            "Epoch [19], Batch [460/938], Loss: 0.5154\n",
            "Epoch [19], Batch [470/938], Loss: 0.4642\n",
            "Epoch [19], Batch [480/938], Loss: 0.3465\n",
            "Epoch [19], Batch [490/938], Loss: 0.4288\n",
            "Epoch [19], Batch [500/938], Loss: 0.6442\n",
            "Epoch [19], Batch [510/938], Loss: 0.3950\n",
            "Epoch [19], Batch [520/938], Loss: 0.4013\n",
            "Epoch [19], Batch [530/938], Loss: 0.3587\n",
            "Epoch [19], Batch [540/938], Loss: 0.6431\n",
            "Epoch [19], Batch [550/938], Loss: 0.3976\n",
            "Epoch [19], Batch [560/938], Loss: 0.2720\n",
            "Epoch [19], Batch [570/938], Loss: 0.4933\n",
            "Epoch [19], Batch [580/938], Loss: 0.2991\n",
            "Epoch [19], Batch [590/938], Loss: 0.4018\n",
            "Epoch [19], Batch [600/938], Loss: 0.5927\n",
            "Epoch [19], Batch [610/938], Loss: 0.5892\n",
            "Epoch [19], Batch [620/938], Loss: 0.4795\n",
            "Epoch [19], Batch [630/938], Loss: 0.3356\n",
            "Epoch [19], Batch [640/938], Loss: 0.4946\n",
            "Epoch [19], Batch [650/938], Loss: 0.4429\n",
            "Epoch [19], Batch [660/938], Loss: 0.5461\n",
            "Epoch [19], Batch [670/938], Loss: 0.5282\n",
            "Epoch [19], Batch [680/938], Loss: 0.5077\n",
            "Epoch [19], Batch [690/938], Loss: 0.5393\n",
            "Epoch [19], Batch [700/938], Loss: 0.2671\n",
            "Epoch [19], Batch [710/938], Loss: 0.3933\n",
            "Epoch [19], Batch [720/938], Loss: 0.5864\n",
            "Epoch [19], Batch [730/938], Loss: 0.5042\n",
            "Epoch [19], Batch [740/938], Loss: 0.4652\n",
            "Epoch [19], Batch [750/938], Loss: 0.4397\n",
            "Epoch [19], Batch [760/938], Loss: 0.4435\n",
            "Epoch [19], Batch [770/938], Loss: 0.4606\n",
            "Epoch [19], Batch [780/938], Loss: 0.4418\n",
            "Epoch [19], Batch [790/938], Loss: 0.5812\n",
            "Epoch [19], Batch [800/938], Loss: 0.3648\n",
            "Epoch [19], Batch [810/938], Loss: 0.3847\n",
            "Epoch [19], Batch [820/938], Loss: 0.3310\n",
            "Epoch [19], Batch [830/938], Loss: 0.4502\n",
            "Epoch [19], Batch [840/938], Loss: 0.4281\n",
            "Epoch [19], Batch [850/938], Loss: 0.5551\n",
            "Epoch [19], Batch [860/938], Loss: 0.3175\n",
            "Epoch [19], Batch [870/938], Loss: 0.4498\n",
            "Epoch [19], Batch [880/938], Loss: 0.3968\n",
            "Epoch [19], Batch [890/938], Loss: 0.4413\n",
            "Epoch [19], Batch [900/938], Loss: 0.4529\n",
            "Epoch [19], Batch [910/938], Loss: 0.4312\n",
            "Epoch [19], Batch [920/938], Loss: 0.3996\n",
            "Epoch [19], Batch [930/938], Loss: 0.4171\n",
            "Epoch [19], Batch [938/938], Loss: 0.2774\n",
            "Accuracy of train set: 84.63%\n",
            "Epoch [19], Batch [1/157], test Loss: 0.3576\n",
            "Epoch [19], Batch [2/157], test Loss: 0.1876\n",
            "Epoch [19], Batch [3/157], test Loss: 0.5764\n",
            "Epoch [19], Batch [4/157], test Loss: 0.3531\n",
            "Epoch [19], Batch [5/157], test Loss: 0.4027\n",
            "Epoch [19], Batch [6/157], test Loss: 0.4490\n",
            "Epoch [19], Batch [7/157], test Loss: 0.3856\n",
            "Epoch [19], Batch [8/157], test Loss: 0.3978\n",
            "Epoch [19], Batch [9/157], test Loss: 0.6752\n",
            "Epoch [19], Batch [10/157], test Loss: 0.7049\n",
            "Epoch [19], Batch [11/157], test Loss: 0.4750\n",
            "Epoch [19], Batch [12/157], test Loss: 0.4875\n",
            "Epoch [19], Batch [13/157], test Loss: 0.5570\n",
            "Epoch [19], Batch [14/157], test Loss: 0.4692\n",
            "Epoch [19], Batch [15/157], test Loss: 0.5569\n",
            "Epoch [19], Batch [16/157], test Loss: 0.5388\n",
            "Epoch [19], Batch [17/157], test Loss: 0.3872\n",
            "Epoch [19], Batch [18/157], test Loss: 0.4504\n",
            "Epoch [19], Batch [19/157], test Loss: 0.4780\n",
            "Epoch [19], Batch [20/157], test Loss: 0.4502\n",
            "Epoch [19], Batch [21/157], test Loss: 0.4318\n",
            "Epoch [19], Batch [22/157], test Loss: 0.3493\n",
            "Epoch [19], Batch [23/157], test Loss: 0.6405\n",
            "Epoch [19], Batch [24/157], test Loss: 0.4528\n",
            "Epoch [19], Batch [25/157], test Loss: 0.5051\n",
            "Epoch [19], Batch [26/157], test Loss: 0.4807\n",
            "Epoch [19], Batch [27/157], test Loss: 0.4697\n",
            "Epoch [19], Batch [28/157], test Loss: 0.3032\n",
            "Epoch [19], Batch [29/157], test Loss: 0.3628\n",
            "Epoch [19], Batch [30/157], test Loss: 0.3541\n",
            "Epoch [19], Batch [31/157], test Loss: 0.6196\n",
            "Epoch [19], Batch [32/157], test Loss: 0.5715\n",
            "Epoch [19], Batch [33/157], test Loss: 0.5046\n",
            "Epoch [19], Batch [34/157], test Loss: 0.4011\n",
            "Epoch [19], Batch [35/157], test Loss: 0.4974\n",
            "Epoch [19], Batch [36/157], test Loss: 0.4584\n",
            "Epoch [19], Batch [37/157], test Loss: 0.4152\n",
            "Epoch [19], Batch [38/157], test Loss: 0.2762\n",
            "Epoch [19], Batch [39/157], test Loss: 0.3700\n",
            "Epoch [19], Batch [40/157], test Loss: 0.5023\n",
            "Epoch [19], Batch [41/157], test Loss: 0.4158\n",
            "Epoch [19], Batch [42/157], test Loss: 0.7555\n",
            "Epoch [19], Batch [43/157], test Loss: 0.6036\n",
            "Epoch [19], Batch [44/157], test Loss: 0.3887\n",
            "Epoch [19], Batch [45/157], test Loss: 0.5060\n",
            "Epoch [19], Batch [46/157], test Loss: 0.5327\n",
            "Epoch [19], Batch [47/157], test Loss: 0.3738\n",
            "Epoch [19], Batch [48/157], test Loss: 0.3185\n",
            "Epoch [19], Batch [49/157], test Loss: 0.4445\n",
            "Epoch [19], Batch [50/157], test Loss: 0.6398\n",
            "Epoch [19], Batch [51/157], test Loss: 0.4631\n",
            "Epoch [19], Batch [52/157], test Loss: 0.5066\n",
            "Epoch [19], Batch [53/157], test Loss: 0.3787\n",
            "Epoch [19], Batch [54/157], test Loss: 0.5649\n",
            "Epoch [19], Batch [55/157], test Loss: 0.3304\n",
            "Epoch [19], Batch [56/157], test Loss: 0.6327\n",
            "Epoch [19], Batch [57/157], test Loss: 0.5376\n",
            "Epoch [19], Batch [58/157], test Loss: 0.3415\n",
            "Epoch [19], Batch [59/157], test Loss: 0.4076\n",
            "Epoch [19], Batch [60/157], test Loss: 0.5241\n",
            "Epoch [19], Batch [61/157], test Loss: 0.5473\n",
            "Epoch [19], Batch [62/157], test Loss: 0.4258\n",
            "Epoch [19], Batch [63/157], test Loss: 0.3288\n",
            "Epoch [19], Batch [64/157], test Loss: 0.5029\n",
            "Epoch [19], Batch [65/157], test Loss: 0.5394\n",
            "Epoch [19], Batch [66/157], test Loss: 0.2907\n",
            "Epoch [19], Batch [67/157], test Loss: 0.4423\n",
            "Epoch [19], Batch [68/157], test Loss: 0.5610\n",
            "Epoch [19], Batch [69/157], test Loss: 0.3131\n",
            "Epoch [19], Batch [70/157], test Loss: 0.5824\n",
            "Epoch [19], Batch [71/157], test Loss: 0.5733\n",
            "Epoch [19], Batch [72/157], test Loss: 0.8156\n",
            "Epoch [19], Batch [73/157], test Loss: 0.3516\n",
            "Epoch [19], Batch [74/157], test Loss: 0.4179\n",
            "Epoch [19], Batch [75/157], test Loss: 0.6060\n",
            "Epoch [19], Batch [76/157], test Loss: 0.3437\n",
            "Epoch [19], Batch [77/157], test Loss: 0.4294\n",
            "Epoch [19], Batch [78/157], test Loss: 0.4756\n",
            "Epoch [19], Batch [79/157], test Loss: 0.5056\n",
            "Epoch [19], Batch [80/157], test Loss: 0.4452\n",
            "Epoch [19], Batch [81/157], test Loss: 0.4044\n",
            "Epoch [19], Batch [82/157], test Loss: 0.4307\n",
            "Epoch [19], Batch [83/157], test Loss: 0.4631\n",
            "Epoch [19], Batch [84/157], test Loss: 0.3743\n",
            "Epoch [19], Batch [85/157], test Loss: 0.7780\n",
            "Epoch [19], Batch [86/157], test Loss: 0.3832\n",
            "Epoch [19], Batch [87/157], test Loss: 0.5718\n",
            "Epoch [19], Batch [88/157], test Loss: 0.6790\n",
            "Epoch [19], Batch [89/157], test Loss: 0.3168\n",
            "Epoch [19], Batch [90/157], test Loss: 0.3585\n",
            "Epoch [19], Batch [91/157], test Loss: 0.2807\n",
            "Epoch [19], Batch [92/157], test Loss: 0.2907\n",
            "Epoch [19], Batch [93/157], test Loss: 0.4822\n",
            "Epoch [19], Batch [94/157], test Loss: 0.4090\n",
            "Epoch [19], Batch [95/157], test Loss: 0.5257\n",
            "Epoch [19], Batch [96/157], test Loss: 0.6593\n",
            "Epoch [19], Batch [97/157], test Loss: 0.6571\n",
            "Epoch [19], Batch [98/157], test Loss: 0.3941\n",
            "Epoch [19], Batch [99/157], test Loss: 0.5625\n",
            "Epoch [19], Batch [100/157], test Loss: 0.3912\n",
            "Epoch [19], Batch [101/157], test Loss: 0.4172\n",
            "Epoch [19], Batch [102/157], test Loss: 0.4283\n",
            "Epoch [19], Batch [103/157], test Loss: 0.3515\n",
            "Epoch [19], Batch [104/157], test Loss: 0.6790\n",
            "Epoch [19], Batch [105/157], test Loss: 0.5255\n",
            "Epoch [19], Batch [106/157], test Loss: 0.4493\n",
            "Epoch [19], Batch [107/157], test Loss: 0.5204\n",
            "Epoch [19], Batch [108/157], test Loss: 0.5293\n",
            "Epoch [19], Batch [109/157], test Loss: 0.4939\n",
            "Epoch [19], Batch [110/157], test Loss: 0.3607\n",
            "Epoch [19], Batch [111/157], test Loss: 0.6667\n",
            "Epoch [19], Batch [112/157], test Loss: 0.4732\n",
            "Epoch [19], Batch [113/157], test Loss: 0.4147\n",
            "Epoch [19], Batch [114/157], test Loss: 0.5050\n",
            "Epoch [19], Batch [115/157], test Loss: 0.4020\n",
            "Epoch [19], Batch [116/157], test Loss: 0.3618\n",
            "Epoch [19], Batch [117/157], test Loss: 0.4879\n",
            "Epoch [19], Batch [118/157], test Loss: 0.4072\n",
            "Epoch [19], Batch [119/157], test Loss: 0.3779\n",
            "Epoch [19], Batch [120/157], test Loss: 0.7009\n",
            "Epoch [19], Batch [121/157], test Loss: 0.4089\n",
            "Epoch [19], Batch [122/157], test Loss: 0.5396\n",
            "Epoch [19], Batch [123/157], test Loss: 0.3060\n",
            "Epoch [19], Batch [124/157], test Loss: 0.6449\n",
            "Epoch [19], Batch [125/157], test Loss: 0.4109\n",
            "Epoch [19], Batch [126/157], test Loss: 0.3445\n",
            "Epoch [19], Batch [127/157], test Loss: 0.5547\n",
            "Epoch [19], Batch [128/157], test Loss: 0.4551\n",
            "Epoch [19], Batch [129/157], test Loss: 0.4053\n",
            "Epoch [19], Batch [130/157], test Loss: 0.4733\n",
            "Epoch [19], Batch [131/157], test Loss: 0.4936\n",
            "Epoch [19], Batch [132/157], test Loss: 0.4532\n",
            "Epoch [19], Batch [133/157], test Loss: 0.2870\n",
            "Epoch [19], Batch [134/157], test Loss: 0.4789\n",
            "Epoch [19], Batch [135/157], test Loss: 0.5011\n",
            "Epoch [19], Batch [136/157], test Loss: 0.6034\n",
            "Epoch [19], Batch [137/157], test Loss: 0.4088\n",
            "Epoch [19], Batch [138/157], test Loss: 0.4439\n",
            "Epoch [19], Batch [139/157], test Loss: 0.7293\n",
            "Epoch [19], Batch [140/157], test Loss: 0.4688\n",
            "Epoch [19], Batch [141/157], test Loss: 0.5840\n",
            "Epoch [19], Batch [142/157], test Loss: 0.4061\n",
            "Epoch [19], Batch [143/157], test Loss: 0.4717\n",
            "Epoch [19], Batch [144/157], test Loss: 0.4235\n",
            "Epoch [19], Batch [145/157], test Loss: 0.4718\n",
            "Epoch [19], Batch [146/157], test Loss: 0.4054\n",
            "Epoch [19], Batch [147/157], test Loss: 0.4351\n",
            "Epoch [19], Batch [148/157], test Loss: 0.5672\n",
            "Epoch [19], Batch [149/157], test Loss: 0.3440\n",
            "Epoch [19], Batch [150/157], test Loss: 0.3742\n",
            "Epoch [19], Batch [151/157], test Loss: 0.4861\n",
            "Epoch [19], Batch [152/157], test Loss: 0.6241\n",
            "Epoch [19], Batch [153/157], test Loss: 0.7783\n",
            "Epoch [19], Batch [154/157], test Loss: 0.5985\n",
            "Epoch [19], Batch [155/157], test Loss: 0.4519\n",
            "Epoch [19], Batch [156/157], test Loss: 0.7019\n",
            "Epoch [19], Batch [157/157], test Loss: 0.3274\n",
            "Accuracy of test set: 83.32%\n",
            "Epoch [20/25] - Train Loss: 0.4358, Train Accuracy: 84.63% - Test Loss: 0.4732, Test Accuracy: 83.32%\n",
            "Epoch [20], Batch [10/938], Loss: 0.3430\n",
            "Epoch [20], Batch [20/938], Loss: 0.4238\n",
            "Epoch [20], Batch [30/938], Loss: 0.4409\n",
            "Epoch [20], Batch [40/938], Loss: 0.3940\n",
            "Epoch [20], Batch [50/938], Loss: 0.3257\n",
            "Epoch [20], Batch [60/938], Loss: 0.4982\n",
            "Epoch [20], Batch [70/938], Loss: 0.3661\n",
            "Epoch [20], Batch [80/938], Loss: 0.3535\n",
            "Epoch [20], Batch [90/938], Loss: 0.2845\n",
            "Epoch [20], Batch [100/938], Loss: 0.4837\n",
            "Epoch [20], Batch [110/938], Loss: 0.5498\n",
            "Epoch [20], Batch [120/938], Loss: 0.3979\n",
            "Epoch [20], Batch [130/938], Loss: 0.3670\n",
            "Epoch [20], Batch [140/938], Loss: 0.6040\n",
            "Epoch [20], Batch [150/938], Loss: 0.3760\n",
            "Epoch [20], Batch [160/938], Loss: 0.3769\n",
            "Epoch [20], Batch [170/938], Loss: 0.5800\n",
            "Epoch [20], Batch [180/938], Loss: 0.5472\n",
            "Epoch [20], Batch [190/938], Loss: 0.4341\n",
            "Epoch [20], Batch [200/938], Loss: 0.4046\n",
            "Epoch [20], Batch [210/938], Loss: 0.2781\n",
            "Epoch [20], Batch [220/938], Loss: 0.4817\n",
            "Epoch [20], Batch [230/938], Loss: 0.2690\n",
            "Epoch [20], Batch [240/938], Loss: 0.3809\n",
            "Epoch [20], Batch [250/938], Loss: 0.2986\n",
            "Epoch [20], Batch [260/938], Loss: 0.4593\n",
            "Epoch [20], Batch [270/938], Loss: 0.3638\n",
            "Epoch [20], Batch [280/938], Loss: 0.5307\n",
            "Epoch [20], Batch [290/938], Loss: 0.3096\n",
            "Epoch [20], Batch [300/938], Loss: 0.5195\n",
            "Epoch [20], Batch [310/938], Loss: 0.3425\n",
            "Epoch [20], Batch [320/938], Loss: 0.4182\n",
            "Epoch [20], Batch [330/938], Loss: 0.3886\n",
            "Epoch [20], Batch [340/938], Loss: 0.4570\n",
            "Epoch [20], Batch [350/938], Loss: 0.3961\n",
            "Epoch [20], Batch [360/938], Loss: 0.3830\n",
            "Epoch [20], Batch [370/938], Loss: 0.7124\n",
            "Epoch [20], Batch [380/938], Loss: 0.2625\n",
            "Epoch [20], Batch [390/938], Loss: 0.4279\n",
            "Epoch [20], Batch [400/938], Loss: 0.3556\n",
            "Epoch [20], Batch [410/938], Loss: 0.3153\n",
            "Epoch [20], Batch [420/938], Loss: 0.3110\n",
            "Epoch [20], Batch [430/938], Loss: 0.3263\n",
            "Epoch [20], Batch [440/938], Loss: 0.4701\n",
            "Epoch [20], Batch [450/938], Loss: 0.5533\n",
            "Epoch [20], Batch [460/938], Loss: 0.3822\n",
            "Epoch [20], Batch [470/938], Loss: 0.4131\n",
            "Epoch [20], Batch [480/938], Loss: 0.4152\n",
            "Epoch [20], Batch [490/938], Loss: 0.4983\n",
            "Epoch [20], Batch [500/938], Loss: 0.5683\n",
            "Epoch [20], Batch [510/938], Loss: 0.4294\n",
            "Epoch [20], Batch [520/938], Loss: 0.4100\n",
            "Epoch [20], Batch [530/938], Loss: 0.3476\n",
            "Epoch [20], Batch [540/938], Loss: 0.4085\n",
            "Epoch [20], Batch [550/938], Loss: 0.6509\n",
            "Epoch [20], Batch [560/938], Loss: 0.5647\n",
            "Epoch [20], Batch [570/938], Loss: 0.4814\n",
            "Epoch [20], Batch [580/938], Loss: 0.3576\n",
            "Epoch [20], Batch [590/938], Loss: 0.4744\n",
            "Epoch [20], Batch [600/938], Loss: 0.2556\n",
            "Epoch [20], Batch [610/938], Loss: 0.6145\n",
            "Epoch [20], Batch [620/938], Loss: 0.2237\n",
            "Epoch [20], Batch [630/938], Loss: 0.5914\n",
            "Epoch [20], Batch [640/938], Loss: 0.3633\n",
            "Epoch [20], Batch [650/938], Loss: 0.3626\n",
            "Epoch [20], Batch [660/938], Loss: 0.4225\n",
            "Epoch [20], Batch [670/938], Loss: 0.3362\n",
            "Epoch [20], Batch [680/938], Loss: 0.5633\n",
            "Epoch [20], Batch [690/938], Loss: 0.7002\n",
            "Epoch [20], Batch [700/938], Loss: 0.2152\n",
            "Epoch [20], Batch [710/938], Loss: 0.3656\n",
            "Epoch [20], Batch [720/938], Loss: 0.3570\n",
            "Epoch [20], Batch [730/938], Loss: 0.5462\n",
            "Epoch [20], Batch [740/938], Loss: 0.2675\n",
            "Epoch [20], Batch [750/938], Loss: 0.4808\n",
            "Epoch [20], Batch [760/938], Loss: 0.5166\n",
            "Epoch [20], Batch [770/938], Loss: 0.2401\n",
            "Epoch [20], Batch [780/938], Loss: 0.4370\n",
            "Epoch [20], Batch [790/938], Loss: 0.5658\n",
            "Epoch [20], Batch [800/938], Loss: 0.2978\n",
            "Epoch [20], Batch [810/938], Loss: 0.4919\n",
            "Epoch [20], Batch [820/938], Loss: 0.4126\n",
            "Epoch [20], Batch [830/938], Loss: 0.5010\n",
            "Epoch [20], Batch [840/938], Loss: 0.4467\n",
            "Epoch [20], Batch [850/938], Loss: 0.5314\n",
            "Epoch [20], Batch [860/938], Loss: 0.3770\n",
            "Epoch [20], Batch [870/938], Loss: 0.5336\n",
            "Epoch [20], Batch [880/938], Loss: 0.4460\n",
            "Epoch [20], Batch [890/938], Loss: 0.3537\n",
            "Epoch [20], Batch [900/938], Loss: 0.3018\n",
            "Epoch [20], Batch [910/938], Loss: 0.4358\n",
            "Epoch [20], Batch [920/938], Loss: 0.3744\n",
            "Epoch [20], Batch [930/938], Loss: 0.4499\n",
            "Epoch [20], Batch [938/938], Loss: 0.6006\n",
            "Accuracy of train set: 84.90%\n",
            "Epoch [20], Batch [1/157], test Loss: 0.6162\n",
            "Epoch [20], Batch [2/157], test Loss: 0.4546\n",
            "Epoch [20], Batch [3/157], test Loss: 0.4767\n",
            "Epoch [20], Batch [4/157], test Loss: 0.5744\n",
            "Epoch [20], Batch [5/157], test Loss: 0.4972\n",
            "Epoch [20], Batch [6/157], test Loss: 0.6155\n",
            "Epoch [20], Batch [7/157], test Loss: 0.7949\n",
            "Epoch [20], Batch [8/157], test Loss: 0.6905\n",
            "Epoch [20], Batch [9/157], test Loss: 0.5245\n",
            "Epoch [20], Batch [10/157], test Loss: 0.3670\n",
            "Epoch [20], Batch [11/157], test Loss: 0.4666\n",
            "Epoch [20], Batch [12/157], test Loss: 0.5159\n",
            "Epoch [20], Batch [13/157], test Loss: 0.3881\n",
            "Epoch [20], Batch [14/157], test Loss: 0.3830\n",
            "Epoch [20], Batch [15/157], test Loss: 0.4164\n",
            "Epoch [20], Batch [16/157], test Loss: 0.7034\n",
            "Epoch [20], Batch [17/157], test Loss: 0.4231\n",
            "Epoch [20], Batch [18/157], test Loss: 0.4781\n",
            "Epoch [20], Batch [19/157], test Loss: 0.4300\n",
            "Epoch [20], Batch [20/157], test Loss: 0.3895\n",
            "Epoch [20], Batch [21/157], test Loss: 0.5093\n",
            "Epoch [20], Batch [22/157], test Loss: 0.3202\n",
            "Epoch [20], Batch [23/157], test Loss: 0.5510\n",
            "Epoch [20], Batch [24/157], test Loss: 0.5210\n",
            "Epoch [20], Batch [25/157], test Loss: 0.5975\n",
            "Epoch [20], Batch [26/157], test Loss: 0.5925\n",
            "Epoch [20], Batch [27/157], test Loss: 0.5084\n",
            "Epoch [20], Batch [28/157], test Loss: 0.4685\n",
            "Epoch [20], Batch [29/157], test Loss: 0.5153\n",
            "Epoch [20], Batch [30/157], test Loss: 0.6731\n",
            "Epoch [20], Batch [31/157], test Loss: 0.4418\n",
            "Epoch [20], Batch [32/157], test Loss: 0.3681\n",
            "Epoch [20], Batch [33/157], test Loss: 0.5141\n",
            "Epoch [20], Batch [34/157], test Loss: 0.4436\n",
            "Epoch [20], Batch [35/157], test Loss: 0.5072\n",
            "Epoch [20], Batch [36/157], test Loss: 0.5873\n",
            "Epoch [20], Batch [37/157], test Loss: 0.4833\n",
            "Epoch [20], Batch [38/157], test Loss: 0.3825\n",
            "Epoch [20], Batch [39/157], test Loss: 0.5453\n",
            "Epoch [20], Batch [40/157], test Loss: 0.5089\n",
            "Epoch [20], Batch [41/157], test Loss: 0.5419\n",
            "Epoch [20], Batch [42/157], test Loss: 0.3606\n",
            "Epoch [20], Batch [43/157], test Loss: 0.4934\n",
            "Epoch [20], Batch [44/157], test Loss: 0.5898\n",
            "Epoch [20], Batch [45/157], test Loss: 0.3976\n",
            "Epoch [20], Batch [46/157], test Loss: 0.4970\n",
            "Epoch [20], Batch [47/157], test Loss: 0.5515\n",
            "Epoch [20], Batch [48/157], test Loss: 0.5319\n",
            "Epoch [20], Batch [49/157], test Loss: 0.5701\n",
            "Epoch [20], Batch [50/157], test Loss: 0.3780\n",
            "Epoch [20], Batch [51/157], test Loss: 0.4003\n",
            "Epoch [20], Batch [52/157], test Loss: 0.4558\n",
            "Epoch [20], Batch [53/157], test Loss: 0.6041\n",
            "Epoch [20], Batch [54/157], test Loss: 0.3297\n",
            "Epoch [20], Batch [55/157], test Loss: 0.4434\n",
            "Epoch [20], Batch [56/157], test Loss: 0.6820\n",
            "Epoch [20], Batch [57/157], test Loss: 0.4589\n",
            "Epoch [20], Batch [58/157], test Loss: 0.4135\n",
            "Epoch [20], Batch [59/157], test Loss: 0.3523\n",
            "Epoch [20], Batch [60/157], test Loss: 0.6724\n",
            "Epoch [20], Batch [61/157], test Loss: 0.6925\n",
            "Epoch [20], Batch [62/157], test Loss: 0.5475\n",
            "Epoch [20], Batch [63/157], test Loss: 0.4414\n",
            "Epoch [20], Batch [64/157], test Loss: 0.4608\n",
            "Epoch [20], Batch [65/157], test Loss: 0.3733\n",
            "Epoch [20], Batch [66/157], test Loss: 0.5188\n",
            "Epoch [20], Batch [67/157], test Loss: 0.4953\n",
            "Epoch [20], Batch [68/157], test Loss: 0.3922\n",
            "Epoch [20], Batch [69/157], test Loss: 0.3471\n",
            "Epoch [20], Batch [70/157], test Loss: 0.5827\n",
            "Epoch [20], Batch [71/157], test Loss: 0.5668\n",
            "Epoch [20], Batch [72/157], test Loss: 0.2801\n",
            "Epoch [20], Batch [73/157], test Loss: 0.6713\n",
            "Epoch [20], Batch [74/157], test Loss: 0.6538\n",
            "Epoch [20], Batch [75/157], test Loss: 0.4094\n",
            "Epoch [20], Batch [76/157], test Loss: 0.5433\n",
            "Epoch [20], Batch [77/157], test Loss: 0.4283\n",
            "Epoch [20], Batch [78/157], test Loss: 0.4116\n",
            "Epoch [20], Batch [79/157], test Loss: 0.4647\n",
            "Epoch [20], Batch [80/157], test Loss: 0.7094\n",
            "Epoch [20], Batch [81/157], test Loss: 0.5720\n",
            "Epoch [20], Batch [82/157], test Loss: 0.4171\n",
            "Epoch [20], Batch [83/157], test Loss: 0.5410\n",
            "Epoch [20], Batch [84/157], test Loss: 0.6963\n",
            "Epoch [20], Batch [85/157], test Loss: 0.3995\n",
            "Epoch [20], Batch [86/157], test Loss: 0.5120\n",
            "Epoch [20], Batch [87/157], test Loss: 0.3099\n",
            "Epoch [20], Batch [88/157], test Loss: 0.4743\n",
            "Epoch [20], Batch [89/157], test Loss: 0.4741\n",
            "Epoch [20], Batch [90/157], test Loss: 0.6185\n",
            "Epoch [20], Batch [91/157], test Loss: 0.3338\n",
            "Epoch [20], Batch [92/157], test Loss: 0.5753\n",
            "Epoch [20], Batch [93/157], test Loss: 0.4433\n",
            "Epoch [20], Batch [94/157], test Loss: 0.5930\n",
            "Epoch [20], Batch [95/157], test Loss: 0.5241\n",
            "Epoch [20], Batch [96/157], test Loss: 0.4814\n",
            "Epoch [20], Batch [97/157], test Loss: 0.3661\n",
            "Epoch [20], Batch [98/157], test Loss: 0.6127\n",
            "Epoch [20], Batch [99/157], test Loss: 0.5841\n",
            "Epoch [20], Batch [100/157], test Loss: 0.4257\n",
            "Epoch [20], Batch [101/157], test Loss: 0.5339\n",
            "Epoch [20], Batch [102/157], test Loss: 0.4111\n",
            "Epoch [20], Batch [103/157], test Loss: 0.6648\n",
            "Epoch [20], Batch [104/157], test Loss: 0.4490\n",
            "Epoch [20], Batch [105/157], test Loss: 0.5715\n",
            "Epoch [20], Batch [106/157], test Loss: 0.4164\n",
            "Epoch [20], Batch [107/157], test Loss: 0.5506\n",
            "Epoch [20], Batch [108/157], test Loss: 0.5789\n",
            "Epoch [20], Batch [109/157], test Loss: 0.6094\n",
            "Epoch [20], Batch [110/157], test Loss: 0.4345\n",
            "Epoch [20], Batch [111/157], test Loss: 0.4619\n",
            "Epoch [20], Batch [112/157], test Loss: 0.5571\n",
            "Epoch [20], Batch [113/157], test Loss: 0.5853\n",
            "Epoch [20], Batch [114/157], test Loss: 0.4934\n",
            "Epoch [20], Batch [115/157], test Loss: 0.4570\n",
            "Epoch [20], Batch [116/157], test Loss: 0.3458\n",
            "Epoch [20], Batch [117/157], test Loss: 0.4051\n",
            "Epoch [20], Batch [118/157], test Loss: 0.9371\n",
            "Epoch [20], Batch [119/157], test Loss: 0.4381\n",
            "Epoch [20], Batch [120/157], test Loss: 0.5209\n",
            "Epoch [20], Batch [121/157], test Loss: 0.5298\n",
            "Epoch [20], Batch [122/157], test Loss: 0.3778\n",
            "Epoch [20], Batch [123/157], test Loss: 0.3825\n",
            "Epoch [20], Batch [124/157], test Loss: 0.4463\n",
            "Epoch [20], Batch [125/157], test Loss: 0.4805\n",
            "Epoch [20], Batch [126/157], test Loss: 0.5611\n",
            "Epoch [20], Batch [127/157], test Loss: 0.5043\n",
            "Epoch [20], Batch [128/157], test Loss: 0.6222\n",
            "Epoch [20], Batch [129/157], test Loss: 0.4752\n",
            "Epoch [20], Batch [130/157], test Loss: 0.5670\n",
            "Epoch [20], Batch [131/157], test Loss: 0.4680\n",
            "Epoch [20], Batch [132/157], test Loss: 0.3282\n",
            "Epoch [20], Batch [133/157], test Loss: 0.4890\n",
            "Epoch [20], Batch [134/157], test Loss: 0.4399\n",
            "Epoch [20], Batch [135/157], test Loss: 0.3672\n",
            "Epoch [20], Batch [136/157], test Loss: 0.4384\n",
            "Epoch [20], Batch [137/157], test Loss: 0.5045\n",
            "Epoch [20], Batch [138/157], test Loss: 0.3141\n",
            "Epoch [20], Batch [139/157], test Loss: 0.3731\n",
            "Epoch [20], Batch [140/157], test Loss: 0.4601\n",
            "Epoch [20], Batch [141/157], test Loss: 0.4565\n",
            "Epoch [20], Batch [142/157], test Loss: 0.4379\n",
            "Epoch [20], Batch [143/157], test Loss: 0.3760\n",
            "Epoch [20], Batch [144/157], test Loss: 0.3897\n",
            "Epoch [20], Batch [145/157], test Loss: 0.5465\n",
            "Epoch [20], Batch [146/157], test Loss: 0.4755\n",
            "Epoch [20], Batch [147/157], test Loss: 0.3568\n",
            "Epoch [20], Batch [148/157], test Loss: 0.3856\n",
            "Epoch [20], Batch [149/157], test Loss: 0.3259\n",
            "Epoch [20], Batch [150/157], test Loss: 0.5744\n",
            "Epoch [20], Batch [151/157], test Loss: 0.5843\n",
            "Epoch [20], Batch [152/157], test Loss: 0.5980\n",
            "Epoch [20], Batch [153/157], test Loss: 0.3222\n",
            "Epoch [20], Batch [154/157], test Loss: 0.7376\n",
            "Epoch [20], Batch [155/157], test Loss: 0.4465\n",
            "Epoch [20], Batch [156/157], test Loss: 0.6817\n",
            "Epoch [20], Batch [157/157], test Loss: 0.3054\n",
            "Accuracy of test set: 81.72%\n",
            "Epoch [21/25] - Train Loss: 0.4284, Train Accuracy: 84.90% - Test Loss: 0.4934, Test Accuracy: 81.72%\n",
            "Epoch [21], Batch [10/938], Loss: 0.3898\n",
            "Epoch [21], Batch [20/938], Loss: 0.4544\n",
            "Epoch [21], Batch [30/938], Loss: 0.4913\n",
            "Epoch [21], Batch [40/938], Loss: 0.3825\n",
            "Epoch [21], Batch [50/938], Loss: 0.3029\n",
            "Epoch [21], Batch [60/938], Loss: 0.1984\n",
            "Epoch [21], Batch [70/938], Loss: 0.6166\n",
            "Epoch [21], Batch [80/938], Loss: 0.3266\n",
            "Epoch [21], Batch [90/938], Loss: 0.5418\n",
            "Epoch [21], Batch [100/938], Loss: 0.5703\n",
            "Epoch [21], Batch [110/938], Loss: 0.3260\n",
            "Epoch [21], Batch [120/938], Loss: 0.2583\n",
            "Epoch [21], Batch [130/938], Loss: 0.4643\n",
            "Epoch [21], Batch [140/938], Loss: 0.5929\n",
            "Epoch [21], Batch [150/938], Loss: 0.4604\n",
            "Epoch [21], Batch [160/938], Loss: 0.4112\n",
            "Epoch [21], Batch [170/938], Loss: 0.4042\n",
            "Epoch [21], Batch [180/938], Loss: 0.2990\n",
            "Epoch [21], Batch [190/938], Loss: 0.6660\n",
            "Epoch [21], Batch [200/938], Loss: 0.4232\n",
            "Epoch [21], Batch [210/938], Loss: 0.2620\n",
            "Epoch [21], Batch [220/938], Loss: 0.4068\n",
            "Epoch [21], Batch [230/938], Loss: 0.3713\n",
            "Epoch [21], Batch [240/938], Loss: 0.3129\n",
            "Epoch [21], Batch [250/938], Loss: 0.4894\n",
            "Epoch [21], Batch [260/938], Loss: 0.3545\n",
            "Epoch [21], Batch [270/938], Loss: 0.2155\n",
            "Epoch [21], Batch [280/938], Loss: 0.4016\n",
            "Epoch [21], Batch [290/938], Loss: 0.4269\n",
            "Epoch [21], Batch [300/938], Loss: 0.4247\n",
            "Epoch [21], Batch [310/938], Loss: 0.4431\n",
            "Epoch [21], Batch [320/938], Loss: 0.3779\n",
            "Epoch [21], Batch [330/938], Loss: 0.4455\n",
            "Epoch [21], Batch [340/938], Loss: 0.5398\n",
            "Epoch [21], Batch [350/938], Loss: 0.2956\n",
            "Epoch [21], Batch [360/938], Loss: 0.4352\n",
            "Epoch [21], Batch [370/938], Loss: 0.3799\n",
            "Epoch [21], Batch [380/938], Loss: 0.3829\n",
            "Epoch [21], Batch [390/938], Loss: 0.4734\n",
            "Epoch [21], Batch [400/938], Loss: 0.2991\n",
            "Epoch [21], Batch [410/938], Loss: 0.4960\n",
            "Epoch [21], Batch [420/938], Loss: 0.5805\n",
            "Epoch [21], Batch [430/938], Loss: 0.3729\n",
            "Epoch [21], Batch [440/938], Loss: 0.5289\n",
            "Epoch [21], Batch [450/938], Loss: 0.6020\n",
            "Epoch [21], Batch [460/938], Loss: 0.4479\n",
            "Epoch [21], Batch [470/938], Loss: 0.3078\n",
            "Epoch [21], Batch [480/938], Loss: 0.3627\n",
            "Epoch [21], Batch [490/938], Loss: 0.4453\n",
            "Epoch [21], Batch [500/938], Loss: 0.3217\n",
            "Epoch [21], Batch [510/938], Loss: 0.3821\n",
            "Epoch [21], Batch [520/938], Loss: 0.5154\n",
            "Epoch [21], Batch [530/938], Loss: 0.4369\n",
            "Epoch [21], Batch [540/938], Loss: 0.4284\n",
            "Epoch [21], Batch [550/938], Loss: 0.2671\n",
            "Epoch [21], Batch [560/938], Loss: 0.4186\n",
            "Epoch [21], Batch [570/938], Loss: 0.4839\n",
            "Epoch [21], Batch [580/938], Loss: 0.4652\n",
            "Epoch [21], Batch [590/938], Loss: 0.3832\n",
            "Epoch [21], Batch [600/938], Loss: 0.4158\n",
            "Epoch [21], Batch [610/938], Loss: 0.4821\n",
            "Epoch [21], Batch [620/938], Loss: 0.4518\n",
            "Epoch [21], Batch [630/938], Loss: 0.4327\n",
            "Epoch [21], Batch [640/938], Loss: 0.4341\n",
            "Epoch [21], Batch [650/938], Loss: 0.5204\n",
            "Epoch [21], Batch [660/938], Loss: 0.3593\n",
            "Epoch [21], Batch [670/938], Loss: 0.3262\n",
            "Epoch [21], Batch [680/938], Loss: 0.2741\n",
            "Epoch [21], Batch [690/938], Loss: 0.4697\n",
            "Epoch [21], Batch [700/938], Loss: 0.2919\n",
            "Epoch [21], Batch [710/938], Loss: 0.4943\n",
            "Epoch [21], Batch [720/938], Loss: 0.3132\n",
            "Epoch [21], Batch [730/938], Loss: 0.4393\n",
            "Epoch [21], Batch [740/938], Loss: 0.2796\n",
            "Epoch [21], Batch [750/938], Loss: 0.3537\n",
            "Epoch [21], Batch [760/938], Loss: 0.4344\n",
            "Epoch [21], Batch [770/938], Loss: 0.5532\n",
            "Epoch [21], Batch [780/938], Loss: 0.2561\n",
            "Epoch [21], Batch [790/938], Loss: 0.4994\n",
            "Epoch [21], Batch [800/938], Loss: 0.5042\n",
            "Epoch [21], Batch [810/938], Loss: 0.3884\n",
            "Epoch [21], Batch [820/938], Loss: 0.4390\n",
            "Epoch [21], Batch [830/938], Loss: 0.5118\n",
            "Epoch [21], Batch [840/938], Loss: 0.3430\n",
            "Epoch [21], Batch [850/938], Loss: 0.2737\n",
            "Epoch [21], Batch [860/938], Loss: 0.3846\n",
            "Epoch [21], Batch [870/938], Loss: 0.3300\n",
            "Epoch [21], Batch [880/938], Loss: 0.5533\n",
            "Epoch [21], Batch [890/938], Loss: 0.5547\n",
            "Epoch [21], Batch [900/938], Loss: 0.2552\n",
            "Epoch [21], Batch [910/938], Loss: 0.2573\n",
            "Epoch [21], Batch [920/938], Loss: 0.4694\n",
            "Epoch [21], Batch [930/938], Loss: 0.2650\n",
            "Epoch [21], Batch [938/938], Loss: 0.5380\n",
            "Accuracy of train set: 85.20%\n",
            "Epoch [21], Batch [1/157], test Loss: 0.5443\n",
            "Epoch [21], Batch [2/157], test Loss: 0.3804\n",
            "Epoch [21], Batch [3/157], test Loss: 0.6784\n",
            "Epoch [21], Batch [4/157], test Loss: 0.4374\n",
            "Epoch [21], Batch [5/157], test Loss: 0.2835\n",
            "Epoch [21], Batch [6/157], test Loss: 0.3299\n",
            "Epoch [21], Batch [7/157], test Loss: 0.4069\n",
            "Epoch [21], Batch [8/157], test Loss: 0.5066\n",
            "Epoch [21], Batch [9/157], test Loss: 0.3694\n",
            "Epoch [21], Batch [10/157], test Loss: 0.4247\n",
            "Epoch [21], Batch [11/157], test Loss: 0.4435\n",
            "Epoch [21], Batch [12/157], test Loss: 0.4537\n",
            "Epoch [21], Batch [13/157], test Loss: 0.3258\n",
            "Epoch [21], Batch [14/157], test Loss: 0.4329\n",
            "Epoch [21], Batch [15/157], test Loss: 0.7248\n",
            "Epoch [21], Batch [16/157], test Loss: 0.4849\n",
            "Epoch [21], Batch [17/157], test Loss: 0.5548\n",
            "Epoch [21], Batch [18/157], test Loss: 0.4488\n",
            "Epoch [21], Batch [19/157], test Loss: 0.2826\n",
            "Epoch [21], Batch [20/157], test Loss: 0.3744\n",
            "Epoch [21], Batch [21/157], test Loss: 0.3087\n",
            "Epoch [21], Batch [22/157], test Loss: 0.3563\n",
            "Epoch [21], Batch [23/157], test Loss: 0.3718\n",
            "Epoch [21], Batch [24/157], test Loss: 0.5203\n",
            "Epoch [21], Batch [25/157], test Loss: 0.4068\n",
            "Epoch [21], Batch [26/157], test Loss: 0.5080\n",
            "Epoch [21], Batch [27/157], test Loss: 0.5088\n",
            "Epoch [21], Batch [28/157], test Loss: 0.3243\n",
            "Epoch [21], Batch [29/157], test Loss: 0.3911\n",
            "Epoch [21], Batch [30/157], test Loss: 0.4508\n",
            "Epoch [21], Batch [31/157], test Loss: 0.5850\n",
            "Epoch [21], Batch [32/157], test Loss: 0.4989\n",
            "Epoch [21], Batch [33/157], test Loss: 0.8310\n",
            "Epoch [21], Batch [34/157], test Loss: 0.3190\n",
            "Epoch [21], Batch [35/157], test Loss: 0.6688\n",
            "Epoch [21], Batch [36/157], test Loss: 0.3646\n",
            "Epoch [21], Batch [37/157], test Loss: 0.5411\n",
            "Epoch [21], Batch [38/157], test Loss: 0.5136\n",
            "Epoch [21], Batch [39/157], test Loss: 0.4340\n",
            "Epoch [21], Batch [40/157], test Loss: 0.4977\n",
            "Epoch [21], Batch [41/157], test Loss: 0.3629\n",
            "Epoch [21], Batch [42/157], test Loss: 0.6368\n",
            "Epoch [21], Batch [43/157], test Loss: 0.3933\n",
            "Epoch [21], Batch [44/157], test Loss: 0.3832\n",
            "Epoch [21], Batch [45/157], test Loss: 0.5912\n",
            "Epoch [21], Batch [46/157], test Loss: 0.4971\n",
            "Epoch [21], Batch [47/157], test Loss: 0.6383\n",
            "Epoch [21], Batch [48/157], test Loss: 0.3889\n",
            "Epoch [21], Batch [49/157], test Loss: 0.4569\n",
            "Epoch [21], Batch [50/157], test Loss: 0.6098\n",
            "Epoch [21], Batch [51/157], test Loss: 0.4232\n",
            "Epoch [21], Batch [52/157], test Loss: 0.4094\n",
            "Epoch [21], Batch [53/157], test Loss: 0.5404\n",
            "Epoch [21], Batch [54/157], test Loss: 0.4520\n",
            "Epoch [21], Batch [55/157], test Loss: 0.4890\n",
            "Epoch [21], Batch [56/157], test Loss: 0.4235\n",
            "Epoch [21], Batch [57/157], test Loss: 0.4314\n",
            "Epoch [21], Batch [58/157], test Loss: 0.3829\n",
            "Epoch [21], Batch [59/157], test Loss: 0.4881\n",
            "Epoch [21], Batch [60/157], test Loss: 0.3956\n",
            "Epoch [21], Batch [61/157], test Loss: 0.3750\n",
            "Epoch [21], Batch [62/157], test Loss: 0.3148\n",
            "Epoch [21], Batch [63/157], test Loss: 0.3715\n",
            "Epoch [21], Batch [64/157], test Loss: 0.5376\n",
            "Epoch [21], Batch [65/157], test Loss: 0.5532\n",
            "Epoch [21], Batch [66/157], test Loss: 0.5640\n",
            "Epoch [21], Batch [67/157], test Loss: 0.3189\n",
            "Epoch [21], Batch [68/157], test Loss: 0.4526\n",
            "Epoch [21], Batch [69/157], test Loss: 0.4433\n",
            "Epoch [21], Batch [70/157], test Loss: 0.4460\n",
            "Epoch [21], Batch [71/157], test Loss: 0.4957\n",
            "Epoch [21], Batch [72/157], test Loss: 0.4329\n",
            "Epoch [21], Batch [73/157], test Loss: 0.5236\n",
            "Epoch [21], Batch [74/157], test Loss: 0.4478\n",
            "Epoch [21], Batch [75/157], test Loss: 0.2751\n",
            "Epoch [21], Batch [76/157], test Loss: 0.4483\n",
            "Epoch [21], Batch [77/157], test Loss: 0.3953\n",
            "Epoch [21], Batch [78/157], test Loss: 0.5993\n",
            "Epoch [21], Batch [79/157], test Loss: 0.4479\n",
            "Epoch [21], Batch [80/157], test Loss: 0.3307\n",
            "Epoch [21], Batch [81/157], test Loss: 0.4282\n",
            "Epoch [21], Batch [82/157], test Loss: 0.5006\n",
            "Epoch [21], Batch [83/157], test Loss: 0.4631\n",
            "Epoch [21], Batch [84/157], test Loss: 0.3498\n",
            "Epoch [21], Batch [85/157], test Loss: 0.4982\n",
            "Epoch [21], Batch [86/157], test Loss: 0.3776\n",
            "Epoch [21], Batch [87/157], test Loss: 0.5116\n",
            "Epoch [21], Batch [88/157], test Loss: 0.4945\n",
            "Epoch [21], Batch [89/157], test Loss: 0.6893\n",
            "Epoch [21], Batch [90/157], test Loss: 0.6287\n",
            "Epoch [21], Batch [91/157], test Loss: 0.4856\n",
            "Epoch [21], Batch [92/157], test Loss: 0.3662\n",
            "Epoch [21], Batch [93/157], test Loss: 0.5198\n",
            "Epoch [21], Batch [94/157], test Loss: 0.4267\n",
            "Epoch [21], Batch [95/157], test Loss: 0.4430\n",
            "Epoch [21], Batch [96/157], test Loss: 0.3691\n",
            "Epoch [21], Batch [97/157], test Loss: 0.2358\n",
            "Epoch [21], Batch [98/157], test Loss: 0.3910\n",
            "Epoch [21], Batch [99/157], test Loss: 0.3181\n",
            "Epoch [21], Batch [100/157], test Loss: 0.4573\n",
            "Epoch [21], Batch [101/157], test Loss: 0.1896\n",
            "Epoch [21], Batch [102/157], test Loss: 0.7755\n",
            "Epoch [21], Batch [103/157], test Loss: 0.4437\n",
            "Epoch [21], Batch [104/157], test Loss: 0.4163\n",
            "Epoch [21], Batch [105/157], test Loss: 0.4827\n",
            "Epoch [21], Batch [106/157], test Loss: 0.5176\n",
            "Epoch [21], Batch [107/157], test Loss: 0.5868\n",
            "Epoch [21], Batch [108/157], test Loss: 0.3954\n",
            "Epoch [21], Batch [109/157], test Loss: 0.5550\n",
            "Epoch [21], Batch [110/157], test Loss: 0.3287\n",
            "Epoch [21], Batch [111/157], test Loss: 0.4463\n",
            "Epoch [21], Batch [112/157], test Loss: 0.5072\n",
            "Epoch [21], Batch [113/157], test Loss: 0.6371\n",
            "Epoch [21], Batch [114/157], test Loss: 0.3016\n",
            "Epoch [21], Batch [115/157], test Loss: 0.2766\n",
            "Epoch [21], Batch [116/157], test Loss: 0.3151\n",
            "Epoch [21], Batch [117/157], test Loss: 0.5893\n",
            "Epoch [21], Batch [118/157], test Loss: 0.3788\n",
            "Epoch [21], Batch [119/157], test Loss: 0.6347\n",
            "Epoch [21], Batch [120/157], test Loss: 0.4092\n",
            "Epoch [21], Batch [121/157], test Loss: 0.3593\n",
            "Epoch [21], Batch [122/157], test Loss: 0.4714\n",
            "Epoch [21], Batch [123/157], test Loss: 0.3855\n",
            "Epoch [21], Batch [124/157], test Loss: 0.5101\n",
            "Epoch [21], Batch [125/157], test Loss: 0.4794\n",
            "Epoch [21], Batch [126/157], test Loss: 0.4094\n",
            "Epoch [21], Batch [127/157], test Loss: 0.4354\n",
            "Epoch [21], Batch [128/157], test Loss: 0.4193\n",
            "Epoch [21], Batch [129/157], test Loss: 0.3196\n",
            "Epoch [21], Batch [130/157], test Loss: 0.3541\n",
            "Epoch [21], Batch [131/157], test Loss: 0.4689\n",
            "Epoch [21], Batch [132/157], test Loss: 0.5361\n",
            "Epoch [21], Batch [133/157], test Loss: 0.3309\n",
            "Epoch [21], Batch [134/157], test Loss: 0.2932\n",
            "Epoch [21], Batch [135/157], test Loss: 0.3527\n",
            "Epoch [21], Batch [136/157], test Loss: 0.4239\n",
            "Epoch [21], Batch [137/157], test Loss: 0.4438\n",
            "Epoch [21], Batch [138/157], test Loss: 0.4335\n",
            "Epoch [21], Batch [139/157], test Loss: 0.3420\n",
            "Epoch [21], Batch [140/157], test Loss: 0.4294\n",
            "Epoch [21], Batch [141/157], test Loss: 0.4197\n",
            "Epoch [21], Batch [142/157], test Loss: 0.4649\n",
            "Epoch [21], Batch [143/157], test Loss: 0.6424\n",
            "Epoch [21], Batch [144/157], test Loss: 0.4436\n",
            "Epoch [21], Batch [145/157], test Loss: 0.2344\n",
            "Epoch [21], Batch [146/157], test Loss: 0.4962\n",
            "Epoch [21], Batch [147/157], test Loss: 0.4217\n",
            "Epoch [21], Batch [148/157], test Loss: 0.4055\n",
            "Epoch [21], Batch [149/157], test Loss: 0.3876\n",
            "Epoch [21], Batch [150/157], test Loss: 0.4117\n",
            "Epoch [21], Batch [151/157], test Loss: 0.4474\n",
            "Epoch [21], Batch [152/157], test Loss: 0.6276\n",
            "Epoch [21], Batch [153/157], test Loss: 0.3930\n",
            "Epoch [21], Batch [154/157], test Loss: 0.5054\n",
            "Epoch [21], Batch [155/157], test Loss: 0.4153\n",
            "Epoch [21], Batch [156/157], test Loss: 0.4560\n",
            "Epoch [21], Batch [157/157], test Loss: 0.4064\n",
            "Accuracy of test set: 83.93%\n",
            "Epoch [22/25] - Train Loss: 0.4211, Train Accuracy: 85.20% - Test Loss: 0.4482, Test Accuracy: 83.93%\n",
            "Epoch [22], Batch [10/938], Loss: 0.5014\n",
            "Epoch [22], Batch [20/938], Loss: 0.3658\n",
            "Epoch [22], Batch [30/938], Loss: 0.3662\n",
            "Epoch [22], Batch [40/938], Loss: 0.2731\n",
            "Epoch [22], Batch [50/938], Loss: 0.3268\n",
            "Epoch [22], Batch [60/938], Loss: 0.4581\n",
            "Epoch [22], Batch [70/938], Loss: 0.4653\n",
            "Epoch [22], Batch [80/938], Loss: 0.3473\n",
            "Epoch [22], Batch [90/938], Loss: 0.3971\n",
            "Epoch [22], Batch [100/938], Loss: 0.3493\n",
            "Epoch [22], Batch [110/938], Loss: 0.3508\n",
            "Epoch [22], Batch [120/938], Loss: 0.3967\n",
            "Epoch [22], Batch [130/938], Loss: 0.4021\n",
            "Epoch [22], Batch [140/938], Loss: 0.3841\n",
            "Epoch [22], Batch [150/938], Loss: 0.4922\n",
            "Epoch [22], Batch [160/938], Loss: 0.3716\n",
            "Epoch [22], Batch [170/938], Loss: 0.4213\n",
            "Epoch [22], Batch [180/938], Loss: 0.2738\n",
            "Epoch [22], Batch [190/938], Loss: 0.3406\n",
            "Epoch [22], Batch [200/938], Loss: 0.3130\n",
            "Epoch [22], Batch [210/938], Loss: 0.3053\n",
            "Epoch [22], Batch [220/938], Loss: 0.4039\n",
            "Epoch [22], Batch [230/938], Loss: 0.3835\n",
            "Epoch [22], Batch [240/938], Loss: 0.3207\n",
            "Epoch [22], Batch [250/938], Loss: 0.5425\n",
            "Epoch [22], Batch [260/938], Loss: 0.4895\n",
            "Epoch [22], Batch [270/938], Loss: 0.5520\n",
            "Epoch [22], Batch [280/938], Loss: 0.5383\n",
            "Epoch [22], Batch [290/938], Loss: 0.3766\n",
            "Epoch [22], Batch [300/938], Loss: 0.3881\n",
            "Epoch [22], Batch [310/938], Loss: 0.3979\n",
            "Epoch [22], Batch [320/938], Loss: 0.3497\n",
            "Epoch [22], Batch [330/938], Loss: 0.3548\n",
            "Epoch [22], Batch [340/938], Loss: 0.5932\n",
            "Epoch [22], Batch [350/938], Loss: 0.4230\n",
            "Epoch [22], Batch [360/938], Loss: 0.2839\n",
            "Epoch [22], Batch [370/938], Loss: 0.4076\n",
            "Epoch [22], Batch [380/938], Loss: 0.4206\n",
            "Epoch [22], Batch [390/938], Loss: 0.3737\n",
            "Epoch [22], Batch [400/938], Loss: 0.4905\n",
            "Epoch [22], Batch [410/938], Loss: 0.6569\n",
            "Epoch [22], Batch [420/938], Loss: 0.2805\n",
            "Epoch [22], Batch [430/938], Loss: 0.2372\n",
            "Epoch [22], Batch [440/938], Loss: 0.6277\n",
            "Epoch [22], Batch [450/938], Loss: 0.4083\n",
            "Epoch [22], Batch [460/938], Loss: 0.3407\n",
            "Epoch [22], Batch [470/938], Loss: 0.3936\n",
            "Epoch [22], Batch [480/938], Loss: 0.3673\n",
            "Epoch [22], Batch [490/938], Loss: 0.2662\n",
            "Epoch [22], Batch [500/938], Loss: 0.4179\n",
            "Epoch [22], Batch [510/938], Loss: 0.4390\n",
            "Epoch [22], Batch [520/938], Loss: 0.4584\n",
            "Epoch [22], Batch [530/938], Loss: 0.4124\n",
            "Epoch [22], Batch [540/938], Loss: 0.3172\n",
            "Epoch [22], Batch [550/938], Loss: 0.3272\n",
            "Epoch [22], Batch [560/938], Loss: 0.3298\n",
            "Epoch [22], Batch [570/938], Loss: 0.6426\n",
            "Epoch [22], Batch [580/938], Loss: 0.3015\n",
            "Epoch [22], Batch [590/938], Loss: 0.4795\n",
            "Epoch [22], Batch [600/938], Loss: 0.2441\n",
            "Epoch [22], Batch [610/938], Loss: 0.4597\n",
            "Epoch [22], Batch [620/938], Loss: 0.4468\n",
            "Epoch [22], Batch [630/938], Loss: 0.3939\n",
            "Epoch [22], Batch [640/938], Loss: 0.3995\n",
            "Epoch [22], Batch [650/938], Loss: 0.2970\n",
            "Epoch [22], Batch [660/938], Loss: 0.3385\n",
            "Epoch [22], Batch [670/938], Loss: 0.5488\n",
            "Epoch [22], Batch [680/938], Loss: 0.3167\n",
            "Epoch [22], Batch [690/938], Loss: 0.5253\n",
            "Epoch [22], Batch [700/938], Loss: 0.5219\n",
            "Epoch [22], Batch [710/938], Loss: 0.5069\n",
            "Epoch [22], Batch [720/938], Loss: 0.5043\n",
            "Epoch [22], Batch [730/938], Loss: 0.4653\n",
            "Epoch [22], Batch [740/938], Loss: 0.4275\n",
            "Epoch [22], Batch [750/938], Loss: 0.6401\n",
            "Epoch [22], Batch [760/938], Loss: 0.3522\n",
            "Epoch [22], Batch [770/938], Loss: 0.4709\n",
            "Epoch [22], Batch [780/938], Loss: 0.3909\n",
            "Epoch [22], Batch [790/938], Loss: 0.3797\n",
            "Epoch [22], Batch [800/938], Loss: 0.3329\n",
            "Epoch [22], Batch [810/938], Loss: 0.4140\n",
            "Epoch [22], Batch [820/938], Loss: 0.3633\n",
            "Epoch [22], Batch [830/938], Loss: 0.2947\n",
            "Epoch [22], Batch [840/938], Loss: 0.5196\n",
            "Epoch [22], Batch [850/938], Loss: 0.5056\n",
            "Epoch [22], Batch [860/938], Loss: 0.3794\n",
            "Epoch [22], Batch [870/938], Loss: 0.4643\n",
            "Epoch [22], Batch [880/938], Loss: 0.5619\n",
            "Epoch [22], Batch [890/938], Loss: 0.3696\n",
            "Epoch [22], Batch [900/938], Loss: 0.3897\n",
            "Epoch [22], Batch [910/938], Loss: 0.3394\n",
            "Epoch [22], Batch [920/938], Loss: 0.3893\n",
            "Epoch [22], Batch [930/938], Loss: 0.4912\n",
            "Epoch [22], Batch [938/938], Loss: 0.6027\n",
            "Accuracy of train set: 85.52%\n",
            "Epoch [22], Batch [1/157], test Loss: 0.5038\n",
            "Epoch [22], Batch [2/157], test Loss: 0.6604\n",
            "Epoch [22], Batch [3/157], test Loss: 0.7111\n",
            "Epoch [22], Batch [4/157], test Loss: 0.4367\n",
            "Epoch [22], Batch [5/157], test Loss: 0.3932\n",
            "Epoch [22], Batch [6/157], test Loss: 0.4235\n",
            "Epoch [22], Batch [7/157], test Loss: 0.7307\n",
            "Epoch [22], Batch [8/157], test Loss: 0.4485\n",
            "Epoch [22], Batch [9/157], test Loss: 0.3334\n",
            "Epoch [22], Batch [10/157], test Loss: 0.3429\n",
            "Epoch [22], Batch [11/157], test Loss: 0.3498\n",
            "Epoch [22], Batch [12/157], test Loss: 0.5326\n",
            "Epoch [22], Batch [13/157], test Loss: 0.4789\n",
            "Epoch [22], Batch [14/157], test Loss: 0.5918\n",
            "Epoch [22], Batch [15/157], test Loss: 0.5031\n",
            "Epoch [22], Batch [16/157], test Loss: 0.5708\n",
            "Epoch [22], Batch [17/157], test Loss: 0.8117\n",
            "Epoch [22], Batch [18/157], test Loss: 0.4482\n",
            "Epoch [22], Batch [19/157], test Loss: 0.3837\n",
            "Epoch [22], Batch [20/157], test Loss: 0.3257\n",
            "Epoch [22], Batch [21/157], test Loss: 0.3047\n",
            "Epoch [22], Batch [22/157], test Loss: 0.4142\n",
            "Epoch [22], Batch [23/157], test Loss: 0.3578\n",
            "Epoch [22], Batch [24/157], test Loss: 0.3932\n",
            "Epoch [22], Batch [25/157], test Loss: 0.4805\n",
            "Epoch [22], Batch [26/157], test Loss: 0.3233\n",
            "Epoch [22], Batch [27/157], test Loss: 0.6354\n",
            "Epoch [22], Batch [28/157], test Loss: 0.4478\n",
            "Epoch [22], Batch [29/157], test Loss: 0.6317\n",
            "Epoch [22], Batch [30/157], test Loss: 0.4130\n",
            "Epoch [22], Batch [31/157], test Loss: 0.6539\n",
            "Epoch [22], Batch [32/157], test Loss: 0.3819\n",
            "Epoch [22], Batch [33/157], test Loss: 0.6331\n",
            "Epoch [22], Batch [34/157], test Loss: 0.5689\n",
            "Epoch [22], Batch [35/157], test Loss: 0.6004\n",
            "Epoch [22], Batch [36/157], test Loss: 0.4478\n",
            "Epoch [22], Batch [37/157], test Loss: 0.4496\n",
            "Epoch [22], Batch [38/157], test Loss: 0.5010\n",
            "Epoch [22], Batch [39/157], test Loss: 0.2876\n",
            "Epoch [22], Batch [40/157], test Loss: 0.6958\n",
            "Epoch [22], Batch [41/157], test Loss: 0.4252\n",
            "Epoch [22], Batch [42/157], test Loss: 0.3687\n",
            "Epoch [22], Batch [43/157], test Loss: 0.5980\n",
            "Epoch [22], Batch [44/157], test Loss: 0.5728\n",
            "Epoch [22], Batch [45/157], test Loss: 0.2927\n",
            "Epoch [22], Batch [46/157], test Loss: 0.2621\n",
            "Epoch [22], Batch [47/157], test Loss: 0.5577\n",
            "Epoch [22], Batch [48/157], test Loss: 0.4301\n",
            "Epoch [22], Batch [49/157], test Loss: 0.4603\n",
            "Epoch [22], Batch [50/157], test Loss: 0.6104\n",
            "Epoch [22], Batch [51/157], test Loss: 0.6852\n",
            "Epoch [22], Batch [52/157], test Loss: 0.6778\n",
            "Epoch [22], Batch [53/157], test Loss: 0.3647\n",
            "Epoch [22], Batch [54/157], test Loss: 0.3180\n",
            "Epoch [22], Batch [55/157], test Loss: 0.6722\n",
            "Epoch [22], Batch [56/157], test Loss: 0.5425\n",
            "Epoch [22], Batch [57/157], test Loss: 0.5146\n",
            "Epoch [22], Batch [58/157], test Loss: 0.6217\n",
            "Epoch [22], Batch [59/157], test Loss: 0.4720\n",
            "Epoch [22], Batch [60/157], test Loss: 0.6336\n",
            "Epoch [22], Batch [61/157], test Loss: 0.3328\n",
            "Epoch [22], Batch [62/157], test Loss: 0.5310\n",
            "Epoch [22], Batch [63/157], test Loss: 0.3487\n",
            "Epoch [22], Batch [64/157], test Loss: 0.3151\n",
            "Epoch [22], Batch [65/157], test Loss: 0.5789\n",
            "Epoch [22], Batch [66/157], test Loss: 0.4888\n",
            "Epoch [22], Batch [67/157], test Loss: 0.5465\n",
            "Epoch [22], Batch [68/157], test Loss: 0.6321\n",
            "Epoch [22], Batch [69/157], test Loss: 0.3733\n",
            "Epoch [22], Batch [70/157], test Loss: 0.3432\n",
            "Epoch [22], Batch [71/157], test Loss: 0.4593\n",
            "Epoch [22], Batch [72/157], test Loss: 0.4547\n",
            "Epoch [22], Batch [73/157], test Loss: 0.5174\n",
            "Epoch [22], Batch [74/157], test Loss: 0.4497\n",
            "Epoch [22], Batch [75/157], test Loss: 0.4723\n",
            "Epoch [22], Batch [76/157], test Loss: 0.3745\n",
            "Epoch [22], Batch [77/157], test Loss: 0.2783\n",
            "Epoch [22], Batch [78/157], test Loss: 0.5677\n",
            "Epoch [22], Batch [79/157], test Loss: 0.3216\n",
            "Epoch [22], Batch [80/157], test Loss: 0.4149\n",
            "Epoch [22], Batch [81/157], test Loss: 0.4853\n",
            "Epoch [22], Batch [82/157], test Loss: 0.4156\n",
            "Epoch [22], Batch [83/157], test Loss: 0.5444\n",
            "Epoch [22], Batch [84/157], test Loss: 0.4594\n",
            "Epoch [22], Batch [85/157], test Loss: 0.7214\n",
            "Epoch [22], Batch [86/157], test Loss: 0.3370\n",
            "Epoch [22], Batch [87/157], test Loss: 0.5110\n",
            "Epoch [22], Batch [88/157], test Loss: 0.4040\n",
            "Epoch [22], Batch [89/157], test Loss: 0.4841\n",
            "Epoch [22], Batch [90/157], test Loss: 0.4336\n",
            "Epoch [22], Batch [91/157], test Loss: 0.5416\n",
            "Epoch [22], Batch [92/157], test Loss: 0.5315\n",
            "Epoch [22], Batch [93/157], test Loss: 0.4257\n",
            "Epoch [22], Batch [94/157], test Loss: 0.6644\n",
            "Epoch [22], Batch [95/157], test Loss: 0.4100\n",
            "Epoch [22], Batch [96/157], test Loss: 0.4093\n",
            "Epoch [22], Batch [97/157], test Loss: 0.4837\n",
            "Epoch [22], Batch [98/157], test Loss: 0.5427\n",
            "Epoch [22], Batch [99/157], test Loss: 0.4675\n",
            "Epoch [22], Batch [100/157], test Loss: 0.3885\n",
            "Epoch [22], Batch [101/157], test Loss: 0.3112\n",
            "Epoch [22], Batch [102/157], test Loss: 0.4583\n",
            "Epoch [22], Batch [103/157], test Loss: 0.4659\n",
            "Epoch [22], Batch [104/157], test Loss: 0.4393\n",
            "Epoch [22], Batch [105/157], test Loss: 0.4857\n",
            "Epoch [22], Batch [106/157], test Loss: 0.5113\n",
            "Epoch [22], Batch [107/157], test Loss: 0.4089\n",
            "Epoch [22], Batch [108/157], test Loss: 0.4519\n",
            "Epoch [22], Batch [109/157], test Loss: 0.6689\n",
            "Epoch [22], Batch [110/157], test Loss: 0.4107\n",
            "Epoch [22], Batch [111/157], test Loss: 0.3402\n",
            "Epoch [22], Batch [112/157], test Loss: 0.3917\n",
            "Epoch [22], Batch [113/157], test Loss: 0.4981\n",
            "Epoch [22], Batch [114/157], test Loss: 0.6073\n",
            "Epoch [22], Batch [115/157], test Loss: 0.3287\n",
            "Epoch [22], Batch [116/157], test Loss: 0.5852\n",
            "Epoch [22], Batch [117/157], test Loss: 0.3372\n",
            "Epoch [22], Batch [118/157], test Loss: 0.3655\n",
            "Epoch [22], Batch [119/157], test Loss: 0.3690\n",
            "Epoch [22], Batch [120/157], test Loss: 0.4609\n",
            "Epoch [22], Batch [121/157], test Loss: 0.4762\n",
            "Epoch [22], Batch [122/157], test Loss: 0.5443\n",
            "Epoch [22], Batch [123/157], test Loss: 0.4412\n",
            "Epoch [22], Batch [124/157], test Loss: 0.3715\n",
            "Epoch [22], Batch [125/157], test Loss: 0.2652\n",
            "Epoch [22], Batch [126/157], test Loss: 0.3385\n",
            "Epoch [22], Batch [127/157], test Loss: 0.4252\n",
            "Epoch [22], Batch [128/157], test Loss: 0.5197\n",
            "Epoch [22], Batch [129/157], test Loss: 0.4620\n",
            "Epoch [22], Batch [130/157], test Loss: 0.4341\n",
            "Epoch [22], Batch [131/157], test Loss: 0.4312\n",
            "Epoch [22], Batch [132/157], test Loss: 0.4704\n",
            "Epoch [22], Batch [133/157], test Loss: 0.3330\n",
            "Epoch [22], Batch [134/157], test Loss: 0.5868\n",
            "Epoch [22], Batch [135/157], test Loss: 0.5203\n",
            "Epoch [22], Batch [136/157], test Loss: 0.4929\n",
            "Epoch [22], Batch [137/157], test Loss: 0.4254\n",
            "Epoch [22], Batch [138/157], test Loss: 0.4271\n",
            "Epoch [22], Batch [139/157], test Loss: 0.7160\n",
            "Epoch [22], Batch [140/157], test Loss: 0.4119\n",
            "Epoch [22], Batch [141/157], test Loss: 0.4587\n",
            "Epoch [22], Batch [142/157], test Loss: 0.5478\n",
            "Epoch [22], Batch [143/157], test Loss: 0.4881\n",
            "Epoch [22], Batch [144/157], test Loss: 0.4472\n",
            "Epoch [22], Batch [145/157], test Loss: 0.4388\n",
            "Epoch [22], Batch [146/157], test Loss: 0.6073\n",
            "Epoch [22], Batch [147/157], test Loss: 0.4167\n",
            "Epoch [22], Batch [148/157], test Loss: 0.8445\n",
            "Epoch [22], Batch [149/157], test Loss: 0.3745\n",
            "Epoch [22], Batch [150/157], test Loss: 0.3642\n",
            "Epoch [22], Batch [151/157], test Loss: 0.3302\n",
            "Epoch [22], Batch [152/157], test Loss: 0.4348\n",
            "Epoch [22], Batch [153/157], test Loss: 0.6116\n",
            "Epoch [22], Batch [154/157], test Loss: 0.3089\n",
            "Epoch [22], Batch [155/157], test Loss: 0.4918\n",
            "Epoch [22], Batch [156/157], test Loss: 0.2728\n",
            "Epoch [22], Batch [157/157], test Loss: 0.5140\n",
            "Accuracy of test set: 83.16%\n",
            "Epoch [23/25] - Train Loss: 0.4124, Train Accuracy: 85.52% - Test Loss: 0.4719, Test Accuracy: 83.16%\n",
            "Epoch [23], Batch [10/938], Loss: 0.3261\n",
            "Epoch [23], Batch [20/938], Loss: 0.2481\n",
            "Epoch [23], Batch [30/938], Loss: 0.3275\n",
            "Epoch [23], Batch [40/938], Loss: 0.3587\n",
            "Epoch [23], Batch [50/938], Loss: 0.3323\n",
            "Epoch [23], Batch [60/938], Loss: 0.3072\n",
            "Epoch [23], Batch [70/938], Loss: 0.4339\n",
            "Epoch [23], Batch [80/938], Loss: 0.5259\n",
            "Epoch [23], Batch [90/938], Loss: 0.4050\n",
            "Epoch [23], Batch [100/938], Loss: 0.3847\n",
            "Epoch [23], Batch [110/938], Loss: 0.2939\n",
            "Epoch [23], Batch [120/938], Loss: 0.4416\n",
            "Epoch [23], Batch [130/938], Loss: 0.5392\n",
            "Epoch [23], Batch [140/938], Loss: 0.4166\n",
            "Epoch [23], Batch [150/938], Loss: 0.3792\n",
            "Epoch [23], Batch [160/938], Loss: 0.5565\n",
            "Epoch [23], Batch [170/938], Loss: 0.3442\n",
            "Epoch [23], Batch [180/938], Loss: 0.3840\n",
            "Epoch [23], Batch [190/938], Loss: 0.5288\n",
            "Epoch [23], Batch [200/938], Loss: 0.5623\n",
            "Epoch [23], Batch [210/938], Loss: 0.5657\n",
            "Epoch [23], Batch [220/938], Loss: 0.5290\n",
            "Epoch [23], Batch [230/938], Loss: 0.3845\n",
            "Epoch [23], Batch [240/938], Loss: 0.3684\n",
            "Epoch [23], Batch [250/938], Loss: 0.4242\n",
            "Epoch [23], Batch [260/938], Loss: 0.2825\n",
            "Epoch [23], Batch [270/938], Loss: 0.4706\n",
            "Epoch [23], Batch [280/938], Loss: 0.4446\n",
            "Epoch [23], Batch [290/938], Loss: 0.4690\n",
            "Epoch [23], Batch [300/938], Loss: 0.2865\n",
            "Epoch [23], Batch [310/938], Loss: 0.5103\n",
            "Epoch [23], Batch [320/938], Loss: 0.4563\n",
            "Epoch [23], Batch [330/938], Loss: 0.1435\n",
            "Epoch [23], Batch [340/938], Loss: 0.5812\n",
            "Epoch [23], Batch [350/938], Loss: 0.3861\n",
            "Epoch [23], Batch [360/938], Loss: 0.5128\n",
            "Epoch [23], Batch [370/938], Loss: 0.4700\n",
            "Epoch [23], Batch [380/938], Loss: 0.3018\n",
            "Epoch [23], Batch [390/938], Loss: 0.4101\n",
            "Epoch [23], Batch [400/938], Loss: 0.2176\n",
            "Epoch [23], Batch [410/938], Loss: 0.5006\n",
            "Epoch [23], Batch [420/938], Loss: 0.4134\n",
            "Epoch [23], Batch [430/938], Loss: 0.5394\n",
            "Epoch [23], Batch [440/938], Loss: 0.4294\n",
            "Epoch [23], Batch [450/938], Loss: 0.2732\n",
            "Epoch [23], Batch [460/938], Loss: 0.4114\n",
            "Epoch [23], Batch [470/938], Loss: 0.4929\n",
            "Epoch [23], Batch [480/938], Loss: 0.3479\n",
            "Epoch [23], Batch [490/938], Loss: 0.8507\n",
            "Epoch [23], Batch [500/938], Loss: 0.6069\n",
            "Epoch [23], Batch [510/938], Loss: 0.3350\n",
            "Epoch [23], Batch [520/938], Loss: 0.4977\n",
            "Epoch [23], Batch [530/938], Loss: 0.3536\n",
            "Epoch [23], Batch [540/938], Loss: 0.4316\n",
            "Epoch [23], Batch [550/938], Loss: 0.4859\n",
            "Epoch [23], Batch [560/938], Loss: 0.3740\n",
            "Epoch [23], Batch [570/938], Loss: 0.5654\n",
            "Epoch [23], Batch [580/938], Loss: 0.4574\n",
            "Epoch [23], Batch [590/938], Loss: 0.2980\n",
            "Epoch [23], Batch [600/938], Loss: 0.2876\n",
            "Epoch [23], Batch [610/938], Loss: 0.3156\n",
            "Epoch [23], Batch [620/938], Loss: 0.5992\n",
            "Epoch [23], Batch [630/938], Loss: 0.6020\n",
            "Epoch [23], Batch [640/938], Loss: 0.2704\n",
            "Epoch [23], Batch [650/938], Loss: 0.4489\n",
            "Epoch [23], Batch [660/938], Loss: 0.2943\n",
            "Epoch [23], Batch [670/938], Loss: 0.3431\n",
            "Epoch [23], Batch [680/938], Loss: 0.2504\n",
            "Epoch [23], Batch [690/938], Loss: 0.5200\n",
            "Epoch [23], Batch [700/938], Loss: 0.5525\n",
            "Epoch [23], Batch [710/938], Loss: 0.5354\n",
            "Epoch [23], Batch [720/938], Loss: 0.2929\n",
            "Epoch [23], Batch [730/938], Loss: 0.3540\n",
            "Epoch [23], Batch [740/938], Loss: 0.5798\n",
            "Epoch [23], Batch [750/938], Loss: 0.5824\n",
            "Epoch [23], Batch [760/938], Loss: 0.3311\n",
            "Epoch [23], Batch [770/938], Loss: 0.4898\n",
            "Epoch [23], Batch [780/938], Loss: 0.4432\n",
            "Epoch [23], Batch [790/938], Loss: 0.4184\n",
            "Epoch [23], Batch [800/938], Loss: 0.2793\n",
            "Epoch [23], Batch [810/938], Loss: 0.3124\n",
            "Epoch [23], Batch [820/938], Loss: 0.3821\n",
            "Epoch [23], Batch [830/938], Loss: 0.3507\n",
            "Epoch [23], Batch [840/938], Loss: 0.3205\n",
            "Epoch [23], Batch [850/938], Loss: 0.4683\n",
            "Epoch [23], Batch [860/938], Loss: 0.3278\n",
            "Epoch [23], Batch [870/938], Loss: 0.4610\n",
            "Epoch [23], Batch [880/938], Loss: 0.3821\n",
            "Epoch [23], Batch [890/938], Loss: 0.4112\n",
            "Epoch [23], Batch [900/938], Loss: 0.4917\n",
            "Epoch [23], Batch [910/938], Loss: 0.3318\n",
            "Epoch [23], Batch [920/938], Loss: 0.3070\n",
            "Epoch [23], Batch [930/938], Loss: 0.3017\n",
            "Epoch [23], Batch [938/938], Loss: 0.2094\n",
            "Accuracy of train set: 85.77%\n",
            "Epoch [23], Batch [1/157], test Loss: 0.3197\n",
            "Epoch [23], Batch [2/157], test Loss: 0.4543\n",
            "Epoch [23], Batch [3/157], test Loss: 0.5362\n",
            "Epoch [23], Batch [4/157], test Loss: 0.3565\n",
            "Epoch [23], Batch [5/157], test Loss: 0.4431\n",
            "Epoch [23], Batch [6/157], test Loss: 0.5624\n",
            "Epoch [23], Batch [7/157], test Loss: 0.4670\n",
            "Epoch [23], Batch [8/157], test Loss: 0.4514\n",
            "Epoch [23], Batch [9/157], test Loss: 0.4106\n",
            "Epoch [23], Batch [10/157], test Loss: 0.4425\n",
            "Epoch [23], Batch [11/157], test Loss: 0.2476\n",
            "Epoch [23], Batch [12/157], test Loss: 0.5600\n",
            "Epoch [23], Batch [13/157], test Loss: 0.4254\n",
            "Epoch [23], Batch [14/157], test Loss: 0.5007\n",
            "Epoch [23], Batch [15/157], test Loss: 0.5302\n",
            "Epoch [23], Batch [16/157], test Loss: 0.4920\n",
            "Epoch [23], Batch [17/157], test Loss: 0.4040\n",
            "Epoch [23], Batch [18/157], test Loss: 0.4754\n",
            "Epoch [23], Batch [19/157], test Loss: 0.5651\n",
            "Epoch [23], Batch [20/157], test Loss: 0.7391\n",
            "Epoch [23], Batch [21/157], test Loss: 0.5115\n",
            "Epoch [23], Batch [22/157], test Loss: 0.4776\n",
            "Epoch [23], Batch [23/157], test Loss: 0.3130\n",
            "Epoch [23], Batch [24/157], test Loss: 0.4663\n",
            "Epoch [23], Batch [25/157], test Loss: 0.4790\n",
            "Epoch [23], Batch [26/157], test Loss: 0.3811\n",
            "Epoch [23], Batch [27/157], test Loss: 0.4164\n",
            "Epoch [23], Batch [28/157], test Loss: 0.4034\n",
            "Epoch [23], Batch [29/157], test Loss: 0.5412\n",
            "Epoch [23], Batch [30/157], test Loss: 0.5038\n",
            "Epoch [23], Batch [31/157], test Loss: 0.4974\n",
            "Epoch [23], Batch [32/157], test Loss: 0.3710\n",
            "Epoch [23], Batch [33/157], test Loss: 0.4324\n",
            "Epoch [23], Batch [34/157], test Loss: 0.5165\n",
            "Epoch [23], Batch [35/157], test Loss: 0.3710\n",
            "Epoch [23], Batch [36/157], test Loss: 0.4998\n",
            "Epoch [23], Batch [37/157], test Loss: 0.4791\n",
            "Epoch [23], Batch [38/157], test Loss: 0.5368\n",
            "Epoch [23], Batch [39/157], test Loss: 0.6788\n",
            "Epoch [23], Batch [40/157], test Loss: 0.6645\n",
            "Epoch [23], Batch [41/157], test Loss: 0.4015\n",
            "Epoch [23], Batch [42/157], test Loss: 0.5900\n",
            "Epoch [23], Batch [43/157], test Loss: 0.3558\n",
            "Epoch [23], Batch [44/157], test Loss: 0.4546\n",
            "Epoch [23], Batch [45/157], test Loss: 0.4251\n",
            "Epoch [23], Batch [46/157], test Loss: 0.4273\n",
            "Epoch [23], Batch [47/157], test Loss: 0.4647\n",
            "Epoch [23], Batch [48/157], test Loss: 0.6641\n",
            "Epoch [23], Batch [49/157], test Loss: 0.3965\n",
            "Epoch [23], Batch [50/157], test Loss: 0.5802\n",
            "Epoch [23], Batch [51/157], test Loss: 0.5739\n",
            "Epoch [23], Batch [52/157], test Loss: 0.3240\n",
            "Epoch [23], Batch [53/157], test Loss: 0.5946\n",
            "Epoch [23], Batch [54/157], test Loss: 0.6018\n",
            "Epoch [23], Batch [55/157], test Loss: 0.5719\n",
            "Epoch [23], Batch [56/157], test Loss: 0.3584\n",
            "Epoch [23], Batch [57/157], test Loss: 0.5297\n",
            "Epoch [23], Batch [58/157], test Loss: 0.3184\n",
            "Epoch [23], Batch [59/157], test Loss: 0.3778\n",
            "Epoch [23], Batch [60/157], test Loss: 0.6738\n",
            "Epoch [23], Batch [61/157], test Loss: 0.7231\n",
            "Epoch [23], Batch [62/157], test Loss: 0.3975\n",
            "Epoch [23], Batch [63/157], test Loss: 0.6543\n",
            "Epoch [23], Batch [64/157], test Loss: 0.8096\n",
            "Epoch [23], Batch [65/157], test Loss: 0.4912\n",
            "Epoch [23], Batch [66/157], test Loss: 0.4267\n",
            "Epoch [23], Batch [67/157], test Loss: 0.4531\n",
            "Epoch [23], Batch [68/157], test Loss: 0.6604\n",
            "Epoch [23], Batch [69/157], test Loss: 0.3991\n",
            "Epoch [23], Batch [70/157], test Loss: 0.3127\n",
            "Epoch [23], Batch [71/157], test Loss: 0.5299\n",
            "Epoch [23], Batch [72/157], test Loss: 0.4044\n",
            "Epoch [23], Batch [73/157], test Loss: 0.5857\n",
            "Epoch [23], Batch [74/157], test Loss: 0.5228\n",
            "Epoch [23], Batch [75/157], test Loss: 0.4256\n",
            "Epoch [23], Batch [76/157], test Loss: 0.5524\n",
            "Epoch [23], Batch [77/157], test Loss: 0.3545\n",
            "Epoch [23], Batch [78/157], test Loss: 0.5472\n",
            "Epoch [23], Batch [79/157], test Loss: 0.3284\n",
            "Epoch [23], Batch [80/157], test Loss: 0.4231\n",
            "Epoch [23], Batch [81/157], test Loss: 0.4927\n",
            "Epoch [23], Batch [82/157], test Loss: 0.4523\n",
            "Epoch [23], Batch [83/157], test Loss: 0.5832\n",
            "Epoch [23], Batch [84/157], test Loss: 0.3078\n",
            "Epoch [23], Batch [85/157], test Loss: 0.4183\n",
            "Epoch [23], Batch [86/157], test Loss: 0.5781\n",
            "Epoch [23], Batch [87/157], test Loss: 0.5193\n",
            "Epoch [23], Batch [88/157], test Loss: 0.3432\n",
            "Epoch [23], Batch [89/157], test Loss: 0.4363\n",
            "Epoch [23], Batch [90/157], test Loss: 0.4353\n",
            "Epoch [23], Batch [91/157], test Loss: 0.2071\n",
            "Epoch [23], Batch [92/157], test Loss: 0.4923\n",
            "Epoch [23], Batch [93/157], test Loss: 0.7581\n",
            "Epoch [23], Batch [94/157], test Loss: 0.3004\n",
            "Epoch [23], Batch [95/157], test Loss: 0.3935\n",
            "Epoch [23], Batch [96/157], test Loss: 0.4446\n",
            "Epoch [23], Batch [97/157], test Loss: 0.3853\n",
            "Epoch [23], Batch [98/157], test Loss: 0.4351\n",
            "Epoch [23], Batch [99/157], test Loss: 0.2864\n",
            "Epoch [23], Batch [100/157], test Loss: 0.4539\n",
            "Epoch [23], Batch [101/157], test Loss: 0.3387\n",
            "Epoch [23], Batch [102/157], test Loss: 0.3725\n",
            "Epoch [23], Batch [103/157], test Loss: 0.5108\n",
            "Epoch [23], Batch [104/157], test Loss: 0.4851\n",
            "Epoch [23], Batch [105/157], test Loss: 0.5544\n",
            "Epoch [23], Batch [106/157], test Loss: 0.5004\n",
            "Epoch [23], Batch [107/157], test Loss: 0.4225\n",
            "Epoch [23], Batch [108/157], test Loss: 0.4366\n",
            "Epoch [23], Batch [109/157], test Loss: 0.3047\n",
            "Epoch [23], Batch [110/157], test Loss: 0.5105\n",
            "Epoch [23], Batch [111/157], test Loss: 0.3330\n",
            "Epoch [23], Batch [112/157], test Loss: 0.4507\n",
            "Epoch [23], Batch [113/157], test Loss: 0.3733\n",
            "Epoch [23], Batch [114/157], test Loss: 0.3217\n",
            "Epoch [23], Batch [115/157], test Loss: 0.7151\n",
            "Epoch [23], Batch [116/157], test Loss: 0.5194\n",
            "Epoch [23], Batch [117/157], test Loss: 0.4532\n",
            "Epoch [23], Batch [118/157], test Loss: 0.5655\n",
            "Epoch [23], Batch [119/157], test Loss: 0.2850\n",
            "Epoch [23], Batch [120/157], test Loss: 0.4885\n",
            "Epoch [23], Batch [121/157], test Loss: 0.4961\n",
            "Epoch [23], Batch [122/157], test Loss: 0.3387\n",
            "Epoch [23], Batch [123/157], test Loss: 0.4529\n",
            "Epoch [23], Batch [124/157], test Loss: 0.3980\n",
            "Epoch [23], Batch [125/157], test Loss: 0.4792\n",
            "Epoch [23], Batch [126/157], test Loss: 0.7818\n",
            "Epoch [23], Batch [127/157], test Loss: 0.5334\n",
            "Epoch [23], Batch [128/157], test Loss: 0.5273\n",
            "Epoch [23], Batch [129/157], test Loss: 0.4326\n",
            "Epoch [23], Batch [130/157], test Loss: 0.5292\n",
            "Epoch [23], Batch [131/157], test Loss: 0.5129\n",
            "Epoch [23], Batch [132/157], test Loss: 0.3848\n",
            "Epoch [23], Batch [133/157], test Loss: 0.4232\n",
            "Epoch [23], Batch [134/157], test Loss: 0.5700\n",
            "Epoch [23], Batch [135/157], test Loss: 0.4782\n",
            "Epoch [23], Batch [136/157], test Loss: 0.2109\n",
            "Epoch [23], Batch [137/157], test Loss: 0.3934\n",
            "Epoch [23], Batch [138/157], test Loss: 0.3329\n",
            "Epoch [23], Batch [139/157], test Loss: 0.3749\n",
            "Epoch [23], Batch [140/157], test Loss: 0.5943\n",
            "Epoch [23], Batch [141/157], test Loss: 0.3772\n",
            "Epoch [23], Batch [142/157], test Loss: 0.5425\n",
            "Epoch [23], Batch [143/157], test Loss: 0.4873\n",
            "Epoch [23], Batch [144/157], test Loss: 0.4417\n",
            "Epoch [23], Batch [145/157], test Loss: 0.4674\n",
            "Epoch [23], Batch [146/157], test Loss: 0.5148\n",
            "Epoch [23], Batch [147/157], test Loss: 0.4432\n",
            "Epoch [23], Batch [148/157], test Loss: 0.5020\n",
            "Epoch [23], Batch [149/157], test Loss: 0.3659\n",
            "Epoch [23], Batch [150/157], test Loss: 0.3766\n",
            "Epoch [23], Batch [151/157], test Loss: 0.4069\n",
            "Epoch [23], Batch [152/157], test Loss: 0.3839\n",
            "Epoch [23], Batch [153/157], test Loss: 0.4618\n",
            "Epoch [23], Batch [154/157], test Loss: 0.7485\n",
            "Epoch [23], Batch [155/157], test Loss: 0.4385\n",
            "Epoch [23], Batch [156/157], test Loss: 0.3847\n",
            "Epoch [23], Batch [157/157], test Loss: 0.3152\n",
            "Accuracy of test set: 83.15%\n",
            "Epoch [24/25] - Train Loss: 0.4044, Train Accuracy: 85.77% - Test Loss: 0.4661, Test Accuracy: 83.15%\n",
            "Epoch [24], Batch [10/938], Loss: 0.2202\n",
            "Epoch [24], Batch [20/938], Loss: 0.5204\n",
            "Epoch [24], Batch [30/938], Loss: 0.4863\n",
            "Epoch [24], Batch [40/938], Loss: 0.3351\n",
            "Epoch [24], Batch [50/938], Loss: 0.6832\n",
            "Epoch [24], Batch [60/938], Loss: 0.4149\n",
            "Epoch [24], Batch [70/938], Loss: 0.4111\n",
            "Epoch [24], Batch [80/938], Loss: 0.2638\n",
            "Epoch [24], Batch [90/938], Loss: 0.4068\n",
            "Epoch [24], Batch [100/938], Loss: 0.2885\n",
            "Epoch [24], Batch [110/938], Loss: 0.3493\n",
            "Epoch [24], Batch [120/938], Loss: 0.4880\n",
            "Epoch [24], Batch [130/938], Loss: 0.2402\n",
            "Epoch [24], Batch [140/938], Loss: 0.4987\n",
            "Epoch [24], Batch [150/938], Loss: 0.4952\n",
            "Epoch [24], Batch [160/938], Loss: 0.3671\n",
            "Epoch [24], Batch [170/938], Loss: 0.4929\n",
            "Epoch [24], Batch [180/938], Loss: 0.3121\n",
            "Epoch [24], Batch [190/938], Loss: 0.3594\n",
            "Epoch [24], Batch [200/938], Loss: 0.4976\n",
            "Epoch [24], Batch [210/938], Loss: 0.2850\n",
            "Epoch [24], Batch [220/938], Loss: 0.4557\n",
            "Epoch [24], Batch [230/938], Loss: 0.4416\n",
            "Epoch [24], Batch [240/938], Loss: 0.5567\n",
            "Epoch [24], Batch [250/938], Loss: 0.5453\n",
            "Epoch [24], Batch [260/938], Loss: 0.5476\n",
            "Epoch [24], Batch [270/938], Loss: 0.6100\n",
            "Epoch [24], Batch [280/938], Loss: 0.4381\n",
            "Epoch [24], Batch [290/938], Loss: 0.3938\n",
            "Epoch [24], Batch [300/938], Loss: 0.5637\n",
            "Epoch [24], Batch [310/938], Loss: 0.2667\n",
            "Epoch [24], Batch [320/938], Loss: 0.3043\n",
            "Epoch [24], Batch [330/938], Loss: 0.4596\n",
            "Epoch [24], Batch [340/938], Loss: 0.3106\n",
            "Epoch [24], Batch [350/938], Loss: 0.3349\n",
            "Epoch [24], Batch [360/938], Loss: 0.3313\n",
            "Epoch [24], Batch [370/938], Loss: 0.4028\n",
            "Epoch [24], Batch [380/938], Loss: 0.4156\n",
            "Epoch [24], Batch [390/938], Loss: 0.4573\n",
            "Epoch [24], Batch [400/938], Loss: 0.3914\n",
            "Epoch [24], Batch [410/938], Loss: 0.3361\n",
            "Epoch [24], Batch [420/938], Loss: 0.4766\n",
            "Epoch [24], Batch [430/938], Loss: 0.5218\n",
            "Epoch [24], Batch [440/938], Loss: 0.4195\n",
            "Epoch [24], Batch [450/938], Loss: 0.3199\n",
            "Epoch [24], Batch [460/938], Loss: 0.2796\n",
            "Epoch [24], Batch [470/938], Loss: 0.2361\n",
            "Epoch [24], Batch [480/938], Loss: 0.3188\n",
            "Epoch [24], Batch [490/938], Loss: 0.2754\n",
            "Epoch [24], Batch [500/938], Loss: 0.4191\n",
            "Epoch [24], Batch [510/938], Loss: 0.2824\n",
            "Epoch [24], Batch [520/938], Loss: 0.4521\n",
            "Epoch [24], Batch [530/938], Loss: 0.4171\n",
            "Epoch [24], Batch [540/938], Loss: 0.4118\n",
            "Epoch [24], Batch [550/938], Loss: 0.2827\n",
            "Epoch [24], Batch [560/938], Loss: 0.2163\n",
            "Epoch [24], Batch [570/938], Loss: 0.3327\n",
            "Epoch [24], Batch [580/938], Loss: 0.3186\n",
            "Epoch [24], Batch [590/938], Loss: 0.3609\n",
            "Epoch [24], Batch [600/938], Loss: 0.3823\n",
            "Epoch [24], Batch [610/938], Loss: 0.3616\n",
            "Epoch [24], Batch [620/938], Loss: 0.4317\n",
            "Epoch [24], Batch [630/938], Loss: 0.4076\n",
            "Epoch [24], Batch [640/938], Loss: 0.4125\n",
            "Epoch [24], Batch [650/938], Loss: 0.4477\n",
            "Epoch [24], Batch [660/938], Loss: 0.4315\n",
            "Epoch [24], Batch [670/938], Loss: 0.4227\n",
            "Epoch [24], Batch [680/938], Loss: 0.3221\n",
            "Epoch [24], Batch [690/938], Loss: 0.2548\n",
            "Epoch [24], Batch [700/938], Loss: 0.4807\n",
            "Epoch [24], Batch [710/938], Loss: 0.4192\n",
            "Epoch [24], Batch [720/938], Loss: 0.3785\n",
            "Epoch [24], Batch [730/938], Loss: 0.3774\n",
            "Epoch [24], Batch [740/938], Loss: 0.3376\n",
            "Epoch [24], Batch [750/938], Loss: 0.5228\n",
            "Epoch [24], Batch [760/938], Loss: 0.4981\n",
            "Epoch [24], Batch [770/938], Loss: 0.3252\n",
            "Epoch [24], Batch [780/938], Loss: 0.2467\n",
            "Epoch [24], Batch [790/938], Loss: 0.5329\n",
            "Epoch [24], Batch [800/938], Loss: 0.4568\n",
            "Epoch [24], Batch [810/938], Loss: 0.4198\n",
            "Epoch [24], Batch [820/938], Loss: 0.4594\n",
            "Epoch [24], Batch [830/938], Loss: 0.3546\n",
            "Epoch [24], Batch [840/938], Loss: 0.4348\n",
            "Epoch [24], Batch [850/938], Loss: 0.5713\n",
            "Epoch [24], Batch [860/938], Loss: 0.4020\n",
            "Epoch [24], Batch [870/938], Loss: 0.3085\n",
            "Epoch [24], Batch [880/938], Loss: 0.4065\n",
            "Epoch [24], Batch [890/938], Loss: 0.4883\n",
            "Epoch [24], Batch [900/938], Loss: 0.3766\n",
            "Epoch [24], Batch [910/938], Loss: 0.4788\n",
            "Epoch [24], Batch [920/938], Loss: 0.5030\n",
            "Epoch [24], Batch [930/938], Loss: 0.4892\n",
            "Epoch [24], Batch [938/938], Loss: 0.3742\n",
            "Accuracy of train set: 86.00%\n",
            "Epoch [24], Batch [1/157], test Loss: 0.3574\n",
            "Epoch [24], Batch [2/157], test Loss: 0.4466\n",
            "Epoch [24], Batch [3/157], test Loss: 0.3096\n",
            "Epoch [24], Batch [4/157], test Loss: 0.3218\n",
            "Epoch [24], Batch [5/157], test Loss: 0.4370\n",
            "Epoch [24], Batch [6/157], test Loss: 0.4037\n",
            "Epoch [24], Batch [7/157], test Loss: 0.4982\n",
            "Epoch [24], Batch [8/157], test Loss: 0.3921\n",
            "Epoch [24], Batch [9/157], test Loss: 0.4078\n",
            "Epoch [24], Batch [10/157], test Loss: 0.3709\n",
            "Epoch [24], Batch [11/157], test Loss: 0.3766\n",
            "Epoch [24], Batch [12/157], test Loss: 0.5833\n",
            "Epoch [24], Batch [13/157], test Loss: 0.4419\n",
            "Epoch [24], Batch [14/157], test Loss: 0.4244\n",
            "Epoch [24], Batch [15/157], test Loss: 0.3717\n",
            "Epoch [24], Batch [16/157], test Loss: 0.4309\n",
            "Epoch [24], Batch [17/157], test Loss: 0.6701\n",
            "Epoch [24], Batch [18/157], test Loss: 0.5259\n",
            "Epoch [24], Batch [19/157], test Loss: 0.5233\n",
            "Epoch [24], Batch [20/157], test Loss: 0.3874\n",
            "Epoch [24], Batch [21/157], test Loss: 0.5771\n",
            "Epoch [24], Batch [22/157], test Loss: 0.3450\n",
            "Epoch [24], Batch [23/157], test Loss: 0.4838\n",
            "Epoch [24], Batch [24/157], test Loss: 0.4821\n",
            "Epoch [24], Batch [25/157], test Loss: 0.3056\n",
            "Epoch [24], Batch [26/157], test Loss: 0.6559\n",
            "Epoch [24], Batch [27/157], test Loss: 0.5323\n",
            "Epoch [24], Batch [28/157], test Loss: 0.3722\n",
            "Epoch [24], Batch [29/157], test Loss: 0.5163\n",
            "Epoch [24], Batch [30/157], test Loss: 0.5718\n",
            "Epoch [24], Batch [31/157], test Loss: 0.4269\n",
            "Epoch [24], Batch [32/157], test Loss: 0.3302\n",
            "Epoch [24], Batch [33/157], test Loss: 0.4046\n",
            "Epoch [24], Batch [34/157], test Loss: 0.6363\n",
            "Epoch [24], Batch [35/157], test Loss: 0.4779\n",
            "Epoch [24], Batch [36/157], test Loss: 0.2649\n",
            "Epoch [24], Batch [37/157], test Loss: 0.3941\n",
            "Epoch [24], Batch [38/157], test Loss: 0.5553\n",
            "Epoch [24], Batch [39/157], test Loss: 0.4470\n",
            "Epoch [24], Batch [40/157], test Loss: 0.3642\n",
            "Epoch [24], Batch [41/157], test Loss: 0.2584\n",
            "Epoch [24], Batch [42/157], test Loss: 0.3649\n",
            "Epoch [24], Batch [43/157], test Loss: 0.6327\n",
            "Epoch [24], Batch [44/157], test Loss: 0.6252\n",
            "Epoch [24], Batch [45/157], test Loss: 0.5589\n",
            "Epoch [24], Batch [46/157], test Loss: 0.3908\n",
            "Epoch [24], Batch [47/157], test Loss: 0.3805\n",
            "Epoch [24], Batch [48/157], test Loss: 0.4088\n",
            "Epoch [24], Batch [49/157], test Loss: 0.4046\n",
            "Epoch [24], Batch [50/157], test Loss: 0.3930\n",
            "Epoch [24], Batch [51/157], test Loss: 0.4314\n",
            "Epoch [24], Batch [52/157], test Loss: 0.3625\n",
            "Epoch [24], Batch [53/157], test Loss: 0.3333\n",
            "Epoch [24], Batch [54/157], test Loss: 0.4794\n",
            "Epoch [24], Batch [55/157], test Loss: 0.4176\n",
            "Epoch [24], Batch [56/157], test Loss: 0.4008\n",
            "Epoch [24], Batch [57/157], test Loss: 0.4107\n",
            "Epoch [24], Batch [58/157], test Loss: 0.3264\n",
            "Epoch [24], Batch [59/157], test Loss: 0.4225\n",
            "Epoch [24], Batch [60/157], test Loss: 0.4428\n",
            "Epoch [24], Batch [61/157], test Loss: 0.4706\n",
            "Epoch [24], Batch [62/157], test Loss: 0.4464\n",
            "Epoch [24], Batch [63/157], test Loss: 0.2077\n",
            "Epoch [24], Batch [64/157], test Loss: 0.4583\n",
            "Epoch [24], Batch [65/157], test Loss: 0.3827\n",
            "Epoch [24], Batch [66/157], test Loss: 0.5754\n",
            "Epoch [24], Batch [67/157], test Loss: 0.3872\n",
            "Epoch [24], Batch [68/157], test Loss: 0.3790\n",
            "Epoch [24], Batch [69/157], test Loss: 0.4642\n",
            "Epoch [24], Batch [70/157], test Loss: 0.4527\n",
            "Epoch [24], Batch [71/157], test Loss: 0.4750\n",
            "Epoch [24], Batch [72/157], test Loss: 0.3678\n",
            "Epoch [24], Batch [73/157], test Loss: 0.2928\n",
            "Epoch [24], Batch [74/157], test Loss: 0.5967\n",
            "Epoch [24], Batch [75/157], test Loss: 0.4867\n",
            "Epoch [24], Batch [76/157], test Loss: 0.4964\n",
            "Epoch [24], Batch [77/157], test Loss: 0.5748\n",
            "Epoch [24], Batch [78/157], test Loss: 0.7157\n",
            "Epoch [24], Batch [79/157], test Loss: 0.2883\n",
            "Epoch [24], Batch [80/157], test Loss: 0.3922\n",
            "Epoch [24], Batch [81/157], test Loss: 0.5945\n",
            "Epoch [24], Batch [82/157], test Loss: 0.5574\n",
            "Epoch [24], Batch [83/157], test Loss: 0.3504\n",
            "Epoch [24], Batch [84/157], test Loss: 0.4704\n",
            "Epoch [24], Batch [85/157], test Loss: 0.5590\n",
            "Epoch [24], Batch [86/157], test Loss: 0.4067\n",
            "Epoch [24], Batch [87/157], test Loss: 0.4377\n",
            "Epoch [24], Batch [88/157], test Loss: 0.4813\n",
            "Epoch [24], Batch [89/157], test Loss: 0.4188\n",
            "Epoch [24], Batch [90/157], test Loss: 0.5557\n",
            "Epoch [24], Batch [91/157], test Loss: 0.3880\n",
            "Epoch [24], Batch [92/157], test Loss: 0.5279\n",
            "Epoch [24], Batch [93/157], test Loss: 0.4821\n",
            "Epoch [24], Batch [94/157], test Loss: 0.6435\n",
            "Epoch [24], Batch [95/157], test Loss: 0.4097\n",
            "Epoch [24], Batch [96/157], test Loss: 0.6795\n",
            "Epoch [24], Batch [97/157], test Loss: 0.4145\n",
            "Epoch [24], Batch [98/157], test Loss: 0.4211\n",
            "Epoch [24], Batch [99/157], test Loss: 0.3445\n",
            "Epoch [24], Batch [100/157], test Loss: 0.5642\n",
            "Epoch [24], Batch [101/157], test Loss: 0.3482\n",
            "Epoch [24], Batch [102/157], test Loss: 0.4928\n",
            "Epoch [24], Batch [103/157], test Loss: 0.4509\n",
            "Epoch [24], Batch [104/157], test Loss: 0.4841\n",
            "Epoch [24], Batch [105/157], test Loss: 0.5850\n",
            "Epoch [24], Batch [106/157], test Loss: 0.4840\n",
            "Epoch [24], Batch [107/157], test Loss: 0.4194\n",
            "Epoch [24], Batch [108/157], test Loss: 0.4486\n",
            "Epoch [24], Batch [109/157], test Loss: 0.3002\n",
            "Epoch [24], Batch [110/157], test Loss: 0.5378\n",
            "Epoch [24], Batch [111/157], test Loss: 0.3147\n",
            "Epoch [24], Batch [112/157], test Loss: 0.5554\n",
            "Epoch [24], Batch [113/157], test Loss: 0.2560\n",
            "Epoch [24], Batch [114/157], test Loss: 0.5635\n",
            "Epoch [24], Batch [115/157], test Loss: 0.4497\n",
            "Epoch [24], Batch [116/157], test Loss: 0.2986\n",
            "Epoch [24], Batch [117/157], test Loss: 0.3898\n",
            "Epoch [24], Batch [118/157], test Loss: 0.4410\n",
            "Epoch [24], Batch [119/157], test Loss: 0.3338\n",
            "Epoch [24], Batch [120/157], test Loss: 0.3311\n",
            "Epoch [24], Batch [121/157], test Loss: 0.5027\n",
            "Epoch [24], Batch [122/157], test Loss: 0.5830\n",
            "Epoch [24], Batch [123/157], test Loss: 0.4275\n",
            "Epoch [24], Batch [124/157], test Loss: 0.3891\n",
            "Epoch [24], Batch [125/157], test Loss: 0.4431\n",
            "Epoch [24], Batch [126/157], test Loss: 0.3676\n",
            "Epoch [24], Batch [127/157], test Loss: 0.3396\n",
            "Epoch [24], Batch [128/157], test Loss: 0.4908\n",
            "Epoch [24], Batch [129/157], test Loss: 0.3084\n",
            "Epoch [24], Batch [130/157], test Loss: 0.5479\n",
            "Epoch [24], Batch [131/157], test Loss: 0.5841\n",
            "Epoch [24], Batch [132/157], test Loss: 0.4849\n",
            "Epoch [24], Batch [133/157], test Loss: 0.4749\n",
            "Epoch [24], Batch [134/157], test Loss: 0.3539\n",
            "Epoch [24], Batch [135/157], test Loss: 0.3590\n",
            "Epoch [24], Batch [136/157], test Loss: 0.4772\n",
            "Epoch [24], Batch [137/157], test Loss: 0.6019\n",
            "Epoch [24], Batch [138/157], test Loss: 0.3717\n",
            "Epoch [24], Batch [139/157], test Loss: 0.3220\n",
            "Epoch [24], Batch [140/157], test Loss: 0.5388\n",
            "Epoch [24], Batch [141/157], test Loss: 0.6075\n",
            "Epoch [24], Batch [142/157], test Loss: 0.3632\n",
            "Epoch [24], Batch [143/157], test Loss: 0.5591\n",
            "Epoch [24], Batch [144/157], test Loss: 0.3985\n",
            "Epoch [24], Batch [145/157], test Loss: 0.5349\n",
            "Epoch [24], Batch [146/157], test Loss: 0.3770\n",
            "Epoch [24], Batch [147/157], test Loss: 0.5278\n",
            "Epoch [24], Batch [148/157], test Loss: 0.4884\n",
            "Epoch [24], Batch [149/157], test Loss: 0.2827\n",
            "Epoch [24], Batch [150/157], test Loss: 0.6798\n",
            "Epoch [24], Batch [151/157], test Loss: 0.3664\n",
            "Epoch [24], Batch [152/157], test Loss: 0.3388\n",
            "Epoch [24], Batch [153/157], test Loss: 0.3338\n",
            "Epoch [24], Batch [154/157], test Loss: 0.5203\n",
            "Epoch [24], Batch [155/157], test Loss: 0.4788\n",
            "Epoch [24], Batch [156/157], test Loss: 0.4276\n",
            "Epoch [24], Batch [157/157], test Loss: 0.4941\n",
            "Accuracy of test set: 84.18%\n",
            "Epoch [25/25] - Train Loss: 0.3974, Train Accuracy: 86.00% - Test Loss: 0.4466, Test Accuracy: 84.18%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_model([train_loader, test_loader], num_epochs=25, learning_rate=0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ceb5783f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceb5783f",
        "outputId": "5666bd3b-88f1-4737-90f3-b827fd2aa4c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test accuracy: 0.8418\n"
          ]
        }
      ],
      "source": [
        "print(f'Final test accuracy: {test_accuracies[-1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e128ed",
      "metadata": {
        "id": "a5e128ed"
      },
      "source": [
        "## Visualization of the labels and predictions\n",
        "\n",
        "In this section, you should visual one image from each class and show both the actual label and the predicted label for that image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6c0b79fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "6c0b79fd",
        "outputId": "13ad712d-1ccf-4689-c9e5-4dc7a2d1a521"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_predictions(model, dataloader, parameters):\n",
        "    class_images = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            outputs = model(x, parameters)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for img, label, pred in zip(x, y, preds):\n",
        "                if label.item() not in class_images:\n",
        "                    class_images[label.item()] = (img, pred.item())\n",
        "\n",
        "                if len(class_images) == 10:\n",
        "                    break\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(10):\n",
        "        img, pred = class_images[i]\n",
        "        img = img.cpu().numpy().squeeze()\n",
        "        actual_label = i\n",
        "        predicted_label = pred\n",
        "\n",
        "        plt.subplot(2, 5, i + 1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f\"Actual: {actual_label}\\nPredicted: {predicted_label}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions(model, test_loader, parameters)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}