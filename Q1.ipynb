{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenperfection/ML/blob/CHW03/Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b46fe41",
      "metadata": {
        "id": "1b46fe41"
      },
      "source": [
        "<h1 align=\"center\">Introduction to Machine Learning - Course Code: 25737</h1>\n",
        "<h4 align=\"center\">Instructor: Dr. Amiri</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
        "<h4 align=\"center\">Computer Assignment 3</h4>\n",
        "<h4 align=\"center\">\n",
        "\n",
        "Question 1\n",
        "\n",
        "</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24a0fc13",
      "metadata": {
        "id": "24a0fc13"
      },
      "source": [
        "# Personal Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44babb65",
      "metadata": {
        "id": "44babb65"
      },
      "outputs": [],
      "source": [
        "# Set your student number\n",
        "student_number = 99102083\n",
        "Name = 'Mohsen'\n",
        "Last_Name = 'Kamalabadi Farahani'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca4a337a",
      "metadata": {
        "id": "ca4a337a"
      },
      "source": [
        "# Rules\n",
        "- You are not allowed to add or remove cells. You **must use the provided space to write your code**. If you don't follow this rule, **your Practical Assignment won't be graded**.  \n",
        "\n",
        "- Collaboration and using the internet is allowed, but your code **must be written by yourself**. **Copying code** from each other or from available resources will result in a **zero score for the assignment**.\n",
        "\n",
        "- You are not allowed to use `torch.nn`, `torch.optim` and any activation function and loss function implemented in torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b76789",
      "metadata": {
        "id": "12b76789",
        "outputId": "88ba0a7f-ecca-4f0c-d48a-92dad6abe156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (1.20.3)\n",
            "Requirement already satisfied: matplotlib in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (3.4.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.20.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.0.4)\n",
            "Requirement already satisfied: six in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: torchvision in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (0.17.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: torch==2.2.2 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (2.2.2)\n",
            "Requirement already satisfied: numpy in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch==2.2.2->torchvision) (4.11.0)\n",
            "Requirement already satisfied: jinja2 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch==2.2.2->torchvision) (2.11.3)\n",
            "Requirement already satisfied: fsspec in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch==2.2.2->torchvision) (2021.8.1)\n",
            "Requirement already satisfied: filelock in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch==2.2.2->torchvision) (3.3.1)\n",
            "Requirement already satisfied: sympy in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch==2.2.2->torchvision) (1.9)\n",
            "Requirement already satisfied: networkx in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch==2.2.2->torchvision) (2.6.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch==2.2.2->torchvision) (1.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch==2.2.2->torchvision) (1.2.1)\n",
            "Requirement already satisfied: torch in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (2.2.2)\n",
            "Requirement already satisfied: sympy in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch) (1.9)\n",
            "Requirement already satisfied: jinja2 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
            "Requirement already satisfied: fsspec in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch) (2021.8.1)\n",
            "Requirement already satisfied: filelock in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.3.1)\n",
            "Requirement already satisfied: networkx in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.6.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (1.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/mohsenfarahani/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install torchvision\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886188c7",
      "metadata": {
        "id": "886188c7"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a0adcc",
      "metadata": {
        "id": "55a0adcc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from typing import Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18510868",
      "metadata": {
        "id": "18510868"
      },
      "source": [
        "## Datasets and Dataloaders\n",
        "\n",
        "Here, we download and load the train and test `FashionMNIST` dataset with the desired transforms. Then, we define the dataloaders for `train` and `test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc8759e2",
      "metadata": {
        "id": "dc8759e2"
      },
      "outputs": [],
      "source": [
        "train_set = FashionMNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_set = FashionMNIST(root='.', train=False, download=True, transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df47fcb",
      "metadata": {
        "id": "5df47fcb"
      },
      "source": [
        "\n",
        "Here you have to calculate the number of classes amd input dimention of the first layer (how many pixels does each image have?)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6763e6",
      "metadata": {
        "id": "8f6763e6",
        "outputId": "d35c335e-aecc-45d3-fe7c-f407c9db88a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 10\n",
            "Input dimension: 784\n"
          ]
        }
      ],
      "source": [
        "## FILL HERE\n",
        "# input_dim = .....\n",
        "# Calculate the number of classes\n",
        "num_classes = len(train_set.classes)\n",
        "\n",
        "# Input dimension\n",
        "input_dim = train_set[0][0].numel()\n",
        "\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Input dimension: {input_dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c695ff60",
      "metadata": {
        "id": "c695ff60"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, 64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, 64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9dac6c2",
      "metadata": {
        "id": "f9dac6c2"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Visualize 1 random image from each class by using `plt.subplots`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d6b0c1",
      "metadata": {
        "id": "e3d6b0c1",
        "outputId": "cee7d7f3-095c-411e-b4d5-82fe1d17a836"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x1080 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "class_images = {}\n",
        "\n",
        "# Iterate through the train_loader to find one image per class\n",
        "for images, labels in train_loader:\n",
        "    for i in range(images.size(0)):\n",
        "        label = labels[i].item()\n",
        "        if label not in class_images:\n",
        "            class_images[label] = images[i]\n",
        "        if len(class_images) == num_classes:\n",
        "            break\n",
        "    if len(class_images) == num_classes:\n",
        "        break\n",
        "\n",
        "# Create a subplot for visualizing the images\n",
        "fig, axes = plt.subplots(1, num_classes, figsize=(15, 15))\n",
        "\n",
        "# Class labels for FashionMNIST\n",
        "class_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Plot each image\n",
        "for i, (label, image) in enumerate(class_images.items()):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(image.squeeze(), cmap='gray')\n",
        "    ax.set_title(class_labels[label])\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a94c5aba",
      "metadata": {
        "id": "a94c5aba"
      },
      "source": [
        "## Initializing model's parameters\n",
        "\n",
        "In this part, we create the model and initialize its parameters and store the values of these parameters in the variable `parameters` which is a dictionary including the weigths and biases of each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d40952",
      "metadata": {
        "id": "e6d40952"
      },
      "outputs": [],
      "source": [
        "def add_linear_layer(parameters: dict, shape, device, i=None):\n",
        "    \"\"\"\n",
        "    This function adds parameters of a linear unit of shape `shape` to the `parameters` dictionary.\n",
        "    \"\"\"\n",
        "    n_in, n_out = shape\n",
        "    with torch.no_grad():\n",
        "        w = torch.zeros(*shape, device=device)\n",
        "        # kaiming initialization for ReLU activations:\n",
        "        bound = 1 / np.sqrt(n_in).item()\n",
        "        w.uniform_(-bound, bound)\n",
        "        b = torch.zeros(n_out, device=device)  # no need to (1, n_out). it will broadcast itself.\n",
        "    w.requires_grad = True\n",
        "    b.requires_grad = True\n",
        "    # `i` is used to give numbers to parameter names\n",
        "    parameters.update({f'w{i}': w, f'b{i}': b})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce914706",
      "metadata": {
        "id": "ce914706"
      },
      "source": [
        "Now we define our neural network with the given layers and add the weights and biases to the dictionary `parameters`. **You are allowed to modify the values of the layers**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f3867d7",
      "metadata": {
        "id": "8f3867d7",
        "outputId": "11d626fa-c4cb-483b-acdc-7686f470144d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['w0', 'b0', 'w1', 'b1', 'w2', 'b2', 'w3', 'b3', 'w4', 'b4'])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# input_dim : input dimention of the first layer, which you have calculated before.\n",
        "layers = [\n",
        "    (input_dim, 512),\n",
        "    (512, 256),\n",
        "    (256, 128),\n",
        "    (128, 64),\n",
        "    (64, num_classes)\n",
        "]\n",
        "num_layers = len(layers)\n",
        "parameters = {}\n",
        "\n",
        "# setting the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# adding the parameters to the dictionary\n",
        "for i, shape in enumerate(layers):\n",
        "    add_linear_layer(parameters, shape, device, i)\n",
        "\n",
        "parameters.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfd2c8e",
      "metadata": {
        "id": "8bfd2c8e"
      },
      "source": [
        "## Defining the required functions\n",
        "\n",
        "In this section, we should define the required functions. For each of these functions, the inputs and the desired outputs are given and you should write all or part of the function. **You are not allowed to use the activation functions and the loss functions implemented in torch**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b413d8",
      "metadata": {
        "id": "f3b413d8"
      },
      "source": [
        "Computing affine and relu outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bebeeb0e",
      "metadata": {
        "id": "bebeeb0e"
      },
      "outputs": [],
      "source": [
        "def affine_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the affine transformation (Wx + b).\n",
        "    x: input tensor of shape (N, d_in)\n",
        "    w: weights tensor of shape (d_in, d_out)\n",
        "    b: bias tensor of shape (d_out)\n",
        "    \"\"\"\n",
        "    return x @ w + b\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    Applies the ReLU activation function element-wise.\n",
        "    x: input tensor\n",
        "    \"\"\"\n",
        "    return torch.maximum(x, torch.zeros_like(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9baa5e",
      "metadata": {
        "id": "5d9baa5e"
      },
      "source": [
        "Function `model` returns output of the whole model for the input `x` using the parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2562962",
      "metadata": {
        "id": "d2562962"
      },
      "outputs": [],
      "source": [
        "def model(x: torch.Tensor, parameters, num_layers=num_layers):\n",
        "    # number of batches\n",
        "    B = x.shape[0]\n",
        "    x = x.view(B, -1)  # Flatten the input\n",
        "\n",
        "    # Iterate through each layer\n",
        "    for i in range(num_layers):\n",
        "        # Get the parameters for the current layer\n",
        "        w = parameters[f'w{i}']\n",
        "        b = parameters[f'b{i}']\n",
        "\n",
        "        # Apply affine transformation\n",
        "        x = affine_forward(x, w, b)\n",
        "\n",
        "        # Apply ReLU activation if it's not the last layer\n",
        "        if i < num_layers - 1:\n",
        "            x = relu(x)\n",
        "\n",
        "    # The final output of the model\n",
        "    output = x\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d17a9b4c",
      "metadata": {
        "id": "d17a9b4c"
      },
      "source": [
        "Implementing cross entropy loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6959621c",
      "metadata": {
        "id": "6959621c"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(scores, y):\n",
        "    \"\"\"\n",
        "    Computes the cross entropy loss.\n",
        "\n",
        "    scores: Tensor of shape (N, C) where N is the batch size and C is the number of classes.\n",
        "    y: Tensor of shape (N,) where each value is the true class label for each sample.\n",
        "\n",
        "    Returns:\n",
        "    loss: Scalar tensor representing the average cross entropy loss over the batch.\n",
        "    \"\"\"\n",
        "    n = len(y)\n",
        "\n",
        "    # Compute the softmax probabilities\n",
        "    exp_scores = torch.exp(scores)\n",
        "    probabilities = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True)\n",
        "\n",
        "    # Compute the log probabilities\n",
        "    log_probabilities = torch.log(probabilities)\n",
        "\n",
        "    # Gather the log probabilities of the correct classes\n",
        "    correct_log_probabilities = log_probabilities[range(n), y]\n",
        "\n",
        "    # Compute the cross entropy loss\n",
        "    loss = -torch.mean(correct_log_probabilities)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a589af",
      "metadata": {
        "id": "15a589af"
      },
      "source": [
        "Implementing a function for optimizing paramters and a function to zeroing out their gradients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3121c147",
      "metadata": {
        "id": "3121c147",
        "outputId": "9c881922-6349-4474-eacb-ac88b50bee4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w0: tensor([[ 0.0225,  0.0388,  0.0159,  ..., -0.0044,  0.0185, -0.0120],\n",
            "        [ 0.0101, -0.0130, -0.0118,  ..., -0.0014,  0.0292,  0.0132],\n",
            "        [ 0.0427, -0.0232, -0.0325,  ..., -0.0144,  0.0271, -0.0336],\n",
            "        [ 0.0179, -0.0163,  0.0100,  ...,  0.0354, -0.0348,  0.0060],\n",
            "        [ 0.0378, -0.0134,  0.0274,  ..., -0.0111, -0.0148,  0.0018]])\n",
            "b0: tensor([ 0.0150, -0.0231,  0.0010,  0.0067, -0.0054])\n",
            "w1: tensor([[ 0.0408, -0.0298, -0.0243,  ...,  0.0234, -0.0433,  0.0371],\n",
            "        [-0.0405, -0.0024, -0.0340,  ...,  0.0468, -0.0005,  0.0201],\n",
            "        [-0.0008,  0.0433, -0.0071,  ...,  0.0314, -0.0205, -0.0156],\n",
            "        [ 0.0043, -0.0359,  0.0151,  ..., -0.0258,  0.0290, -0.0271],\n",
            "        [ 0.0273,  0.0061,  0.0208,  ..., -0.0175,  0.0284,  0.0034]])\n",
            "b1: tensor([-0.0314,  0.0025,  0.0114, -0.0133,  0.0017])\n",
            "w2: tensor([[-2.8407e-02,  1.3652e-02, -1.4689e-02, -3.9676e-02, -3.6970e-03,\n",
            "          4.6598e-03, -7.5488e-02,  3.2247e-02, -1.7365e-02, -6.5694e-03,\n",
            "         -1.8595e-02, -2.9645e-02,  7.1835e-02,  3.3963e-02,  1.4261e-02,\n",
            "          4.6074e-02,  2.6707e-02,  4.4180e-02, -9.9857e-03, -4.2761e-02,\n",
            "         -2.3575e-02,  3.9695e-02,  3.0981e-02,  2.6826e-02,  6.9278e-02,\n",
            "          9.2575e-03, -3.9143e-02, -5.4825e-02,  3.7221e-02,  1.9684e-02,\n",
            "         -3.9858e-02, -4.1640e-02, -1.3322e-02,  9.5286e-03,  2.3121e-02,\n",
            "         -3.4219e-02,  5.6759e-02,  2.2797e-02, -4.7913e-02, -2.3014e-02,\n",
            "         -1.0173e-02, -1.2001e-02, -3.1663e-02, -2.7734e-02, -1.9187e-02,\n",
            "         -4.5496e-02,  4.6305e-02,  4.5687e-02,  3.1078e-02,  3.8854e-02,\n",
            "         -6.4576e-03, -4.2763e-02,  1.9649e-02,  2.8671e-02,  4.0753e-02,\n",
            "          4.7130e-02,  2.6016e-02,  3.1551e-02, -3.9895e-02, -6.9527e-02,\n",
            "          5.2768e-02,  5.6050e-02,  4.5215e-02,  1.1203e-02,  2.1717e-02,\n",
            "          3.6961e-02,  2.5129e-02,  3.2417e-02, -1.1728e-02,  1.4251e-02,\n",
            "         -2.3472e-02, -3.5102e-02,  3.4258e-02,  6.7181e-03,  4.6884e-02,\n",
            "         -5.9846e-02,  3.9527e-02,  1.1164e-02,  4.0140e-02, -1.8825e-02,\n",
            "         -2.6450e-02,  4.3203e-02,  1.4402e-02, -9.3222e-03, -5.5926e-02,\n",
            "          4.9251e-03,  1.6433e-02,  5.7652e-02,  3.2571e-02, -5.3460e-02,\n",
            "          3.8876e-02, -1.3487e-03,  1.9933e-03,  3.2916e-02, -3.0689e-02,\n",
            "          3.3378e-03, -1.1589e-02,  4.9447e-02, -5.2551e-02, -4.9932e-03,\n",
            "         -1.3498e-02, -5.5313e-02,  6.6218e-02,  5.4443e-02, -3.3595e-02,\n",
            "         -1.9126e-02, -7.1603e-04, -2.1546e-02, -3.1311e-02, -2.3037e-02,\n",
            "         -7.3791e-03, -3.0588e-02, -4.6014e-03,  5.0565e-02, -1.3884e-02,\n",
            "          3.5629e-03,  5.5509e-02,  1.6498e-02, -4.3220e-02,  7.1855e-02,\n",
            "          5.9593e-02,  3.9190e-02,  3.9003e-02, -3.1768e-02,  2.5955e-02,\n",
            "          7.8414e-02,  2.4069e-02,  5.9863e-02],\n",
            "        [-5.3802e-02,  4.0746e-02, -4.2462e-02, -3.6164e-02,  4.0511e-02,\n",
            "         -3.1888e-03, -1.5859e-02,  3.5248e-02,  5.3351e-02,  2.8121e-02,\n",
            "         -4.8778e-02,  2.2584e-02,  5.6817e-02, -4.0012e-02, -3.6992e-02,\n",
            "          4.4578e-02, -4.9073e-02,  2.5234e-03, -4.2721e-02, -1.9494e-02,\n",
            "          3.5955e-02,  5.0253e-02,  4.7492e-02,  4.6695e-02,  2.2393e-02,\n",
            "          4.1925e-02,  4.1842e-02, -5.5741e-02,  6.8991e-03,  3.8602e-02,\n",
            "          3.9600e-02, -5.2068e-02, -2.5060e-02, -3.1613e-02, -5.2028e-02,\n",
            "         -7.2980e-02,  7.7476e-03,  1.1639e-02,  1.8526e-02,  1.7864e-02,\n",
            "          4.0687e-02, -2.9543e-02, -2.5728e-02,  2.2826e-02,  3.0486e-02,\n",
            "          4.4808e-02, -4.1555e-04, -2.7302e-02, -4.1137e-02,  5.9977e-02,\n",
            "          3.5750e-02, -6.4117e-02, -4.9825e-02,  2.2778e-02,  5.1410e-02,\n",
            "         -4.7926e-02,  9.2791e-05, -3.4776e-02,  3.0402e-02,  3.8160e-03,\n",
            "         -3.4100e-02,  4.5261e-02, -8.9934e-03,  3.8920e-03,  4.7761e-04,\n",
            "         -3.1944e-02,  5.2463e-02, -2.6172e-02,  1.4522e-02, -2.1211e-02,\n",
            "         -1.3898e-02,  6.5827e-03,  2.2172e-02,  3.1411e-02,  6.9784e-03,\n",
            "          4.2213e-02, -2.7571e-02,  2.4451e-02, -1.1526e-02, -5.3048e-02,\n",
            "         -2.3146e-02,  2.8019e-02,  3.6536e-02,  2.6863e-02, -1.5409e-02,\n",
            "          3.2828e-02, -3.1617e-02, -6.3229e-02, -2.2371e-02,  3.0025e-02,\n",
            "          2.9379e-02, -1.7521e-03,  2.0107e-02, -8.5854e-03,  7.3812e-02,\n",
            "         -3.8926e-02, -2.9119e-02,  5.8381e-02,  5.0444e-02,  5.3817e-02,\n",
            "          6.0618e-02,  2.9011e-02, -4.3866e-02,  1.2174e-02, -4.5202e-02,\n",
            "          5.1143e-02, -9.8329e-04,  3.5171e-02,  5.2882e-02, -3.1464e-02,\n",
            "         -5.0007e-02,  5.1752e-02, -2.9475e-02, -4.0249e-02,  4.3005e-02,\n",
            "          2.3062e-02, -6.9704e-02,  4.5882e-03, -4.5345e-02, -2.0561e-02,\n",
            "          6.9519e-02, -2.4035e-02, -1.5368e-02,  3.4974e-02,  6.5726e-02,\n",
            "          1.6307e-02,  1.2431e-03, -1.9141e-02],\n",
            "        [ 1.9680e-02, -5.5016e-02, -4.8846e-02, -5.1465e-02, -5.4752e-02,\n",
            "         -1.9832e-03,  5.9406e-02,  2.3073e-02, -1.6600e-02, -2.2880e-02,\n",
            "          3.6304e-02,  5.4947e-02,  5.4494e-02,  4.5778e-03,  4.5663e-02,\n",
            "          3.5139e-02, -7.1169e-02,  5.5681e-02, -4.7671e-02,  6.2428e-02,\n",
            "         -4.9152e-02,  1.8533e-02, -2.9737e-02,  6.9124e-02, -4.6500e-02,\n",
            "         -1.3600e-02, -2.8812e-02,  4.6530e-02,  1.8254e-02,  5.7927e-02,\n",
            "         -6.3359e-02,  7.6306e-03,  3.3292e-02,  2.5431e-02, -4.5276e-02,\n",
            "          6.6199e-02,  2.8165e-02,  1.8647e-02,  1.2303e-02,  5.0722e-02,\n",
            "          4.6004e-02, -8.9630e-03, -5.7806e-02, -2.9907e-03, -1.7635e-02,\n",
            "          1.6040e-02,  3.9493e-02, -1.8796e-02, -5.8160e-02, -4.4506e-02,\n",
            "          1.6728e-02,  4.5748e-02,  4.1997e-02,  4.6299e-02,  4.7887e-02,\n",
            "         -7.0997e-03,  1.6475e-02, -4.5932e-02,  1.3042e-02,  2.8727e-02,\n",
            "          1.1066e-02,  7.2791e-02, -2.4921e-02, -1.3709e-03, -4.8634e-02,\n",
            "          6.2167e-03, -5.6064e-02,  1.8484e-02,  2.2326e-02, -2.4709e-02,\n",
            "          1.7361e-02,  4.0488e-02,  4.9542e-03, -2.1597e-02, -3.0101e-04,\n",
            "         -5.9454e-02, -1.4659e-02,  5.2159e-02,  4.7689e-02, -2.1803e-02,\n",
            "          9.0803e-03,  2.0027e-02, -2.2905e-02,  4.4918e-02, -3.9069e-02,\n",
            "          1.8567e-02, -1.6129e-02, -2.4760e-02,  1.9162e-02, -4.7722e-03,\n",
            "          3.1584e-02, -1.3183e-02, -5.7020e-02, -7.9039e-02,  2.0217e-03,\n",
            "          3.3042e-02,  1.0690e-02, -2.1143e-02,  6.6452e-02,  3.3117e-02,\n",
            "          4.9859e-02,  3.2951e-02, -4.7354e-02,  6.5337e-02, -4.1941e-02,\n",
            "          1.2287e-02,  3.3899e-02,  5.2713e-02, -4.6980e-02,  2.0925e-02,\n",
            "          8.0183e-02, -4.6460e-02, -3.8827e-02, -4.0219e-02,  9.6642e-03,\n",
            "          4.5783e-02, -4.5252e-02, -7.2427e-02, -2.1852e-02,  3.6795e-02,\n",
            "          3.2001e-02,  2.3161e-02,  3.9430e-02, -2.0212e-02, -2.3091e-02,\n",
            "         -1.4327e-02, -3.6639e-04,  4.7389e-02],\n",
            "        [ 6.4195e-02,  1.9776e-02, -1.9458e-02, -3.1113e-02,  2.4305e-02,\n",
            "         -2.4035e-02,  4.0725e-02, -5.4712e-02,  5.0339e-02,  1.8090e-02,\n",
            "         -1.7345e-02,  1.6779e-02,  1.9199e-02, -5.2988e-02,  5.0216e-02,\n",
            "          4.8884e-02, -1.9340e-02,  4.8991e-02,  4.8232e-03, -2.8441e-02,\n",
            "          3.2866e-02, -6.6083e-02,  1.2857e-02,  9.7219e-03, -5.9444e-02,\n",
            "          7.7651e-03, -2.6664e-02, -4.1082e-02,  5.5432e-03,  1.9019e-02,\n",
            "          4.6733e-02, -2.1063e-02,  5.0199e-02,  3.8959e-02, -4.5906e-02,\n",
            "          3.3413e-02,  3.1864e-02,  1.1166e-02, -2.5158e-02,  3.0267e-02,\n",
            "          2.5361e-02,  3.2159e-02,  5.5609e-03,  1.4138e-02, -5.3028e-02,\n",
            "          1.6629e-02, -4.7282e-03, -5.2630e-02,  2.8398e-03,  4.5005e-02,\n",
            "          3.3313e-02,  2.4622e-02,  4.8265e-02,  3.0553e-02,  3.8043e-02,\n",
            "         -3.8634e-03,  2.7384e-02, -2.8775e-02, -5.5300e-02, -3.7452e-02,\n",
            "         -2.1835e-02,  1.1771e-02, -3.8845e-02, -3.4332e-02, -4.1588e-02,\n",
            "         -1.0742e-02,  3.2091e-02, -4.8066e-02, -3.1871e-02,  1.6206e-02,\n",
            "          3.7289e-02,  2.7605e-02, -4.8692e-02, -6.8424e-03, -1.9065e-02,\n",
            "         -2.7230e-02,  6.9269e-02,  2.8235e-02, -2.0590e-02, -3.1174e-02,\n",
            "          3.5856e-02, -1.2928e-02,  1.8669e-02,  4.9909e-02,  2.7424e-02,\n",
            "         -5.3999e-02,  7.2501e-02, -2.1992e-02,  2.2468e-02, -3.8053e-02,\n",
            "         -1.0789e-02, -4.0036e-02,  3.9266e-02,  5.6119e-02,  3.0338e-04,\n",
            "         -2.0750e-02, -1.1210e-02,  1.3375e-03,  9.4093e-03, -1.0321e-02,\n",
            "          2.9274e-02,  4.4933e-02, -2.2313e-02, -4.3322e-02, -3.4679e-02,\n",
            "         -4.6594e-02,  5.1443e-02, -1.0160e-02,  3.0953e-02,  4.4895e-02,\n",
            "          2.3085e-02, -9.9798e-03, -3.6974e-02, -1.6501e-02,  6.7316e-03,\n",
            "         -4.2634e-02, -3.4055e-03, -4.1483e-02,  2.5993e-02, -2.1662e-02,\n",
            "         -2.5499e-02,  5.2941e-02, -3.6872e-02,  7.1894e-02,  1.1013e-02,\n",
            "          1.2161e-02,  3.0222e-02,  1.4353e-02],\n",
            "        [-3.9207e-02,  4.1935e-02, -9.4013e-03, -2.9262e-02,  4.8712e-02,\n",
            "          5.0108e-03,  6.4252e-02, -1.8692e-02, -4.9949e-02,  2.7023e-02,\n",
            "          6.1101e-02, -6.7355e-02, -4.0448e-02, -1.4031e-02, -5.8194e-03,\n",
            "          3.4621e-02, -1.8405e-02,  4.8211e-02,  2.3745e-02, -1.8370e-02,\n",
            "          3.8688e-02, -2.8832e-02, -6.4057e-03,  3.7705e-02, -6.9931e-02,\n",
            "         -5.4801e-02, -3.7206e-02,  4.5621e-02,  5.6413e-02, -3.1117e-02,\n",
            "          3.6567e-02, -2.0963e-02,  2.1340e-02, -3.2882e-02, -3.6104e-02,\n",
            "         -2.8452e-02,  5.5241e-03,  2.2797e-02,  5.3030e-02,  1.6366e-02,\n",
            "          2.7473e-02,  1.5245e-02, -5.7429e-02,  9.5221e-03,  6.8307e-02,\n",
            "         -6.1652e-02, -5.0133e-02,  3.1577e-02,  2.0621e-02,  5.6936e-02,\n",
            "         -6.6095e-02,  4.4089e-02,  4.6562e-02, -4.1868e-02, -1.6447e-02,\n",
            "          6.0772e-02, -1.9915e-02,  7.4368e-03,  6.3459e-02, -2.2662e-02,\n",
            "         -5.2943e-02, -6.0185e-02, -3.3749e-02,  5.3198e-02,  6.3749e-02,\n",
            "          4.2165e-03,  4.8595e-02, -3.6041e-02,  4.0817e-03,  5.2848e-02,\n",
            "          1.8805e-03, -1.8560e-02,  6.5415e-03,  3.7246e-04,  1.1946e-03,\n",
            "         -3.4649e-02,  3.3336e-02, -5.6438e-02,  9.5984e-03,  3.3033e-02,\n",
            "         -7.2164e-03,  1.8881e-02, -9.6156e-03, -2.1472e-02, -4.7085e-02,\n",
            "          2.0869e-03, -3.2501e-02,  1.6807e-02,  1.1769e-02, -4.2051e-02,\n",
            "          4.8529e-02,  1.2456e-02, -4.0941e-02,  3.7852e-02, -5.4740e-02,\n",
            "         -4.0281e-02,  2.3004e-02, -2.8194e-02, -1.3471e-02,  4.9747e-03,\n",
            "         -2.9401e-02,  6.8261e-02,  5.4518e-02,  6.1543e-02,  1.6593e-02,\n",
            "          5.2592e-03, -6.8264e-02,  2.3864e-02,  5.5956e-02,  4.1642e-02,\n",
            "         -4.4056e-02, -3.4855e-02, -3.5747e-02,  3.1919e-02,  9.9579e-03,\n",
            "         -6.8685e-03, -1.8027e-02, -1.1313e-03, -8.8602e-03,  6.8462e-02,\n",
            "         -1.7459e-02,  5.1485e-02, -4.7582e-02, -3.4715e-02, -4.4876e-02,\n",
            "          5.2442e-02, -2.7335e-02,  4.0659e-02]])\n",
            "b2: tensor([ 0.0258, -0.0079, -0.0091, -0.0046, -0.0003])\n",
            "w3: tensor([[-0.0551,  0.0692, -0.0050,  0.0088, -0.0478,  0.0095, -0.0326, -0.0596,\n",
            "          0.0688, -0.0542, -0.0485,  0.0312, -0.0307,  0.0748, -0.0404, -0.0573,\n",
            "         -0.0657, -0.0979,  0.0354, -0.0751, -0.0895, -0.0256, -0.0685, -0.0459,\n",
            "          0.0077,  0.0437, -0.0767, -0.0624, -0.0085, -0.0060, -0.0755,  0.0598,\n",
            "         -0.0698,  0.0124, -0.0254,  0.0060,  0.0729,  0.0707, -0.0059, -0.0538,\n",
            "          0.0114, -0.0682, -0.0052,  0.0693, -0.0048, -0.0793, -0.0133,  0.0382,\n",
            "          0.0404, -0.0526, -0.0221,  0.0866,  0.0712,  0.0883, -0.0101,  0.0806,\n",
            "         -0.0146,  0.0541, -0.0425,  0.0499,  0.0771,  0.0535, -0.0580,  0.0614],\n",
            "        [ 0.0389,  0.0344, -0.0744, -0.0409, -0.0471,  0.0286,  0.0855, -0.0315,\n",
            "          0.0680, -0.0903,  0.0741,  0.0569,  0.0050,  0.0786, -0.0355,  0.0438,\n",
            "          0.0010,  0.0269,  0.0603,  0.0111,  0.0852, -0.0580,  0.0382,  0.0782,\n",
            "         -0.0030, -0.0033, -0.0072, -0.0364,  0.0190, -0.0181, -0.0723,  0.0505,\n",
            "          0.0017, -0.0538,  0.0647,  0.0086,  0.0755, -0.0551, -0.0055, -0.0400,\n",
            "          0.0746, -0.0782,  0.0375,  0.0234,  0.0147,  0.0409,  0.0212,  0.0188,\n",
            "          0.1038, -0.0563,  0.0131, -0.0891, -0.0448, -0.0135, -0.0554, -0.0534,\n",
            "         -0.0497, -0.0112,  0.0574,  0.0098, -0.0438,  0.0872, -0.0331, -0.0526],\n",
            "        [-0.0001,  0.0340,  0.0713, -0.0365, -0.0135,  0.0096,  0.0141, -0.0722,\n",
            "         -0.0040,  0.0110, -0.0237, -0.0505, -0.0109, -0.0539,  0.0656, -0.0404,\n",
            "         -0.0595,  0.0617, -0.0366, -0.0293,  0.0116,  0.0417, -0.0664, -0.0006,\n",
            "         -0.0065, -0.0375,  0.0698,  0.0546,  0.0413, -0.0582, -0.0569, -0.0369,\n",
            "         -0.0183, -0.0593, -0.0579, -0.0856, -0.0350,  0.0765,  0.0241,  0.0306,\n",
            "          0.0473,  0.0198,  0.0010,  0.0523, -0.0879,  0.0383, -0.0281, -0.0469,\n",
            "         -0.0366, -0.0555, -0.0945, -0.0253, -0.0819,  0.0288, -0.0150,  0.0315,\n",
            "         -0.0111, -0.0226, -0.0118, -0.0848,  0.0142, -0.0396,  0.0148, -0.0185],\n",
            "        [ 0.0191,  0.0250,  0.0057, -0.0324,  0.0816, -0.0426,  0.0272, -0.0718,\n",
            "          0.0495, -0.0055, -0.0752,  0.0638,  0.0377, -0.0586, -0.0272,  0.0316,\n",
            "          0.0287, -0.0157, -0.0548,  0.0843,  0.0886, -0.0530,  0.0610, -0.0953,\n",
            "          0.0162,  0.0738,  0.0169, -0.0568, -0.0727,  0.0079,  0.0205, -0.0046,\n",
            "         -0.0669,  0.0402, -0.0328, -0.0468, -0.0013, -0.0509, -0.0793, -0.0555,\n",
            "         -0.0360, -0.0227,  0.0139,  0.0422, -0.0004, -0.0495, -0.0558, -0.0324,\n",
            "         -0.0805,  0.0725, -0.0481,  0.0581, -0.0825,  0.0375, -0.0396,  0.0930,\n",
            "          0.0081, -0.0343, -0.0918, -0.0238, -0.0331,  0.0694, -0.0246,  0.0714],\n",
            "        [-0.0397,  0.0032, -0.0059,  0.0273,  0.0571, -0.0347,  0.0407,  0.0840,\n",
            "         -0.0682,  0.0688,  0.0327, -0.0691,  0.0114,  0.0079,  0.0689, -0.0372,\n",
            "         -0.0005,  0.0020,  0.0025, -0.0216, -0.0767, -0.0026, -0.0062,  0.0236,\n",
            "          0.0473,  0.0400,  0.0633, -0.0597,  0.0765,  0.0631,  0.0741, -0.0066,\n",
            "          0.0002, -0.0650,  0.0885, -0.0045, -0.0768, -0.1104, -0.0365,  0.0956,\n",
            "          0.0826, -0.0643,  0.0273, -0.0331,  0.0151, -0.0876,  0.0745, -0.0398,\n",
            "         -0.0085, -0.0813,  0.0542, -0.0279,  0.0410, -0.0401, -0.0769,  0.0058,\n",
            "         -0.0562, -0.0840, -0.0851, -0.0688, -0.0299, -0.0687,  0.0008, -0.0018]])\n",
            "b3: tensor([-0.0008,  0.0043, -0.0044,  0.0132, -0.0031])\n",
            "w4: tensor([[ 0.0371,  0.0041, -0.0310, -0.0378, -0.0708, -0.0934,  0.0072,  0.0777,\n",
            "          0.0245,  0.0501],\n",
            "        [-0.0982, -0.0415,  0.0360, -0.0917, -0.0039, -0.0767, -0.1050, -0.0718,\n",
            "         -0.0853,  0.0167],\n",
            "        [ 0.0904, -0.0219, -0.0755, -0.0865,  0.0792,  0.1271,  0.0683, -0.0739,\n",
            "         -0.1180,  0.0907],\n",
            "        [ 0.0391,  0.0125, -0.0960,  0.1050, -0.0975, -0.0239, -0.1402,  0.0382,\n",
            "          0.1356,  0.0463],\n",
            "        [-0.0683,  0.0446, -0.0387,  0.0761, -0.0136, -0.0561,  0.0173,  0.1189,\n",
            "         -0.0543,  0.0965]])\n",
            "b4: tensor([-8.4786e-05, -2.8411e-03, -7.4635e-03,  8.5165e-03,  3.6033e-02])\n"
          ]
        }
      ],
      "source": [
        "def sgd_optimizer(parameters: Dict[str, torch.Tensor], learning_rate=0.001):\n",
        "    '''This function gets the parameters and a learning rate. Then updates the parameters using their\n",
        "    gradient. Finally, you should zero the gradients of the parameters after updating\n",
        "    the parameter value.'''\n",
        "    ## FILL HERE\n",
        "    for param in parameters.values():\n",
        "        if param.requires_grad:\n",
        "            # Update parameter using its gradient\n",
        "            param.data -= learning_rate * param.grad\n",
        "\n",
        "            # Zero the gradient\n",
        "            param.grad.zero_()\n",
        "\n",
        "# Example usage\n",
        "# Assuming `parameters` contains the model's parameters and they have been through a backward pass\n",
        "# Create some dummy data to simulate gradients\n",
        "for param in parameters.values():\n",
        "    param.grad = torch.randn_like(param)  # Simulate some gradients\n",
        "\n",
        "# Run the SGD optimizer\n",
        "sgd_optimizer(parameters, learning_rate=0.01)\n",
        "\n",
        "# Print updated parameters to see changes\n",
        "for key, param in parameters.items():\n",
        "    print(f\"{key}: {param.data[:5]}\")  # Print first 5 elements of each parameter tensor to check updates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e17b4cf8",
      "metadata": {
        "id": "e17b4cf8"
      },
      "source": [
        "Training functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c0f03b",
      "metadata": {
        "id": "76c0f03b"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_pred: np.ndarray, y_true: np.ndarray):\n",
        "    ## FILL HERE\n",
        "    correct_predictions = np.sum(y_pred == y_true)\n",
        "    total_predictions = len(y_true)\n",
        "    acc = correct_predictions / total_predictions\n",
        "    return acc\n",
        "\n",
        "def train(train_loader, learning_rate=0.001, epoch=None):\n",
        "    '''This function implements the training loop for a single epoch. For each batch you should do the following:\n",
        "        1- Calculate the output of the model to the given input batch\n",
        "        2- Calculate the loss based on the model output\n",
        "        3- Update the gradients using backward method\n",
        "        4- Optimize the model parameters using the sgd_optimizer function defined previously\n",
        "        5- Print the train loss (Show the epoch and batch as well)\n",
        "        '''\n",
        "    train_loss = 0\n",
        "    N_train = len(train_loader.dataset)\n",
        "\n",
        "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
        "    # for calculating the accuracy later\n",
        "    Y = []\n",
        "    Y_pred = []\n",
        "\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1- Calculate the output of the model to the given input batch\n",
        "        p = model(x, parameters)\n",
        "\n",
        "        # 2- Calculate the loss based on the model output\n",
        "        loss = cross_entropy_loss(p, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 3- Update the gradients using backward method\n",
        "        loss.backward()\n",
        "\n",
        "        # 4- Optimize the model parameters using the sgd_optimizer function defined previously\n",
        "        sgd_optimizer(parameters, learning_rate)\n",
        "\n",
        "        # 5- Print the train loss (Show the epoch and batch as well)\n",
        "        if (i + 1) % 10 == 0 or (i + 1) == len(train_loader):\n",
        "            print(f'Epoch [{epoch}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        y_pred = p.argmax(dim=-1)\n",
        "        Y.append(y.cpu().numpy())\n",
        "        Y_pred.append(y_pred.cpu().numpy())\n",
        "\n",
        "    Y = np.concatenate(Y)\n",
        "    Y_pred = np.concatenate(Y_pred)\n",
        "    acc = accuracy(Y_pred, Y)\n",
        "    train_loss /= len(train_loader)  # Average loss over the epoch\n",
        "    print(f'Accuracy of train set: {acc * 100:.2f}%')\n",
        "    return train_loss, acc\n",
        "\n",
        "\n",
        "def validate(loader, epoch=None, set_name=None):\n",
        "    '''This function validates the model on the test dataloader. The function goes through each batch and does\n",
        "    the following on each batch:\n",
        "        1- Calculate the model output\n",
        "        2- Calculate the loss using the model output\n",
        "        3- Print the loss for each batch and epoch\n",
        "\n",
        "    Finally, the function calculates the model accuracy.'''\n",
        "    total_loss = 0\n",
        "    N = len(loader.dataset)\n",
        "\n",
        "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
        "    # for calculating the accuracy later\n",
        "    Y = []\n",
        "    Y_pred = []\n",
        "\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1- Calculate the model output\n",
        "        p = model(x, parameters)\n",
        "\n",
        "        # 2- Calculate the loss using the model output\n",
        "        loss = cross_entropy_loss(p, y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 3- Print the loss for each batch and epoch\n",
        "        print(f'Epoch [{epoch}], Batch [{i+1}/{len(loader)}], {set_name} Loss: {loss.item():.4f}')\n",
        "\n",
        "        y_pred = p.argmax(dim=-1)\n",
        "        Y.append(y.cpu().numpy())\n",
        "        Y_pred.append(y_pred.cpu().numpy())\n",
        "\n",
        "    # Concatenate the lists of labels and predictions\n",
        "    Y = np.concatenate(Y)\n",
        "    Y_pred = np.concatenate(Y_pred)\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    acc = accuracy(Y_pred, Y)\n",
        "\n",
        "    # Average loss over all batches\n",
        "    total_loss /= len(loader)\n",
        "\n",
        "    # Print the accuracy\n",
        "    print(f'Accuracy of {set_name} set: {acc * 100:.2f}%')\n",
        "\n",
        "    return total_loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ebb4b6",
      "metadata": {
        "id": "87ebb4b6"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d4eb0b",
      "metadata": {
        "id": "28d4eb0b"
      },
      "outputs": [],
      "source": [
        "def train_model(dataloaders, num_epochs, learning_rate=0.001, model_name='pytorch_model'):\n",
        "    '''This function trains the model for the number of epochs given and stores, calculates and prints the train\n",
        "    and test losses and accuracies. Finally, it plots the accuracy and loss history for training and test sets'''\n",
        "    train_loader, test_loader = dataloaders\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        train_loss, train_acc = train(train_loader, learning_rate, epoch)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        test_loss, test_acc = validate(test_loader, epoch, set_name='test')\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
        "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc*100:.2f}% - '\n",
        "              f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%')\n",
        "\n",
        "    # Plot the loss history of training and test sets\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss History')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(range(1, num_epochs + 1), test_accuracies, label='Test Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy History')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec4bdd2",
      "metadata": {
        "id": "2ec4bdd2",
        "outputId": "6e9e7859-83f9-44da-b130-81426a19f477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0], Batch [10/938], Loss: 2.2782\n",
            "Epoch [0], Batch [20/938], Loss: 2.2754\n",
            "Epoch [0], Batch [30/938], Loss: 2.2780\n",
            "Epoch [0], Batch [40/938], Loss: 2.2761\n",
            "Epoch [0], Batch [50/938], Loss: 2.2701\n",
            "Epoch [0], Batch [60/938], Loss: 2.2751\n",
            "Epoch [0], Batch [70/938], Loss: 2.2719\n",
            "Epoch [0], Batch [80/938], Loss: 2.2713\n",
            "Epoch [0], Batch [90/938], Loss: 2.2690\n",
            "Epoch [0], Batch [100/938], Loss: 2.2701\n",
            "Epoch [0], Batch [110/938], Loss: 2.2734\n",
            "Epoch [0], Batch [120/938], Loss: 2.2646\n",
            "Epoch [0], Batch [130/938], Loss: 2.2652\n",
            "Epoch [0], Batch [140/938], Loss: 2.2633\n",
            "Epoch [0], Batch [150/938], Loss: 2.2736\n",
            "Epoch [0], Batch [160/938], Loss: 2.2616\n",
            "Epoch [0], Batch [170/938], Loss: 2.2556\n",
            "Epoch [0], Batch [180/938], Loss: 2.2505\n",
            "Epoch [0], Batch [190/938], Loss: 2.2562\n",
            "Epoch [0], Batch [200/938], Loss: 2.2533\n",
            "Epoch [0], Batch [210/938], Loss: 2.2676\n",
            "Epoch [0], Batch [220/938], Loss: 2.2566\n",
            "Epoch [0], Batch [230/938], Loss: 2.2556\n",
            "Epoch [0], Batch [240/938], Loss: 2.2490\n",
            "Epoch [0], Batch [250/938], Loss: 2.2511\n",
            "Epoch [0], Batch [260/938], Loss: 2.2547\n",
            "Epoch [0], Batch [270/938], Loss: 2.2414\n",
            "Epoch [0], Batch [280/938], Loss: 2.2522\n",
            "Epoch [0], Batch [290/938], Loss: 2.2354\n",
            "Epoch [0], Batch [300/938], Loss: 2.2394\n",
            "Epoch [0], Batch [310/938], Loss: 2.2493\n",
            "Epoch [0], Batch [320/938], Loss: 2.2331\n",
            "Epoch [0], Batch [330/938], Loss: 2.2418\n",
            "Epoch [0], Batch [340/938], Loss: 2.2232\n",
            "Epoch [0], Batch [350/938], Loss: 2.2243\n",
            "Epoch [0], Batch [360/938], Loss: 2.2308\n",
            "Epoch [0], Batch [370/938], Loss: 2.2435\n",
            "Epoch [0], Batch [380/938], Loss: 2.2402\n",
            "Epoch [0], Batch [390/938], Loss: 2.1970\n",
            "Epoch [0], Batch [400/938], Loss: 2.2199\n",
            "Epoch [0], Batch [410/938], Loss: 2.2083\n",
            "Epoch [0], Batch [420/938], Loss: 2.2038\n",
            "Epoch [0], Batch [430/938], Loss: 2.2067\n",
            "Epoch [0], Batch [440/938], Loss: 2.2155\n",
            "Epoch [0], Batch [450/938], Loss: 2.2097\n",
            "Epoch [0], Batch [460/938], Loss: 2.1825\n",
            "Epoch [0], Batch [470/938], Loss: 2.1945\n",
            "Epoch [0], Batch [480/938], Loss: 2.2056\n",
            "Epoch [0], Batch [490/938], Loss: 2.1879\n",
            "Epoch [0], Batch [500/938], Loss: 2.1665\n",
            "Epoch [0], Batch [510/938], Loss: 2.1826\n",
            "Epoch [0], Batch [520/938], Loss: 2.1737\n",
            "Epoch [0], Batch [530/938], Loss: 2.1663\n",
            "Epoch [0], Batch [540/938], Loss: 2.1701\n",
            "Epoch [0], Batch [550/938], Loss: 2.1511\n",
            "Epoch [0], Batch [560/938], Loss: 2.1808\n",
            "Epoch [0], Batch [570/938], Loss: 2.1693\n",
            "Epoch [0], Batch [580/938], Loss: 2.1278\n",
            "Epoch [0], Batch [590/938], Loss: 2.1569\n",
            "Epoch [0], Batch [600/938], Loss: 2.1278\n",
            "Epoch [0], Batch [610/938], Loss: 2.1207\n",
            "Epoch [0], Batch [620/938], Loss: 2.1460\n",
            "Epoch [0], Batch [630/938], Loss: 2.1047\n",
            "Epoch [0], Batch [640/938], Loss: 2.0915\n",
            "Epoch [0], Batch [650/938], Loss: 2.1244\n",
            "Epoch [0], Batch [660/938], Loss: 2.0941\n",
            "Epoch [0], Batch [670/938], Loss: 2.1143\n",
            "Epoch [0], Batch [680/938], Loss: 2.0995\n",
            "Epoch [0], Batch [690/938], Loss: 2.0996\n",
            "Epoch [0], Batch [700/938], Loss: 2.0725\n",
            "Epoch [0], Batch [710/938], Loss: 2.0821\n",
            "Epoch [0], Batch [720/938], Loss: 2.0640\n",
            "Epoch [0], Batch [730/938], Loss: 2.0275\n",
            "Epoch [0], Batch [740/938], Loss: 2.0265\n",
            "Epoch [0], Batch [750/938], Loss: 2.0375\n",
            "Epoch [0], Batch [760/938], Loss: 2.0049\n",
            "Epoch [0], Batch [770/938], Loss: 2.0420\n",
            "Epoch [0], Batch [780/938], Loss: 2.0569\n",
            "Epoch [0], Batch [790/938], Loss: 2.0032\n",
            "Epoch [0], Batch [800/938], Loss: 2.0257\n",
            "Epoch [0], Batch [810/938], Loss: 2.0074\n",
            "Epoch [0], Batch [820/938], Loss: 1.9694\n",
            "Epoch [0], Batch [830/938], Loss: 1.9464\n",
            "Epoch [0], Batch [840/938], Loss: 1.9546\n",
            "Epoch [0], Batch [850/938], Loss: 1.9631\n",
            "Epoch [0], Batch [860/938], Loss: 1.9198\n",
            "Epoch [0], Batch [870/938], Loss: 1.8928\n",
            "Epoch [0], Batch [880/938], Loss: 1.9129\n",
            "Epoch [0], Batch [890/938], Loss: 1.9130\n",
            "Epoch [0], Batch [900/938], Loss: 1.8648\n",
            "Epoch [0], Batch [910/938], Loss: 1.8606\n",
            "Epoch [0], Batch [920/938], Loss: 1.8895\n",
            "Epoch [0], Batch [930/938], Loss: 1.8198\n",
            "Epoch [0], Batch [938/938], Loss: 1.8590\n",
            "Accuracy of train set: 27.53%\n",
            "Epoch [0], Batch [1/157], test Loss: 1.8449\n",
            "Epoch [0], Batch [2/157], test Loss: 1.8303\n",
            "Epoch [0], Batch [3/157], test Loss: 1.8669\n",
            "Epoch [0], Batch [4/157], test Loss: 1.8355\n",
            "Epoch [0], Batch [5/157], test Loss: 1.8465\n",
            "Epoch [0], Batch [6/157], test Loss: 1.7881\n",
            "Epoch [0], Batch [7/157], test Loss: 1.8959\n",
            "Epoch [0], Batch [8/157], test Loss: 1.8891\n",
            "Epoch [0], Batch [9/157], test Loss: 1.8490\n",
            "Epoch [0], Batch [10/157], test Loss: 1.8533\n",
            "Epoch [0], Batch [11/157], test Loss: 1.8595\n",
            "Epoch [0], Batch [12/157], test Loss: 1.8339\n",
            "Epoch [0], Batch [13/157], test Loss: 1.8647\n",
            "Epoch [0], Batch [14/157], test Loss: 1.8973\n",
            "Epoch [0], Batch [15/157], test Loss: 1.8604\n",
            "Epoch [0], Batch [16/157], test Loss: 1.8233\n",
            "Epoch [0], Batch [17/157], test Loss: 1.8390\n",
            "Epoch [0], Batch [18/157], test Loss: 1.8205\n",
            "Epoch [0], Batch [19/157], test Loss: 1.7532\n",
            "Epoch [0], Batch [20/157], test Loss: 1.8611\n",
            "Epoch [0], Batch [21/157], test Loss: 1.8802\n",
            "Epoch [0], Batch [22/157], test Loss: 1.8243\n",
            "Epoch [0], Batch [23/157], test Loss: 1.8735\n",
            "Epoch [0], Batch [24/157], test Loss: 1.8941\n",
            "Epoch [0], Batch [25/157], test Loss: 1.8859\n",
            "Epoch [0], Batch [26/157], test Loss: 1.8357\n",
            "Epoch [0], Batch [27/157], test Loss: 1.8319\n",
            "Epoch [0], Batch [28/157], test Loss: 1.8702\n",
            "Epoch [0], Batch [29/157], test Loss: 1.8249\n",
            "Epoch [0], Batch [30/157], test Loss: 1.8120\n",
            "Epoch [0], Batch [31/157], test Loss: 1.8243\n",
            "Epoch [0], Batch [32/157], test Loss: 1.8561\n",
            "Epoch [0], Batch [33/157], test Loss: 1.8536\n",
            "Epoch [0], Batch [34/157], test Loss: 1.8782\n",
            "Epoch [0], Batch [35/157], test Loss: 1.8648\n",
            "Epoch [0], Batch [36/157], test Loss: 1.8431\n",
            "Epoch [0], Batch [37/157], test Loss: 1.8657\n",
            "Epoch [0], Batch [38/157], test Loss: 1.8728\n",
            "Epoch [0], Batch [39/157], test Loss: 1.8503\n",
            "Epoch [0], Batch [40/157], test Loss: 1.8838\n",
            "Epoch [0], Batch [41/157], test Loss: 1.7973\n",
            "Epoch [0], Batch [42/157], test Loss: 1.8366\n",
            "Epoch [0], Batch [43/157], test Loss: 1.8429\n",
            "Epoch [0], Batch [44/157], test Loss: 1.8220\n",
            "Epoch [0], Batch [45/157], test Loss: 1.8523\n",
            "Epoch [0], Batch [46/157], test Loss: 1.8449\n",
            "Epoch [0], Batch [47/157], test Loss: 1.8609\n",
            "Epoch [0], Batch [48/157], test Loss: 1.8484\n",
            "Epoch [0], Batch [49/157], test Loss: 1.8204\n",
            "Epoch [0], Batch [50/157], test Loss: 1.8415\n",
            "Epoch [0], Batch [51/157], test Loss: 1.8703\n",
            "Epoch [0], Batch [52/157], test Loss: 1.8954\n",
            "Epoch [0], Batch [53/157], test Loss: 1.8182\n",
            "Epoch [0], Batch [54/157], test Loss: 1.8775\n",
            "Epoch [0], Batch [55/157], test Loss: 1.8446\n",
            "Epoch [0], Batch [56/157], test Loss: 1.8674\n",
            "Epoch [0], Batch [57/157], test Loss: 1.8392\n",
            "Epoch [0], Batch [58/157], test Loss: 1.8495\n",
            "Epoch [0], Batch [59/157], test Loss: 1.8287\n",
            "Epoch [0], Batch [60/157], test Loss: 1.8262\n",
            "Epoch [0], Batch [61/157], test Loss: 1.8393\n",
            "Epoch [0], Batch [62/157], test Loss: 1.7978\n",
            "Epoch [0], Batch [63/157], test Loss: 1.8469\n",
            "Epoch [0], Batch [64/157], test Loss: 1.8246\n",
            "Epoch [0], Batch [65/157], test Loss: 1.9101\n",
            "Epoch [0], Batch [66/157], test Loss: 1.8675\n",
            "Epoch [0], Batch [67/157], test Loss: 1.8637\n",
            "Epoch [0], Batch [68/157], test Loss: 1.8963\n",
            "Epoch [0], Batch [69/157], test Loss: 1.8439\n",
            "Epoch [0], Batch [70/157], test Loss: 1.8531\n",
            "Epoch [0], Batch [71/157], test Loss: 1.8651\n",
            "Epoch [0], Batch [72/157], test Loss: 1.7951\n",
            "Epoch [0], Batch [73/157], test Loss: 1.8914\n",
            "Epoch [0], Batch [74/157], test Loss: 1.8918\n",
            "Epoch [0], Batch [75/157], test Loss: 1.8341\n",
            "Epoch [0], Batch [76/157], test Loss: 1.8263\n",
            "Epoch [0], Batch [77/157], test Loss: 1.8046\n",
            "Epoch [0], Batch [78/157], test Loss: 1.9014\n",
            "Epoch [0], Batch [79/157], test Loss: 1.8607\n",
            "Epoch [0], Batch [80/157], test Loss: 1.8203\n",
            "Epoch [0], Batch [81/157], test Loss: 1.8246\n",
            "Epoch [0], Batch [82/157], test Loss: 1.8454\n",
            "Epoch [0], Batch [83/157], test Loss: 1.8570\n",
            "Epoch [0], Batch [84/157], test Loss: 1.8713\n",
            "Epoch [0], Batch [85/157], test Loss: 1.8893\n",
            "Epoch [0], Batch [86/157], test Loss: 1.8311\n",
            "Epoch [0], Batch [87/157], test Loss: 1.8413\n",
            "Epoch [0], Batch [88/157], test Loss: 1.8783\n",
            "Epoch [0], Batch [89/157], test Loss: 1.8297\n",
            "Epoch [0], Batch [90/157], test Loss: 1.8567\n",
            "Epoch [0], Batch [91/157], test Loss: 1.8481\n",
            "Epoch [0], Batch [92/157], test Loss: 1.8801\n",
            "Epoch [0], Batch [93/157], test Loss: 1.8069\n",
            "Epoch [0], Batch [94/157], test Loss: 1.8179\n",
            "Epoch [0], Batch [95/157], test Loss: 1.8522\n",
            "Epoch [0], Batch [96/157], test Loss: 1.8657\n",
            "Epoch [0], Batch [97/157], test Loss: 1.8269\n",
            "Epoch [0], Batch [98/157], test Loss: 1.8336\n",
            "Epoch [0], Batch [99/157], test Loss: 1.8315\n",
            "Epoch [0], Batch [100/157], test Loss: 1.8060\n",
            "Epoch [0], Batch [101/157], test Loss: 1.8049\n",
            "Epoch [0], Batch [102/157], test Loss: 1.8852\n",
            "Epoch [0], Batch [103/157], test Loss: 1.9107\n",
            "Epoch [0], Batch [104/157], test Loss: 1.8476\n",
            "Epoch [0], Batch [105/157], test Loss: 1.8768\n",
            "Epoch [0], Batch [106/157], test Loss: 1.8419\n",
            "Epoch [0], Batch [107/157], test Loss: 1.8248\n",
            "Epoch [0], Batch [108/157], test Loss: 1.8410\n",
            "Epoch [0], Batch [109/157], test Loss: 1.8345\n",
            "Epoch [0], Batch [110/157], test Loss: 1.8727\n",
            "Epoch [0], Batch [111/157], test Loss: 1.8772\n",
            "Epoch [0], Batch [112/157], test Loss: 1.8819\n",
            "Epoch [0], Batch [113/157], test Loss: 1.7693\n",
            "Epoch [0], Batch [114/157], test Loss: 1.8176\n",
            "Epoch [0], Batch [115/157], test Loss: 1.8418\n",
            "Epoch [0], Batch [116/157], test Loss: 1.8393\n",
            "Epoch [0], Batch [117/157], test Loss: 1.8384\n",
            "Epoch [0], Batch [118/157], test Loss: 1.8490\n",
            "Epoch [0], Batch [119/157], test Loss: 1.7865\n",
            "Epoch [0], Batch [120/157], test Loss: 1.8409\n",
            "Epoch [0], Batch [121/157], test Loss: 1.8721\n",
            "Epoch [0], Batch [122/157], test Loss: 1.8295\n",
            "Epoch [0], Batch [123/157], test Loss: 1.8986\n",
            "Epoch [0], Batch [124/157], test Loss: 1.8827\n",
            "Epoch [0], Batch [125/157], test Loss: 1.8589\n",
            "Epoch [0], Batch [126/157], test Loss: 1.8114\n",
            "Epoch [0], Batch [127/157], test Loss: 1.8055\n",
            "Epoch [0], Batch [128/157], test Loss: 1.9018\n",
            "Epoch [0], Batch [129/157], test Loss: 1.9073\n",
            "Epoch [0], Batch [130/157], test Loss: 1.8172\n",
            "Epoch [0], Batch [131/157], test Loss: 1.8464\n",
            "Epoch [0], Batch [132/157], test Loss: 1.8136\n",
            "Epoch [0], Batch [133/157], test Loss: 1.8146\n",
            "Epoch [0], Batch [134/157], test Loss: 1.8286\n",
            "Epoch [0], Batch [135/157], test Loss: 1.8676\n",
            "Epoch [0], Batch [136/157], test Loss: 1.8903\n",
            "Epoch [0], Batch [137/157], test Loss: 1.8486\n",
            "Epoch [0], Batch [138/157], test Loss: 1.8771\n",
            "Epoch [0], Batch [139/157], test Loss: 1.8233\n",
            "Epoch [0], Batch [140/157], test Loss: 1.8111\n",
            "Epoch [0], Batch [141/157], test Loss: 1.8152\n",
            "Epoch [0], Batch [142/157], test Loss: 1.8608\n",
            "Epoch [0], Batch [143/157], test Loss: 1.8125\n",
            "Epoch [0], Batch [144/157], test Loss: 1.8638\n",
            "Epoch [0], Batch [145/157], test Loss: 1.8289\n",
            "Epoch [0], Batch [146/157], test Loss: 1.9178\n",
            "Epoch [0], Batch [147/157], test Loss: 1.8468\n",
            "Epoch [0], Batch [148/157], test Loss: 1.8604\n",
            "Epoch [0], Batch [149/157], test Loss: 1.8304\n",
            "Epoch [0], Batch [150/157], test Loss: 1.8738\n",
            "Epoch [0], Batch [151/157], test Loss: 1.8357\n",
            "Epoch [0], Batch [152/157], test Loss: 1.8565\n",
            "Epoch [0], Batch [153/157], test Loss: 1.8553\n",
            "Epoch [0], Batch [154/157], test Loss: 1.8607\n",
            "Epoch [0], Batch [155/157], test Loss: 1.8571\n",
            "Epoch [0], Batch [156/157], test Loss: 1.8983\n",
            "Epoch [0], Batch [157/157], test Loss: 1.7916\n",
            "Accuracy of test set: 32.63%\n",
            "Epoch [1/25] - Train Loss: 2.1513, Train Accuracy: 27.53% - Test Loss: 1.8484, Test Accuracy: 32.63%\n",
            "Epoch [1], Batch [10/938], Loss: 1.8989\n",
            "Epoch [1], Batch [20/938], Loss: 1.8733\n",
            "Epoch [1], Batch [30/938], Loss: 1.8775\n",
            "Epoch [1], Batch [40/938], Loss: 1.7954\n",
            "Epoch [1], Batch [50/938], Loss: 1.8308\n",
            "Epoch [1], Batch [60/938], Loss: 1.7713\n",
            "Epoch [1], Batch [70/938], Loss: 1.6959\n",
            "Epoch [1], Batch [80/938], Loss: 1.7718\n",
            "Epoch [1], Batch [90/938], Loss: 1.7007\n",
            "Epoch [1], Batch [100/938], Loss: 1.6546\n",
            "Epoch [1], Batch [110/938], Loss: 1.6925\n",
            "Epoch [1], Batch [120/938], Loss: 1.6768\n",
            "Epoch [1], Batch [130/938], Loss: 1.6511\n",
            "Epoch [1], Batch [140/938], Loss: 1.6949\n",
            "Epoch [1], Batch [150/938], Loss: 1.6267\n",
            "Epoch [1], Batch [160/938], Loss: 1.7001\n",
            "Epoch [1], Batch [170/938], Loss: 1.6740\n",
            "Epoch [1], Batch [180/938], Loss: 1.6436\n",
            "Epoch [1], Batch [190/938], Loss: 1.6055\n",
            "Epoch [1], Batch [200/938], Loss: 1.5770\n",
            "Epoch [1], Batch [210/938], Loss: 1.5254\n",
            "Epoch [1], Batch [220/938], Loss: 1.5347\n",
            "Epoch [1], Batch [230/938], Loss: 1.4896\n",
            "Epoch [1], Batch [240/938], Loss: 1.4682\n",
            "Epoch [1], Batch [250/938], Loss: 1.4660\n",
            "Epoch [1], Batch [260/938], Loss: 1.5041\n",
            "Epoch [1], Batch [270/938], Loss: 1.4998\n",
            "Epoch [1], Batch [280/938], Loss: 1.3653\n",
            "Epoch [1], Batch [290/938], Loss: 1.4869\n",
            "Epoch [1], Batch [300/938], Loss: 1.4019\n",
            "Epoch [1], Batch [310/938], Loss: 1.4239\n",
            "Epoch [1], Batch [320/938], Loss: 1.3458\n",
            "Epoch [1], Batch [330/938], Loss: 1.2655\n",
            "Epoch [1], Batch [340/938], Loss: 1.3909\n",
            "Epoch [1], Batch [350/938], Loss: 1.4541\n",
            "Epoch [1], Batch [360/938], Loss: 1.3222\n",
            "Epoch [1], Batch [370/938], Loss: 1.3255\n",
            "Epoch [1], Batch [380/938], Loss: 1.3011\n",
            "Epoch [1], Batch [390/938], Loss: 1.3749\n",
            "Epoch [1], Batch [400/938], Loss: 1.3088\n",
            "Epoch [1], Batch [410/938], Loss: 1.1586\n",
            "Epoch [1], Batch [420/938], Loss: 1.1853\n",
            "Epoch [1], Batch [430/938], Loss: 1.2010\n",
            "Epoch [1], Batch [440/938], Loss: 1.2898\n",
            "Epoch [1], Batch [450/938], Loss: 1.1734\n",
            "Epoch [1], Batch [460/938], Loss: 1.3053\n",
            "Epoch [1], Batch [470/938], Loss: 1.1987\n",
            "Epoch [1], Batch [480/938], Loss: 1.2551\n",
            "Epoch [1], Batch [490/938], Loss: 1.2800\n",
            "Epoch [1], Batch [500/938], Loss: 1.2478\n",
            "Epoch [1], Batch [510/938], Loss: 1.1426\n",
            "Epoch [1], Batch [520/938], Loss: 1.3567\n",
            "Epoch [1], Batch [530/938], Loss: 1.1743\n",
            "Epoch [1], Batch [540/938], Loss: 1.2541\n",
            "Epoch [1], Batch [550/938], Loss: 1.1269\n",
            "Epoch [1], Batch [560/938], Loss: 1.1646\n",
            "Epoch [1], Batch [570/938], Loss: 1.1313\n",
            "Epoch [1], Batch [580/938], Loss: 1.2180\n",
            "Epoch [1], Batch [590/938], Loss: 1.1613\n",
            "Epoch [1], Batch [600/938], Loss: 1.1255\n",
            "Epoch [1], Batch [610/938], Loss: 1.1657\n",
            "Epoch [1], Batch [620/938], Loss: 1.0717\n",
            "Epoch [1], Batch [630/938], Loss: 0.9233\n",
            "Epoch [1], Batch [640/938], Loss: 1.1647\n",
            "Epoch [1], Batch [650/938], Loss: 1.1329\n",
            "Epoch [1], Batch [660/938], Loss: 1.1003\n",
            "Epoch [1], Batch [670/938], Loss: 1.0588\n",
            "Epoch [1], Batch [680/938], Loss: 1.0941\n",
            "Epoch [1], Batch [690/938], Loss: 1.1057\n",
            "Epoch [1], Batch [700/938], Loss: 0.9641\n",
            "Epoch [1], Batch [710/938], Loss: 0.9366\n",
            "Epoch [1], Batch [720/938], Loss: 1.1379\n",
            "Epoch [1], Batch [730/938], Loss: 1.1500\n",
            "Epoch [1], Batch [740/938], Loss: 1.0187\n",
            "Epoch [1], Batch [750/938], Loss: 1.0000\n",
            "Epoch [1], Batch [760/938], Loss: 1.2271\n",
            "Epoch [1], Batch [770/938], Loss: 0.9647\n",
            "Epoch [1], Batch [780/938], Loss: 1.1415\n",
            "Epoch [1], Batch [790/938], Loss: 0.9845\n",
            "Epoch [1], Batch [800/938], Loss: 1.0495\n",
            "Epoch [1], Batch [810/938], Loss: 0.9760\n",
            "Epoch [1], Batch [820/938], Loss: 0.8933\n",
            "Epoch [1], Batch [830/938], Loss: 0.9992\n",
            "Epoch [1], Batch [840/938], Loss: 1.0696\n",
            "Epoch [1], Batch [850/938], Loss: 1.0586\n",
            "Epoch [1], Batch [860/938], Loss: 0.9982\n",
            "Epoch [1], Batch [870/938], Loss: 1.1457\n",
            "Epoch [1], Batch [880/938], Loss: 1.1268\n",
            "Epoch [1], Batch [890/938], Loss: 1.1465\n",
            "Epoch [1], Batch [900/938], Loss: 0.9050\n",
            "Epoch [1], Batch [910/938], Loss: 1.0209\n",
            "Epoch [1], Batch [920/938], Loss: 1.0693\n",
            "Epoch [1], Batch [930/938], Loss: 0.8224\n",
            "Epoch [1], Batch [938/938], Loss: 1.0762\n",
            "Accuracy of train set: 49.35%\n",
            "Epoch [1], Batch [1/157], test Loss: 0.9684\n",
            "Epoch [1], Batch [2/157], test Loss: 1.0427\n",
            "Epoch [1], Batch [3/157], test Loss: 0.8440\n",
            "Epoch [1], Batch [4/157], test Loss: 0.9549\n",
            "Epoch [1], Batch [5/157], test Loss: 1.0814\n",
            "Epoch [1], Batch [6/157], test Loss: 1.0604\n",
            "Epoch [1], Batch [7/157], test Loss: 0.8667\n",
            "Epoch [1], Batch [8/157], test Loss: 0.9590\n",
            "Epoch [1], Batch [9/157], test Loss: 1.1126\n",
            "Epoch [1], Batch [10/157], test Loss: 1.0437\n",
            "Epoch [1], Batch [11/157], test Loss: 1.0131\n",
            "Epoch [1], Batch [12/157], test Loss: 0.9432\n",
            "Epoch [1], Batch [13/157], test Loss: 1.0702\n",
            "Epoch [1], Batch [14/157], test Loss: 0.8934\n",
            "Epoch [1], Batch [15/157], test Loss: 0.9350\n",
            "Epoch [1], Batch [16/157], test Loss: 1.0169\n",
            "Epoch [1], Batch [17/157], test Loss: 1.0640\n",
            "Epoch [1], Batch [18/157], test Loss: 1.1124\n",
            "Epoch [1], Batch [19/157], test Loss: 1.1041\n",
            "Epoch [1], Batch [20/157], test Loss: 1.1073\n",
            "Epoch [1], Batch [21/157], test Loss: 0.9937\n",
            "Epoch [1], Batch [22/157], test Loss: 0.9176\n",
            "Epoch [1], Batch [23/157], test Loss: 1.0435\n",
            "Epoch [1], Batch [24/157], test Loss: 1.0046\n",
            "Epoch [1], Batch [25/157], test Loss: 1.1395\n",
            "Epoch [1], Batch [26/157], test Loss: 1.0947\n",
            "Epoch [1], Batch [27/157], test Loss: 1.1375\n",
            "Epoch [1], Batch [28/157], test Loss: 1.1725\n",
            "Epoch [1], Batch [29/157], test Loss: 1.0152\n",
            "Epoch [1], Batch [30/157], test Loss: 0.9363\n",
            "Epoch [1], Batch [31/157], test Loss: 1.0013\n",
            "Epoch [1], Batch [32/157], test Loss: 1.3185\n",
            "Epoch [1], Batch [33/157], test Loss: 0.8378\n",
            "Epoch [1], Batch [34/157], test Loss: 1.0920\n",
            "Epoch [1], Batch [35/157], test Loss: 0.9163\n",
            "Epoch [1], Batch [36/157], test Loss: 0.9588\n",
            "Epoch [1], Batch [37/157], test Loss: 1.1092\n",
            "Epoch [1], Batch [38/157], test Loss: 1.0128\n",
            "Epoch [1], Batch [39/157], test Loss: 1.1753\n",
            "Epoch [1], Batch [40/157], test Loss: 0.8656\n",
            "Epoch [1], Batch [41/157], test Loss: 0.8742\n",
            "Epoch [1], Batch [42/157], test Loss: 0.9753\n",
            "Epoch [1], Batch [43/157], test Loss: 1.1709\n",
            "Epoch [1], Batch [44/157], test Loss: 0.8261\n",
            "Epoch [1], Batch [45/157], test Loss: 0.9619\n",
            "Epoch [1], Batch [46/157], test Loss: 1.0408\n",
            "Epoch [1], Batch [47/157], test Loss: 0.9038\n",
            "Epoch [1], Batch [48/157], test Loss: 0.8822\n",
            "Epoch [1], Batch [49/157], test Loss: 1.0439\n",
            "Epoch [1], Batch [50/157], test Loss: 1.0290\n",
            "Epoch [1], Batch [51/157], test Loss: 1.0924\n",
            "Epoch [1], Batch [52/157], test Loss: 1.1299\n",
            "Epoch [1], Batch [53/157], test Loss: 1.0834\n",
            "Epoch [1], Batch [54/157], test Loss: 1.1345\n",
            "Epoch [1], Batch [55/157], test Loss: 0.8387\n",
            "Epoch [1], Batch [56/157], test Loss: 0.9887\n",
            "Epoch [1], Batch [57/157], test Loss: 0.9092\n",
            "Epoch [1], Batch [58/157], test Loss: 1.1799\n",
            "Epoch [1], Batch [59/157], test Loss: 0.8341\n",
            "Epoch [1], Batch [60/157], test Loss: 1.0824\n",
            "Epoch [1], Batch [61/157], test Loss: 0.9552\n",
            "Epoch [1], Batch [62/157], test Loss: 1.0020\n",
            "Epoch [1], Batch [63/157], test Loss: 0.9303\n",
            "Epoch [1], Batch [64/157], test Loss: 1.0463\n",
            "Epoch [1], Batch [65/157], test Loss: 1.1236\n",
            "Epoch [1], Batch [66/157], test Loss: 0.9338\n",
            "Epoch [1], Batch [67/157], test Loss: 1.0121\n",
            "Epoch [1], Batch [68/157], test Loss: 1.0006\n",
            "Epoch [1], Batch [69/157], test Loss: 0.9869\n",
            "Epoch [1], Batch [70/157], test Loss: 1.1190\n",
            "Epoch [1], Batch [71/157], test Loss: 1.0938\n",
            "Epoch [1], Batch [72/157], test Loss: 0.8681\n",
            "Epoch [1], Batch [73/157], test Loss: 0.7976\n",
            "Epoch [1], Batch [74/157], test Loss: 1.0005\n",
            "Epoch [1], Batch [75/157], test Loss: 0.9307\n",
            "Epoch [1], Batch [76/157], test Loss: 1.0837\n",
            "Epoch [1], Batch [77/157], test Loss: 0.9546\n",
            "Epoch [1], Batch [78/157], test Loss: 0.8918\n",
            "Epoch [1], Batch [79/157], test Loss: 0.9415\n",
            "Epoch [1], Batch [80/157], test Loss: 0.9439\n",
            "Epoch [1], Batch [81/157], test Loss: 1.3132\n",
            "Epoch [1], Batch [82/157], test Loss: 1.0418\n",
            "Epoch [1], Batch [83/157], test Loss: 0.9500\n",
            "Epoch [1], Batch [84/157], test Loss: 1.0618\n",
            "Epoch [1], Batch [85/157], test Loss: 0.9744\n",
            "Epoch [1], Batch [86/157], test Loss: 1.0758\n",
            "Epoch [1], Batch [87/157], test Loss: 1.0039\n",
            "Epoch [1], Batch [88/157], test Loss: 0.9832\n",
            "Epoch [1], Batch [89/157], test Loss: 0.9765\n",
            "Epoch [1], Batch [90/157], test Loss: 0.8898\n",
            "Epoch [1], Batch [91/157], test Loss: 1.2458\n",
            "Epoch [1], Batch [92/157], test Loss: 1.1485\n",
            "Epoch [1], Batch [93/157], test Loss: 0.8999\n",
            "Epoch [1], Batch [94/157], test Loss: 0.9697\n",
            "Epoch [1], Batch [95/157], test Loss: 0.8369\n",
            "Epoch [1], Batch [96/157], test Loss: 0.9219\n",
            "Epoch [1], Batch [97/157], test Loss: 1.1728\n",
            "Epoch [1], Batch [98/157], test Loss: 1.1600\n",
            "Epoch [1], Batch [99/157], test Loss: 1.0425\n",
            "Epoch [1], Batch [100/157], test Loss: 0.9983\n",
            "Epoch [1], Batch [101/157], test Loss: 1.0296\n",
            "Epoch [1], Batch [102/157], test Loss: 1.0817\n",
            "Epoch [1], Batch [103/157], test Loss: 1.0534\n",
            "Epoch [1], Batch [104/157], test Loss: 1.1255\n",
            "Epoch [1], Batch [105/157], test Loss: 1.0390\n",
            "Epoch [1], Batch [106/157], test Loss: 0.9667\n",
            "Epoch [1], Batch [107/157], test Loss: 0.9447\n",
            "Epoch [1], Batch [108/157], test Loss: 0.8969\n",
            "Epoch [1], Batch [109/157], test Loss: 1.0053\n",
            "Epoch [1], Batch [110/157], test Loss: 0.9043\n",
            "Epoch [1], Batch [111/157], test Loss: 0.9111\n",
            "Epoch [1], Batch [112/157], test Loss: 0.9828\n",
            "Epoch [1], Batch [113/157], test Loss: 0.9972\n",
            "Epoch [1], Batch [114/157], test Loss: 0.8457\n",
            "Epoch [1], Batch [115/157], test Loss: 1.0564\n",
            "Epoch [1], Batch [116/157], test Loss: 1.1588\n",
            "Epoch [1], Batch [117/157], test Loss: 0.8808\n",
            "Epoch [1], Batch [118/157], test Loss: 0.9380\n",
            "Epoch [1], Batch [119/157], test Loss: 1.0275\n",
            "Epoch [1], Batch [120/157], test Loss: 1.0970\n",
            "Epoch [1], Batch [121/157], test Loss: 0.9731\n",
            "Epoch [1], Batch [122/157], test Loss: 1.1678\n",
            "Epoch [1], Batch [123/157], test Loss: 0.9294\n",
            "Epoch [1], Batch [124/157], test Loss: 1.0895\n",
            "Epoch [1], Batch [125/157], test Loss: 1.0050\n",
            "Epoch [1], Batch [126/157], test Loss: 1.0339\n",
            "Epoch [1], Batch [127/157], test Loss: 1.1081\n",
            "Epoch [1], Batch [128/157], test Loss: 0.9282\n",
            "Epoch [1], Batch [129/157], test Loss: 1.1580\n",
            "Epoch [1], Batch [130/157], test Loss: 1.0809\n",
            "Epoch [1], Batch [131/157], test Loss: 0.9163\n",
            "Epoch [1], Batch [132/157], test Loss: 0.9675\n",
            "Epoch [1], Batch [133/157], test Loss: 0.9638\n",
            "Epoch [1], Batch [134/157], test Loss: 0.9736\n",
            "Epoch [1], Batch [135/157], test Loss: 1.0949\n",
            "Epoch [1], Batch [136/157], test Loss: 0.8891\n",
            "Epoch [1], Batch [137/157], test Loss: 1.0642\n",
            "Epoch [1], Batch [138/157], test Loss: 1.1485\n",
            "Epoch [1], Batch [139/157], test Loss: 0.9670\n",
            "Epoch [1], Batch [140/157], test Loss: 1.0249\n",
            "Epoch [1], Batch [141/157], test Loss: 0.9141\n",
            "Epoch [1], Batch [142/157], test Loss: 1.0149\n",
            "Epoch [1], Batch [143/157], test Loss: 1.1049\n",
            "Epoch [1], Batch [144/157], test Loss: 0.9656\n",
            "Epoch [1], Batch [145/157], test Loss: 0.8700\n",
            "Epoch [1], Batch [146/157], test Loss: 0.9437\n",
            "Epoch [1], Batch [147/157], test Loss: 0.7697\n",
            "Epoch [1], Batch [148/157], test Loss: 0.8758\n",
            "Epoch [1], Batch [149/157], test Loss: 0.9137\n",
            "Epoch [1], Batch [150/157], test Loss: 1.1411\n",
            "Epoch [1], Batch [151/157], test Loss: 1.0820\n",
            "Epoch [1], Batch [152/157], test Loss: 0.9919\n",
            "Epoch [1], Batch [153/157], test Loss: 1.0693\n",
            "Epoch [1], Batch [154/157], test Loss: 1.0684\n",
            "Epoch [1], Batch [155/157], test Loss: 1.2395\n",
            "Epoch [1], Batch [156/157], test Loss: 0.9709\n",
            "Epoch [1], Batch [157/157], test Loss: 0.8812\n",
            "Accuracy of test set: 61.08%\n",
            "Epoch [2/25] - Train Loss: 1.3022, Train Accuracy: 49.35% - Test Loss: 1.0092, Test Accuracy: 61.08%\n",
            "Epoch [2], Batch [10/938], Loss: 1.0037\n",
            "Epoch [2], Batch [20/938], Loss: 0.9881\n",
            "Epoch [2], Batch [30/938], Loss: 1.0194\n",
            "Epoch [2], Batch [40/938], Loss: 0.8694\n",
            "Epoch [2], Batch [50/938], Loss: 0.9773\n",
            "Epoch [2], Batch [60/938], Loss: 0.8228\n",
            "Epoch [2], Batch [70/938], Loss: 0.9500\n",
            "Epoch [2], Batch [80/938], Loss: 1.2567\n",
            "Epoch [2], Batch [90/938], Loss: 0.9138\n",
            "Epoch [2], Batch [100/938], Loss: 1.0864\n",
            "Epoch [2], Batch [110/938], Loss: 0.8727\n",
            "Epoch [2], Batch [120/938], Loss: 1.0859\n",
            "Epoch [2], Batch [130/938], Loss: 1.0362\n",
            "Epoch [2], Batch [140/938], Loss: 0.7509\n",
            "Epoch [2], Batch [150/938], Loss: 0.9445\n",
            "Epoch [2], Batch [160/938], Loss: 0.8687\n",
            "Epoch [2], Batch [170/938], Loss: 0.9203\n",
            "Epoch [2], Batch [180/938], Loss: 0.9370\n",
            "Epoch [2], Batch [190/938], Loss: 1.0770\n",
            "Epoch [2], Batch [200/938], Loss: 0.9881\n",
            "Epoch [2], Batch [210/938], Loss: 1.0860\n",
            "Epoch [2], Batch [220/938], Loss: 0.8710\n",
            "Epoch [2], Batch [230/938], Loss: 0.8885\n",
            "Epoch [2], Batch [240/938], Loss: 0.8300\n",
            "Epoch [2], Batch [250/938], Loss: 1.0292\n",
            "Epoch [2], Batch [260/938], Loss: 0.8960\n",
            "Epoch [2], Batch [270/938], Loss: 0.8413\n",
            "Epoch [2], Batch [280/938], Loss: 0.7842\n",
            "Epoch [2], Batch [290/938], Loss: 0.9597\n",
            "Epoch [2], Batch [300/938], Loss: 1.0758\n",
            "Epoch [2], Batch [310/938], Loss: 0.8332\n",
            "Epoch [2], Batch [320/938], Loss: 0.9999\n",
            "Epoch [2], Batch [330/938], Loss: 0.7832\n",
            "Epoch [2], Batch [340/938], Loss: 0.9073\n",
            "Epoch [2], Batch [350/938], Loss: 0.9593\n",
            "Epoch [2], Batch [360/938], Loss: 0.9200\n",
            "Epoch [2], Batch [370/938], Loss: 0.9665\n",
            "Epoch [2], Batch [380/938], Loss: 1.0309\n",
            "Epoch [2], Batch [390/938], Loss: 0.7634\n",
            "Epoch [2], Batch [400/938], Loss: 0.8269\n",
            "Epoch [2], Batch [410/938], Loss: 0.9333\n",
            "Epoch [2], Batch [420/938], Loss: 0.9812\n",
            "Epoch [2], Batch [430/938], Loss: 1.0674\n",
            "Epoch [2], Batch [440/938], Loss: 1.1209\n",
            "Epoch [2], Batch [450/938], Loss: 0.8878\n",
            "Epoch [2], Batch [460/938], Loss: 0.8531\n",
            "Epoch [2], Batch [470/938], Loss: 0.7383\n",
            "Epoch [2], Batch [480/938], Loss: 0.7583\n",
            "Epoch [2], Batch [490/938], Loss: 1.0031\n",
            "Epoch [2], Batch [500/938], Loss: 0.9847\n",
            "Epoch [2], Batch [510/938], Loss: 1.1905\n",
            "Epoch [2], Batch [520/938], Loss: 0.9739\n",
            "Epoch [2], Batch [530/938], Loss: 1.0193\n",
            "Epoch [2], Batch [540/938], Loss: 0.9082\n",
            "Epoch [2], Batch [550/938], Loss: 0.9204\n",
            "Epoch [2], Batch [560/938], Loss: 0.9834\n",
            "Epoch [2], Batch [570/938], Loss: 0.7449\n",
            "Epoch [2], Batch [580/938], Loss: 0.9132\n",
            "Epoch [2], Batch [590/938], Loss: 0.7729\n",
            "Epoch [2], Batch [600/938], Loss: 0.9193\n",
            "Epoch [2], Batch [610/938], Loss: 0.8811\n",
            "Epoch [2], Batch [620/938], Loss: 0.8761\n",
            "Epoch [2], Batch [630/938], Loss: 0.6938\n",
            "Epoch [2], Batch [640/938], Loss: 1.0977\n",
            "Epoch [2], Batch [650/938], Loss: 0.7127\n",
            "Epoch [2], Batch [660/938], Loss: 0.7422\n",
            "Epoch [2], Batch [670/938], Loss: 0.7654\n",
            "Epoch [2], Batch [680/938], Loss: 1.1882\n",
            "Epoch [2], Batch [690/938], Loss: 0.9735\n",
            "Epoch [2], Batch [700/938], Loss: 0.7730\n",
            "Epoch [2], Batch [710/938], Loss: 0.8332\n",
            "Epoch [2], Batch [720/938], Loss: 0.8080\n",
            "Epoch [2], Batch [730/938], Loss: 0.7111\n",
            "Epoch [2], Batch [740/938], Loss: 0.9286\n",
            "Epoch [2], Batch [750/938], Loss: 0.8166\n",
            "Epoch [2], Batch [760/938], Loss: 0.8529\n",
            "Epoch [2], Batch [770/938], Loss: 0.9747\n",
            "Epoch [2], Batch [780/938], Loss: 0.8589\n",
            "Epoch [2], Batch [790/938], Loss: 0.8437\n",
            "Epoch [2], Batch [800/938], Loss: 0.6896\n",
            "Epoch [2], Batch [810/938], Loss: 0.7612\n",
            "Epoch [2], Batch [820/938], Loss: 0.6684\n",
            "Epoch [2], Batch [830/938], Loss: 0.7576\n",
            "Epoch [2], Batch [840/938], Loss: 0.8750\n",
            "Epoch [2], Batch [850/938], Loss: 0.9723\n",
            "Epoch [2], Batch [860/938], Loss: 0.6907\n",
            "Epoch [2], Batch [870/938], Loss: 0.8118\n",
            "Epoch [2], Batch [880/938], Loss: 0.8681\n",
            "Epoch [2], Batch [890/938], Loss: 0.9069\n",
            "Epoch [2], Batch [900/938], Loss: 1.0269\n",
            "Epoch [2], Batch [910/938], Loss: 0.8271\n",
            "Epoch [2], Batch [920/938], Loss: 1.0261\n",
            "Epoch [2], Batch [930/938], Loss: 0.7415\n",
            "Epoch [2], Batch [938/938], Loss: 0.6751\n",
            "Accuracy of train set: 64.89%\n",
            "Epoch [2], Batch [1/157], test Loss: 0.7683\n",
            "Epoch [2], Batch [2/157], test Loss: 0.9553\n",
            "Epoch [2], Batch [3/157], test Loss: 1.0425\n",
            "Epoch [2], Batch [4/157], test Loss: 0.7291\n",
            "Epoch [2], Batch [5/157], test Loss: 0.8080\n",
            "Epoch [2], Batch [6/157], test Loss: 0.9944\n",
            "Epoch [2], Batch [7/157], test Loss: 0.7454\n",
            "Epoch [2], Batch [8/157], test Loss: 0.9344\n",
            "Epoch [2], Batch [9/157], test Loss: 1.0299\n",
            "Epoch [2], Batch [10/157], test Loss: 0.8905\n",
            "Epoch [2], Batch [11/157], test Loss: 0.8350\n",
            "Epoch [2], Batch [12/157], test Loss: 0.8957\n",
            "Epoch [2], Batch [13/157], test Loss: 1.0755\n",
            "Epoch [2], Batch [14/157], test Loss: 0.7663\n",
            "Epoch [2], Batch [15/157], test Loss: 0.8355\n",
            "Epoch [2], Batch [16/157], test Loss: 0.7808\n",
            "Epoch [2], Batch [17/157], test Loss: 0.8947\n",
            "Epoch [2], Batch [18/157], test Loss: 0.7898\n",
            "Epoch [2], Batch [19/157], test Loss: 0.7434\n",
            "Epoch [2], Batch [20/157], test Loss: 0.7809\n",
            "Epoch [2], Batch [21/157], test Loss: 1.0309\n",
            "Epoch [2], Batch [22/157], test Loss: 0.6879\n",
            "Epoch [2], Batch [23/157], test Loss: 0.8842\n",
            "Epoch [2], Batch [24/157], test Loss: 0.9744\n",
            "Epoch [2], Batch [25/157], test Loss: 0.8449\n",
            "Epoch [2], Batch [26/157], test Loss: 0.9001\n",
            "Epoch [2], Batch [27/157], test Loss: 0.9661\n",
            "Epoch [2], Batch [28/157], test Loss: 0.7463\n",
            "Epoch [2], Batch [29/157], test Loss: 0.8887\n",
            "Epoch [2], Batch [30/157], test Loss: 1.0251\n",
            "Epoch [2], Batch [31/157], test Loss: 0.9305\n",
            "Epoch [2], Batch [32/157], test Loss: 0.9607\n",
            "Epoch [2], Batch [33/157], test Loss: 0.8726\n",
            "Epoch [2], Batch [34/157], test Loss: 0.8939\n",
            "Epoch [2], Batch [35/157], test Loss: 0.9430\n",
            "Epoch [2], Batch [36/157], test Loss: 0.8645\n",
            "Epoch [2], Batch [37/157], test Loss: 0.8857\n",
            "Epoch [2], Batch [38/157], test Loss: 0.9896\n",
            "Epoch [2], Batch [39/157], test Loss: 0.8619\n",
            "Epoch [2], Batch [40/157], test Loss: 0.8658\n",
            "Epoch [2], Batch [41/157], test Loss: 0.8112\n",
            "Epoch [2], Batch [42/157], test Loss: 0.7684\n",
            "Epoch [2], Batch [43/157], test Loss: 0.7446\n",
            "Epoch [2], Batch [44/157], test Loss: 0.6447\n",
            "Epoch [2], Batch [45/157], test Loss: 0.8289\n",
            "Epoch [2], Batch [46/157], test Loss: 0.7800\n",
            "Epoch [2], Batch [47/157], test Loss: 0.7243\n",
            "Epoch [2], Batch [48/157], test Loss: 0.8979\n",
            "Epoch [2], Batch [49/157], test Loss: 0.8012\n",
            "Epoch [2], Batch [50/157], test Loss: 0.9070\n",
            "Epoch [2], Batch [51/157], test Loss: 1.0375\n",
            "Epoch [2], Batch [52/157], test Loss: 1.0118\n",
            "Epoch [2], Batch [53/157], test Loss: 0.7381\n",
            "Epoch [2], Batch [54/157], test Loss: 0.8122\n",
            "Epoch [2], Batch [55/157], test Loss: 0.9118\n",
            "Epoch [2], Batch [56/157], test Loss: 0.9580\n",
            "Epoch [2], Batch [57/157], test Loss: 0.9104\n",
            "Epoch [2], Batch [58/157], test Loss: 0.7872\n",
            "Epoch [2], Batch [59/157], test Loss: 0.7620\n",
            "Epoch [2], Batch [60/157], test Loss: 0.7342\n",
            "Epoch [2], Batch [61/157], test Loss: 1.0154\n",
            "Epoch [2], Batch [62/157], test Loss: 0.7580\n",
            "Epoch [2], Batch [63/157], test Loss: 0.8062\n",
            "Epoch [2], Batch [64/157], test Loss: 0.8684\n",
            "Epoch [2], Batch [65/157], test Loss: 0.8489\n",
            "Epoch [2], Batch [66/157], test Loss: 0.8136\n",
            "Epoch [2], Batch [67/157], test Loss: 0.6619\n",
            "Epoch [2], Batch [68/157], test Loss: 0.9085\n",
            "Epoch [2], Batch [69/157], test Loss: 0.8185\n",
            "Epoch [2], Batch [70/157], test Loss: 0.8279\n",
            "Epoch [2], Batch [71/157], test Loss: 0.8782\n",
            "Epoch [2], Batch [72/157], test Loss: 1.0172\n",
            "Epoch [2], Batch [73/157], test Loss: 0.8149\n",
            "Epoch [2], Batch [74/157], test Loss: 0.9837\n",
            "Epoch [2], Batch [75/157], test Loss: 0.9115\n",
            "Epoch [2], Batch [76/157], test Loss: 0.6627\n",
            "Epoch [2], Batch [77/157], test Loss: 1.0283\n",
            "Epoch [2], Batch [78/157], test Loss: 0.7558\n",
            "Epoch [2], Batch [79/157], test Loss: 0.7199\n",
            "Epoch [2], Batch [80/157], test Loss: 0.8953\n",
            "Epoch [2], Batch [81/157], test Loss: 0.8331\n",
            "Epoch [2], Batch [82/157], test Loss: 0.8275\n",
            "Epoch [2], Batch [83/157], test Loss: 0.8909\n",
            "Epoch [2], Batch [84/157], test Loss: 0.9277\n",
            "Epoch [2], Batch [85/157], test Loss: 0.8437\n",
            "Epoch [2], Batch [86/157], test Loss: 0.8923\n",
            "Epoch [2], Batch [87/157], test Loss: 0.8236\n",
            "Epoch [2], Batch [88/157], test Loss: 0.8009\n",
            "Epoch [2], Batch [89/157], test Loss: 0.9574\n",
            "Epoch [2], Batch [90/157], test Loss: 0.7795\n",
            "Epoch [2], Batch [91/157], test Loss: 0.9243\n",
            "Epoch [2], Batch [92/157], test Loss: 0.6799\n",
            "Epoch [2], Batch [93/157], test Loss: 0.7038\n",
            "Epoch [2], Batch [94/157], test Loss: 0.7015\n",
            "Epoch [2], Batch [95/157], test Loss: 0.7968\n",
            "Epoch [2], Batch [96/157], test Loss: 1.0488\n",
            "Epoch [2], Batch [97/157], test Loss: 1.0652\n",
            "Epoch [2], Batch [98/157], test Loss: 0.7807\n",
            "Epoch [2], Batch [99/157], test Loss: 0.9267\n",
            "Epoch [2], Batch [100/157], test Loss: 0.9378\n",
            "Epoch [2], Batch [101/157], test Loss: 0.8004\n",
            "Epoch [2], Batch [102/157], test Loss: 0.8657\n",
            "Epoch [2], Batch [103/157], test Loss: 0.9464\n",
            "Epoch [2], Batch [104/157], test Loss: 0.8039\n",
            "Epoch [2], Batch [105/157], test Loss: 0.8091\n",
            "Epoch [2], Batch [106/157], test Loss: 0.9793\n",
            "Epoch [2], Batch [107/157], test Loss: 0.9531\n",
            "Epoch [2], Batch [108/157], test Loss: 0.7511\n",
            "Epoch [2], Batch [109/157], test Loss: 1.1030\n",
            "Epoch [2], Batch [110/157], test Loss: 0.9022\n",
            "Epoch [2], Batch [111/157], test Loss: 0.7805\n",
            "Epoch [2], Batch [112/157], test Loss: 0.7258\n",
            "Epoch [2], Batch [113/157], test Loss: 1.0008\n",
            "Epoch [2], Batch [114/157], test Loss: 0.6129\n",
            "Epoch [2], Batch [115/157], test Loss: 0.9791\n",
            "Epoch [2], Batch [116/157], test Loss: 0.9580\n",
            "Epoch [2], Batch [117/157], test Loss: 0.7242\n",
            "Epoch [2], Batch [118/157], test Loss: 0.8711\n",
            "Epoch [2], Batch [119/157], test Loss: 0.6651\n",
            "Epoch [2], Batch [120/157], test Loss: 0.7684\n",
            "Epoch [2], Batch [121/157], test Loss: 0.5776\n",
            "Epoch [2], Batch [122/157], test Loss: 0.9930\n",
            "Epoch [2], Batch [123/157], test Loss: 0.8340\n",
            "Epoch [2], Batch [124/157], test Loss: 0.8691\n",
            "Epoch [2], Batch [125/157], test Loss: 0.8196\n",
            "Epoch [2], Batch [126/157], test Loss: 0.6700\n",
            "Epoch [2], Batch [127/157], test Loss: 0.6953\n",
            "Epoch [2], Batch [128/157], test Loss: 0.8027\n",
            "Epoch [2], Batch [129/157], test Loss: 0.7487\n",
            "Epoch [2], Batch [130/157], test Loss: 1.0873\n",
            "Epoch [2], Batch [131/157], test Loss: 0.6528\n",
            "Epoch [2], Batch [132/157], test Loss: 0.8134\n",
            "Epoch [2], Batch [133/157], test Loss: 0.9402\n",
            "Epoch [2], Batch [134/157], test Loss: 0.9625\n",
            "Epoch [2], Batch [135/157], test Loss: 0.7001\n",
            "Epoch [2], Batch [136/157], test Loss: 0.9642\n",
            "Epoch [2], Batch [137/157], test Loss: 0.7265\n",
            "Epoch [2], Batch [138/157], test Loss: 0.9219\n",
            "Epoch [2], Batch [139/157], test Loss: 0.9198\n",
            "Epoch [2], Batch [140/157], test Loss: 0.8547\n",
            "Epoch [2], Batch [141/157], test Loss: 0.8619\n",
            "Epoch [2], Batch [142/157], test Loss: 0.8382\n",
            "Epoch [2], Batch [143/157], test Loss: 0.7716\n",
            "Epoch [2], Batch [144/157], test Loss: 0.6995\n",
            "Epoch [2], Batch [145/157], test Loss: 0.8882\n",
            "Epoch [2], Batch [146/157], test Loss: 0.6621\n",
            "Epoch [2], Batch [147/157], test Loss: 1.0158\n",
            "Epoch [2], Batch [148/157], test Loss: 0.8660\n",
            "Epoch [2], Batch [149/157], test Loss: 0.8977\n",
            "Epoch [2], Batch [150/157], test Loss: 0.8321\n",
            "Epoch [2], Batch [151/157], test Loss: 0.8890\n",
            "Epoch [2], Batch [152/157], test Loss: 0.7418\n",
            "Epoch [2], Batch [153/157], test Loss: 0.9800\n",
            "Epoch [2], Batch [154/157], test Loss: 0.8880\n",
            "Epoch [2], Batch [155/157], test Loss: 0.8938\n",
            "Epoch [2], Batch [156/157], test Loss: 0.8296\n",
            "Epoch [2], Batch [157/157], test Loss: 0.8882\n",
            "Accuracy of test set: 67.73%\n",
            "Epoch [3/25] - Train Loss: 0.8940, Train Accuracy: 64.89% - Test Loss: 0.8536, Test Accuracy: 67.73%\n",
            "Epoch [3], Batch [10/938], Loss: 0.7546\n",
            "Epoch [3], Batch [20/938], Loss: 0.8067\n",
            "Epoch [3], Batch [30/938], Loss: 0.8488\n",
            "Epoch [3], Batch [40/938], Loss: 0.8765\n",
            "Epoch [3], Batch [50/938], Loss: 0.8101\n",
            "Epoch [3], Batch [60/938], Loss: 0.8883\n",
            "Epoch [3], Batch [70/938], Loss: 0.8793\n",
            "Epoch [3], Batch [80/938], Loss: 0.7414\n",
            "Epoch [3], Batch [90/938], Loss: 0.7218\n",
            "Epoch [3], Batch [100/938], Loss: 0.7570\n",
            "Epoch [3], Batch [110/938], Loss: 0.9183\n",
            "Epoch [3], Batch [120/938], Loss: 0.5697\n",
            "Epoch [3], Batch [130/938], Loss: 0.7051\n",
            "Epoch [3], Batch [140/938], Loss: 0.8885\n",
            "Epoch [3], Batch [150/938], Loss: 1.0055\n",
            "Epoch [3], Batch [160/938], Loss: 1.0227\n",
            "Epoch [3], Batch [170/938], Loss: 0.9646\n",
            "Epoch [3], Batch [180/938], Loss: 0.7224\n",
            "Epoch [3], Batch [190/938], Loss: 0.7156\n",
            "Epoch [3], Batch [200/938], Loss: 0.7901\n",
            "Epoch [3], Batch [210/938], Loss: 0.7729\n",
            "Epoch [3], Batch [220/938], Loss: 0.9027\n",
            "Epoch [3], Batch [230/938], Loss: 0.9536\n",
            "Epoch [3], Batch [240/938], Loss: 0.7293\n",
            "Epoch [3], Batch [250/938], Loss: 0.8730\n",
            "Epoch [3], Batch [260/938], Loss: 0.6774\n",
            "Epoch [3], Batch [270/938], Loss: 0.8410\n",
            "Epoch [3], Batch [280/938], Loss: 0.7888\n",
            "Epoch [3], Batch [290/938], Loss: 0.8203\n",
            "Epoch [3], Batch [300/938], Loss: 0.7610\n",
            "Epoch [3], Batch [310/938], Loss: 1.0123\n",
            "Epoch [3], Batch [320/938], Loss: 0.6918\n",
            "Epoch [3], Batch [330/938], Loss: 0.6963\n",
            "Epoch [3], Batch [340/938], Loss: 0.8420\n",
            "Epoch [3], Batch [350/938], Loss: 0.7835\n",
            "Epoch [3], Batch [360/938], Loss: 0.7753\n",
            "Epoch [3], Batch [370/938], Loss: 0.8136\n",
            "Epoch [3], Batch [380/938], Loss: 0.6564\n",
            "Epoch [3], Batch [390/938], Loss: 0.9904\n",
            "Epoch [3], Batch [400/938], Loss: 0.6703\n",
            "Epoch [3], Batch [410/938], Loss: 0.6365\n",
            "Epoch [3], Batch [420/938], Loss: 0.9574\n",
            "Epoch [3], Batch [430/938], Loss: 0.8606\n",
            "Epoch [3], Batch [440/938], Loss: 0.8753\n",
            "Epoch [3], Batch [450/938], Loss: 0.7631\n",
            "Epoch [3], Batch [460/938], Loss: 0.7411\n",
            "Epoch [3], Batch [470/938], Loss: 0.8476\n",
            "Epoch [3], Batch [480/938], Loss: 0.9613\n",
            "Epoch [3], Batch [490/938], Loss: 0.8607\n",
            "Epoch [3], Batch [500/938], Loss: 0.9320\n",
            "Epoch [3], Batch [510/938], Loss: 0.9778\n",
            "Epoch [3], Batch [520/938], Loss: 0.8536\n",
            "Epoch [3], Batch [530/938], Loss: 0.9254\n",
            "Epoch [3], Batch [540/938], Loss: 1.2230\n",
            "Epoch [3], Batch [550/938], Loss: 0.8254\n",
            "Epoch [3], Batch [560/938], Loss: 0.7697\n",
            "Epoch [3], Batch [570/938], Loss: 0.8004\n",
            "Epoch [3], Batch [580/938], Loss: 0.8633\n",
            "Epoch [3], Batch [590/938], Loss: 0.6016\n",
            "Epoch [3], Batch [600/938], Loss: 0.7229\n",
            "Epoch [3], Batch [610/938], Loss: 0.7482\n",
            "Epoch [3], Batch [620/938], Loss: 0.7733\n",
            "Epoch [3], Batch [630/938], Loss: 0.7712\n",
            "Epoch [3], Batch [640/938], Loss: 0.9008\n",
            "Epoch [3], Batch [650/938], Loss: 0.7745\n",
            "Epoch [3], Batch [660/938], Loss: 0.7840\n",
            "Epoch [3], Batch [670/938], Loss: 0.6841\n",
            "Epoch [3], Batch [680/938], Loss: 0.7785\n",
            "Epoch [3], Batch [690/938], Loss: 0.6011\n",
            "Epoch [3], Batch [700/938], Loss: 0.7533\n",
            "Epoch [3], Batch [710/938], Loss: 0.7969\n",
            "Epoch [3], Batch [720/938], Loss: 0.6718\n",
            "Epoch [3], Batch [730/938], Loss: 0.8724\n",
            "Epoch [3], Batch [740/938], Loss: 0.6737\n",
            "Epoch [3], Batch [750/938], Loss: 0.7345\n",
            "Epoch [3], Batch [760/938], Loss: 0.6318\n",
            "Epoch [3], Batch [770/938], Loss: 0.8654\n",
            "Epoch [3], Batch [780/938], Loss: 0.8064\n",
            "Epoch [3], Batch [790/938], Loss: 0.7609\n",
            "Epoch [3], Batch [800/938], Loss: 0.8702\n",
            "Epoch [3], Batch [810/938], Loss: 0.8717\n",
            "Epoch [3], Batch [820/938], Loss: 0.8948\n",
            "Epoch [3], Batch [830/938], Loss: 0.7886\n",
            "Epoch [3], Batch [840/938], Loss: 0.8822\n",
            "Epoch [3], Batch [850/938], Loss: 0.8068\n",
            "Epoch [3], Batch [860/938], Loss: 0.9229\n",
            "Epoch [3], Batch [870/938], Loss: 0.8357\n",
            "Epoch [3], Batch [880/938], Loss: 0.7582\n",
            "Epoch [3], Batch [890/938], Loss: 0.7603\n",
            "Epoch [3], Batch [900/938], Loss: 0.7548\n",
            "Epoch [3], Batch [910/938], Loss: 0.8040\n",
            "Epoch [3], Batch [920/938], Loss: 0.8915\n",
            "Epoch [3], Batch [930/938], Loss: 0.8410\n",
            "Epoch [3], Batch [938/938], Loss: 0.6189\n",
            "Accuracy of train set: 70.00%\n",
            "Epoch [3], Batch [1/157], test Loss: 0.8865\n",
            "Epoch [3], Batch [2/157], test Loss: 0.8082\n",
            "Epoch [3], Batch [3/157], test Loss: 0.7814\n",
            "Epoch [3], Batch [4/157], test Loss: 0.6521\n",
            "Epoch [3], Batch [5/157], test Loss: 0.6475\n",
            "Epoch [3], Batch [6/157], test Loss: 0.7259\n",
            "Epoch [3], Batch [7/157], test Loss: 0.7347\n",
            "Epoch [3], Batch [8/157], test Loss: 0.7350\n",
            "Epoch [3], Batch [9/157], test Loss: 0.8069\n",
            "Epoch [3], Batch [10/157], test Loss: 0.8842\n",
            "Epoch [3], Batch [11/157], test Loss: 0.7933\n",
            "Epoch [3], Batch [12/157], test Loss: 0.6043\n",
            "Epoch [3], Batch [13/157], test Loss: 0.7091\n",
            "Epoch [3], Batch [14/157], test Loss: 0.9821\n",
            "Epoch [3], Batch [15/157], test Loss: 0.8025\n",
            "Epoch [3], Batch [16/157], test Loss: 0.7580\n",
            "Epoch [3], Batch [17/157], test Loss: 0.7280\n",
            "Epoch [3], Batch [18/157], test Loss: 0.8137\n",
            "Epoch [3], Batch [19/157], test Loss: 0.6978\n",
            "Epoch [3], Batch [20/157], test Loss: 0.7326\n",
            "Epoch [3], Batch [21/157], test Loss: 0.9004\n",
            "Epoch [3], Batch [22/157], test Loss: 0.9189\n",
            "Epoch [3], Batch [23/157], test Loss: 0.7693\n",
            "Epoch [3], Batch [24/157], test Loss: 1.0080\n",
            "Epoch [3], Batch [25/157], test Loss: 0.7617\n",
            "Epoch [3], Batch [26/157], test Loss: 0.8194\n",
            "Epoch [3], Batch [27/157], test Loss: 0.8663\n",
            "Epoch [3], Batch [28/157], test Loss: 0.6494\n",
            "Epoch [3], Batch [29/157], test Loss: 0.8031\n",
            "Epoch [3], Batch [30/157], test Loss: 0.8643\n",
            "Epoch [3], Batch [31/157], test Loss: 0.6859\n",
            "Epoch [3], Batch [32/157], test Loss: 0.7927\n",
            "Epoch [3], Batch [33/157], test Loss: 0.7408\n",
            "Epoch [3], Batch [34/157], test Loss: 0.8260\n",
            "Epoch [3], Batch [35/157], test Loss: 0.7493\n",
            "Epoch [3], Batch [36/157], test Loss: 0.7999\n",
            "Epoch [3], Batch [37/157], test Loss: 0.6347\n",
            "Epoch [3], Batch [38/157], test Loss: 0.7729\n",
            "Epoch [3], Batch [39/157], test Loss: 0.8123\n",
            "Epoch [3], Batch [40/157], test Loss: 0.7857\n",
            "Epoch [3], Batch [41/157], test Loss: 0.8859\n",
            "Epoch [3], Batch [42/157], test Loss: 0.8523\n",
            "Epoch [3], Batch [43/157], test Loss: 0.7160\n",
            "Epoch [3], Batch [44/157], test Loss: 0.8177\n",
            "Epoch [3], Batch [45/157], test Loss: 0.9979\n",
            "Epoch [3], Batch [46/157], test Loss: 1.1604\n",
            "Epoch [3], Batch [47/157], test Loss: 0.8716\n",
            "Epoch [3], Batch [48/157], test Loss: 0.8962\n",
            "Epoch [3], Batch [49/157], test Loss: 0.8345\n",
            "Epoch [3], Batch [50/157], test Loss: 0.7105\n",
            "Epoch [3], Batch [51/157], test Loss: 0.7062\n",
            "Epoch [3], Batch [52/157], test Loss: 0.8550\n",
            "Epoch [3], Batch [53/157], test Loss: 0.7481\n",
            "Epoch [3], Batch [54/157], test Loss: 0.9705\n",
            "Epoch [3], Batch [55/157], test Loss: 0.7843\n",
            "Epoch [3], Batch [56/157], test Loss: 0.7195\n",
            "Epoch [3], Batch [57/157], test Loss: 0.9321\n",
            "Epoch [3], Batch [58/157], test Loss: 0.9330\n",
            "Epoch [3], Batch [59/157], test Loss: 0.8010\n",
            "Epoch [3], Batch [60/157], test Loss: 0.9732\n",
            "Epoch [3], Batch [61/157], test Loss: 0.8912\n",
            "Epoch [3], Batch [62/157], test Loss: 0.9453\n",
            "Epoch [3], Batch [63/157], test Loss: 0.7185\n",
            "Epoch [3], Batch [64/157], test Loss: 0.8182\n",
            "Epoch [3], Batch [65/157], test Loss: 0.9592\n",
            "Epoch [3], Batch [66/157], test Loss: 0.5440\n",
            "Epoch [3], Batch [67/157], test Loss: 0.7272\n",
            "Epoch [3], Batch [68/157], test Loss: 0.5827\n",
            "Epoch [3], Batch [69/157], test Loss: 0.7021\n",
            "Epoch [3], Batch [70/157], test Loss: 0.7876\n",
            "Epoch [3], Batch [71/157], test Loss: 0.6705\n",
            "Epoch [3], Batch [72/157], test Loss: 0.6878\n",
            "Epoch [3], Batch [73/157], test Loss: 0.8105\n",
            "Epoch [3], Batch [74/157], test Loss: 0.9482\n",
            "Epoch [3], Batch [75/157], test Loss: 0.6809\n",
            "Epoch [3], Batch [76/157], test Loss: 0.7327\n",
            "Epoch [3], Batch [77/157], test Loss: 0.8010\n",
            "Epoch [3], Batch [78/157], test Loss: 0.6904\n",
            "Epoch [3], Batch [79/157], test Loss: 0.7835\n",
            "Epoch [3], Batch [80/157], test Loss: 0.6754\n",
            "Epoch [3], Batch [81/157], test Loss: 0.6020\n",
            "Epoch [3], Batch [82/157], test Loss: 0.7167\n",
            "Epoch [3], Batch [83/157], test Loss: 0.8371\n",
            "Epoch [3], Batch [84/157], test Loss: 0.7620\n",
            "Epoch [3], Batch [85/157], test Loss: 1.0122\n",
            "Epoch [3], Batch [86/157], test Loss: 0.7772\n",
            "Epoch [3], Batch [87/157], test Loss: 0.8310\n",
            "Epoch [3], Batch [88/157], test Loss: 0.9353\n",
            "Epoch [3], Batch [89/157], test Loss: 0.6226\n",
            "Epoch [3], Batch [90/157], test Loss: 0.7822\n",
            "Epoch [3], Batch [91/157], test Loss: 0.8273\n",
            "Epoch [3], Batch [92/157], test Loss: 0.8251\n",
            "Epoch [3], Batch [93/157], test Loss: 0.7787\n",
            "Epoch [3], Batch [94/157], test Loss: 1.0427\n",
            "Epoch [3], Batch [95/157], test Loss: 0.7136\n",
            "Epoch [3], Batch [96/157], test Loss: 0.6618\n",
            "Epoch [3], Batch [97/157], test Loss: 0.8750\n",
            "Epoch [3], Batch [98/157], test Loss: 0.8839\n",
            "Epoch [3], Batch [99/157], test Loss: 0.8628\n",
            "Epoch [3], Batch [100/157], test Loss: 0.6281\n",
            "Epoch [3], Batch [101/157], test Loss: 0.6985\n",
            "Epoch [3], Batch [102/157], test Loss: 1.0222\n",
            "Epoch [3], Batch [103/157], test Loss: 0.8159\n",
            "Epoch [3], Batch [104/157], test Loss: 0.6411\n",
            "Epoch [3], Batch [105/157], test Loss: 0.8492\n",
            "Epoch [3], Batch [106/157], test Loss: 0.7762\n",
            "Epoch [3], Batch [107/157], test Loss: 0.8266\n",
            "Epoch [3], Batch [108/157], test Loss: 0.6522\n",
            "Epoch [3], Batch [109/157], test Loss: 0.8635\n",
            "Epoch [3], Batch [110/157], test Loss: 0.6662\n",
            "Epoch [3], Batch [111/157], test Loss: 0.7256\n",
            "Epoch [3], Batch [112/157], test Loss: 0.7118\n",
            "Epoch [3], Batch [113/157], test Loss: 0.7974\n",
            "Epoch [3], Batch [114/157], test Loss: 0.6551\n",
            "Epoch [3], Batch [115/157], test Loss: 0.6818\n",
            "Epoch [3], Batch [116/157], test Loss: 0.6424\n",
            "Epoch [3], Batch [117/157], test Loss: 1.0348\n",
            "Epoch [3], Batch [118/157], test Loss: 0.7058\n",
            "Epoch [3], Batch [119/157], test Loss: 0.6066\n",
            "Epoch [3], Batch [120/157], test Loss: 0.7811\n",
            "Epoch [3], Batch [121/157], test Loss: 0.8132\n",
            "Epoch [3], Batch [122/157], test Loss: 0.8030\n",
            "Epoch [3], Batch [123/157], test Loss: 0.7131\n",
            "Epoch [3], Batch [124/157], test Loss: 0.7938\n",
            "Epoch [3], Batch [125/157], test Loss: 0.7689\n",
            "Epoch [3], Batch [126/157], test Loss: 0.7478\n",
            "Epoch [3], Batch [127/157], test Loss: 0.5198\n",
            "Epoch [3], Batch [128/157], test Loss: 0.7646\n",
            "Epoch [3], Batch [129/157], test Loss: 0.7408\n",
            "Epoch [3], Batch [130/157], test Loss: 0.9429\n",
            "Epoch [3], Batch [131/157], test Loss: 0.7112\n",
            "Epoch [3], Batch [132/157], test Loss: 1.0515\n",
            "Epoch [3], Batch [133/157], test Loss: 0.6644\n",
            "Epoch [3], Batch [134/157], test Loss: 0.7633\n",
            "Epoch [3], Batch [135/157], test Loss: 0.8483\n",
            "Epoch [3], Batch [136/157], test Loss: 0.7744\n",
            "Epoch [3], Batch [137/157], test Loss: 0.7421\n",
            "Epoch [3], Batch [138/157], test Loss: 0.8053\n",
            "Epoch [3], Batch [139/157], test Loss: 0.7947\n",
            "Epoch [3], Batch [140/157], test Loss: 0.7369\n",
            "Epoch [3], Batch [141/157], test Loss: 0.6116\n",
            "Epoch [3], Batch [142/157], test Loss: 0.8249\n",
            "Epoch [3], Batch [143/157], test Loss: 0.8305\n",
            "Epoch [3], Batch [144/157], test Loss: 0.6126\n",
            "Epoch [3], Batch [145/157], test Loss: 0.8001\n",
            "Epoch [3], Batch [146/157], test Loss: 0.6991\n",
            "Epoch [3], Batch [147/157], test Loss: 0.8803\n",
            "Epoch [3], Batch [148/157], test Loss: 0.7452\n",
            "Epoch [3], Batch [149/157], test Loss: 0.7401\n",
            "Epoch [3], Batch [150/157], test Loss: 0.7255\n",
            "Epoch [3], Batch [151/157], test Loss: 0.7795\n",
            "Epoch [3], Batch [152/157], test Loss: 0.8052\n",
            "Epoch [3], Batch [153/157], test Loss: 0.7665\n",
            "Epoch [3], Batch [154/157], test Loss: 0.8995\n",
            "Epoch [3], Batch [155/157], test Loss: 0.5910\n",
            "Epoch [3], Batch [156/157], test Loss: 0.7393\n",
            "Epoch [3], Batch [157/157], test Loss: 0.3693\n",
            "Accuracy of test set: 70.39%\n",
            "Epoch [4/25] - Train Loss: 0.7952, Train Accuracy: 70.00% - Test Loss: 0.7810, Test Accuracy: 70.39%\n",
            "Epoch [4], Batch [10/938], Loss: 0.8047\n",
            "Epoch [4], Batch [20/938], Loss: 0.6544\n",
            "Epoch [4], Batch [30/938], Loss: 0.6988\n",
            "Epoch [4], Batch [40/938], Loss: 0.8288\n",
            "Epoch [4], Batch [50/938], Loss: 0.8304\n",
            "Epoch [4], Batch [60/938], Loss: 0.8495\n",
            "Epoch [4], Batch [70/938], Loss: 0.7448\n",
            "Epoch [4], Batch [80/938], Loss: 1.1474\n",
            "Epoch [4], Batch [90/938], Loss: 0.6629\n",
            "Epoch [4], Batch [100/938], Loss: 0.6513\n",
            "Epoch [4], Batch [110/938], Loss: 0.8268\n",
            "Epoch [4], Batch [120/938], Loss: 0.6352\n",
            "Epoch [4], Batch [130/938], Loss: 0.6553\n",
            "Epoch [4], Batch [140/938], Loss: 0.6563\n",
            "Epoch [4], Batch [150/938], Loss: 0.6961\n",
            "Epoch [4], Batch [160/938], Loss: 0.8913\n",
            "Epoch [4], Batch [170/938], Loss: 0.7257\n",
            "Epoch [4], Batch [180/938], Loss: 0.7291\n",
            "Epoch [4], Batch [190/938], Loss: 0.7502\n",
            "Epoch [4], Batch [200/938], Loss: 0.8452\n",
            "Epoch [4], Batch [210/938], Loss: 0.5132\n",
            "Epoch [4], Batch [220/938], Loss: 0.6943\n",
            "Epoch [4], Batch [230/938], Loss: 0.6516\n",
            "Epoch [4], Batch [240/938], Loss: 0.5765\n",
            "Epoch [4], Batch [250/938], Loss: 0.7631\n",
            "Epoch [4], Batch [260/938], Loss: 1.1391\n",
            "Epoch [4], Batch [270/938], Loss: 1.0037\n",
            "Epoch [4], Batch [280/938], Loss: 0.6981\n",
            "Epoch [4], Batch [290/938], Loss: 0.6167\n",
            "Epoch [4], Batch [300/938], Loss: 0.6947\n",
            "Epoch [4], Batch [310/938], Loss: 0.7902\n",
            "Epoch [4], Batch [320/938], Loss: 0.6196\n",
            "Epoch [4], Batch [330/938], Loss: 0.7022\n",
            "Epoch [4], Batch [340/938], Loss: 1.0011\n",
            "Epoch [4], Batch [350/938], Loss: 0.9832\n",
            "Epoch [4], Batch [360/938], Loss: 0.6475\n",
            "Epoch [4], Batch [370/938], Loss: 0.5976\n",
            "Epoch [4], Batch [380/938], Loss: 0.8713\n",
            "Epoch [4], Batch [390/938], Loss: 0.8227\n",
            "Epoch [4], Batch [400/938], Loss: 0.8239\n",
            "Epoch [4], Batch [410/938], Loss: 0.8432\n",
            "Epoch [4], Batch [420/938], Loss: 0.7587\n",
            "Epoch [4], Batch [430/938], Loss: 0.6329\n",
            "Epoch [4], Batch [440/938], Loss: 0.8758\n",
            "Epoch [4], Batch [450/938], Loss: 0.7566\n",
            "Epoch [4], Batch [460/938], Loss: 0.7394\n",
            "Epoch [4], Batch [470/938], Loss: 0.9848\n",
            "Epoch [4], Batch [480/938], Loss: 0.7704\n",
            "Epoch [4], Batch [490/938], Loss: 0.7834\n",
            "Epoch [4], Batch [500/938], Loss: 0.7763\n",
            "Epoch [4], Batch [510/938], Loss: 0.7372\n",
            "Epoch [4], Batch [520/938], Loss: 0.8199\n",
            "Epoch [4], Batch [530/938], Loss: 0.7582\n",
            "Epoch [4], Batch [540/938], Loss: 0.7607\n",
            "Epoch [4], Batch [550/938], Loss: 0.7413\n",
            "Epoch [4], Batch [560/938], Loss: 0.7441\n",
            "Epoch [4], Batch [570/938], Loss: 0.4857\n",
            "Epoch [4], Batch [580/938], Loss: 0.8167\n",
            "Epoch [4], Batch [590/938], Loss: 0.8114\n",
            "Epoch [4], Batch [600/938], Loss: 0.6605\n",
            "Epoch [4], Batch [610/938], Loss: 0.7980\n",
            "Epoch [4], Batch [620/938], Loss: 0.6634\n",
            "Epoch [4], Batch [630/938], Loss: 0.7200\n",
            "Epoch [4], Batch [640/938], Loss: 0.8413\n",
            "Epoch [4], Batch [650/938], Loss: 0.6244\n",
            "Epoch [4], Batch [660/938], Loss: 0.5460\n",
            "Epoch [4], Batch [670/938], Loss: 0.6533\n",
            "Epoch [4], Batch [680/938], Loss: 0.9344\n",
            "Epoch [4], Batch [690/938], Loss: 0.9642\n",
            "Epoch [4], Batch [700/938], Loss: 0.5646\n",
            "Epoch [4], Batch [710/938], Loss: 0.7664\n",
            "Epoch [4], Batch [720/938], Loss: 0.6150\n",
            "Epoch [4], Batch [730/938], Loss: 0.6838\n",
            "Epoch [4], Batch [740/938], Loss: 0.6597\n",
            "Epoch [4], Batch [750/938], Loss: 0.5847\n",
            "Epoch [4], Batch [760/938], Loss: 0.5252\n",
            "Epoch [4], Batch [770/938], Loss: 0.5960\n",
            "Epoch [4], Batch [780/938], Loss: 0.5593\n",
            "Epoch [4], Batch [790/938], Loss: 0.5705\n",
            "Epoch [4], Batch [800/938], Loss: 0.7894\n",
            "Epoch [4], Batch [810/938], Loss: 0.6307\n",
            "Epoch [4], Batch [820/938], Loss: 0.7794\n",
            "Epoch [4], Batch [830/938], Loss: 0.5849\n",
            "Epoch [4], Batch [840/938], Loss: 0.6474\n",
            "Epoch [4], Batch [850/938], Loss: 0.9790\n",
            "Epoch [4], Batch [860/938], Loss: 0.7650\n",
            "Epoch [4], Batch [870/938], Loss: 0.5650\n",
            "Epoch [4], Batch [880/938], Loss: 0.8013\n",
            "Epoch [4], Batch [890/938], Loss: 0.8332\n",
            "Epoch [4], Batch [900/938], Loss: 0.9157\n",
            "Epoch [4], Batch [910/938], Loss: 0.6266\n",
            "Epoch [4], Batch [920/938], Loss: 0.7002\n",
            "Epoch [4], Batch [930/938], Loss: 0.7872\n",
            "Epoch [4], Batch [938/938], Loss: 0.6209\n",
            "Accuracy of train set: 73.72%\n",
            "Epoch [4], Batch [1/157], test Loss: 0.7308\n",
            "Epoch [4], Batch [2/157], test Loss: 0.6730\n",
            "Epoch [4], Batch [3/157], test Loss: 0.6720\n",
            "Epoch [4], Batch [4/157], test Loss: 0.6377\n",
            "Epoch [4], Batch [5/157], test Loss: 0.6625\n",
            "Epoch [4], Batch [6/157], test Loss: 0.7320\n",
            "Epoch [4], Batch [7/157], test Loss: 0.7161\n",
            "Epoch [4], Batch [8/157], test Loss: 0.7470\n",
            "Epoch [4], Batch [9/157], test Loss: 0.5237\n",
            "Epoch [4], Batch [10/157], test Loss: 0.7944\n",
            "Epoch [4], Batch [11/157], test Loss: 0.6686\n",
            "Epoch [4], Batch [12/157], test Loss: 0.7905\n",
            "Epoch [4], Batch [13/157], test Loss: 0.7614\n",
            "Epoch [4], Batch [14/157], test Loss: 0.8289\n",
            "Epoch [4], Batch [15/157], test Loss: 0.6635\n",
            "Epoch [4], Batch [16/157], test Loss: 0.9104\n",
            "Epoch [4], Batch [17/157], test Loss: 0.8272\n",
            "Epoch [4], Batch [18/157], test Loss: 0.6167\n",
            "Epoch [4], Batch [19/157], test Loss: 0.8610\n",
            "Epoch [4], Batch [20/157], test Loss: 0.6617\n",
            "Epoch [4], Batch [21/157], test Loss: 0.7613\n",
            "Epoch [4], Batch [22/157], test Loss: 0.7397\n",
            "Epoch [4], Batch [23/157], test Loss: 0.5665\n",
            "Epoch [4], Batch [24/157], test Loss: 0.6558\n",
            "Epoch [4], Batch [25/157], test Loss: 0.5261\n",
            "Epoch [4], Batch [26/157], test Loss: 0.8571\n",
            "Epoch [4], Batch [27/157], test Loss: 0.9300\n",
            "Epoch [4], Batch [28/157], test Loss: 0.6114\n",
            "Epoch [4], Batch [29/157], test Loss: 0.7546\n",
            "Epoch [4], Batch [30/157], test Loss: 1.2385\n",
            "Epoch [4], Batch [31/157], test Loss: 0.7969\n",
            "Epoch [4], Batch [32/157], test Loss: 0.6382\n",
            "Epoch [4], Batch [33/157], test Loss: 0.5536\n",
            "Epoch [4], Batch [34/157], test Loss: 0.7199\n",
            "Epoch [4], Batch [35/157], test Loss: 0.6413\n",
            "Epoch [4], Batch [36/157], test Loss: 0.8347\n",
            "Epoch [4], Batch [37/157], test Loss: 0.6100\n",
            "Epoch [4], Batch [38/157], test Loss: 0.6910\n",
            "Epoch [4], Batch [39/157], test Loss: 0.7490\n",
            "Epoch [4], Batch [40/157], test Loss: 0.7221\n",
            "Epoch [4], Batch [41/157], test Loss: 0.6896\n",
            "Epoch [4], Batch [42/157], test Loss: 0.6317\n",
            "Epoch [4], Batch [43/157], test Loss: 0.7433\n",
            "Epoch [4], Batch [44/157], test Loss: 0.8534\n",
            "Epoch [4], Batch [45/157], test Loss: 0.7751\n",
            "Epoch [4], Batch [46/157], test Loss: 0.6142\n",
            "Epoch [4], Batch [47/157], test Loss: 0.9084\n",
            "Epoch [4], Batch [48/157], test Loss: 0.7529\n",
            "Epoch [4], Batch [49/157], test Loss: 0.5781\n",
            "Epoch [4], Batch [50/157], test Loss: 0.6757\n",
            "Epoch [4], Batch [51/157], test Loss: 0.8158\n",
            "Epoch [4], Batch [52/157], test Loss: 0.7058\n",
            "Epoch [4], Batch [53/157], test Loss: 0.6573\n",
            "Epoch [4], Batch [54/157], test Loss: 0.7421\n",
            "Epoch [4], Batch [55/157], test Loss: 0.6053\n",
            "Epoch [4], Batch [56/157], test Loss: 0.6148\n",
            "Epoch [4], Batch [57/157], test Loss: 0.6615\n",
            "Epoch [4], Batch [58/157], test Loss: 0.9105\n",
            "Epoch [4], Batch [59/157], test Loss: 0.7455\n",
            "Epoch [4], Batch [60/157], test Loss: 0.6046\n",
            "Epoch [4], Batch [61/157], test Loss: 0.7590\n",
            "Epoch [4], Batch [62/157], test Loss: 0.8521\n",
            "Epoch [4], Batch [63/157], test Loss: 0.8035\n",
            "Epoch [4], Batch [64/157], test Loss: 0.9987\n",
            "Epoch [4], Batch [65/157], test Loss: 0.7214\n",
            "Epoch [4], Batch [66/157], test Loss: 0.5908\n",
            "Epoch [4], Batch [67/157], test Loss: 0.7562\n",
            "Epoch [4], Batch [68/157], test Loss: 0.6443\n",
            "Epoch [4], Batch [69/157], test Loss: 0.6794\n",
            "Epoch [4], Batch [70/157], test Loss: 0.5052\n",
            "Epoch [4], Batch [71/157], test Loss: 0.6764\n",
            "Epoch [4], Batch [72/157], test Loss: 0.8207\n",
            "Epoch [4], Batch [73/157], test Loss: 0.7035\n",
            "Epoch [4], Batch [74/157], test Loss: 0.7237\n",
            "Epoch [4], Batch [75/157], test Loss: 0.6876\n",
            "Epoch [4], Batch [76/157], test Loss: 1.0401\n",
            "Epoch [4], Batch [77/157], test Loss: 0.6554\n",
            "Epoch [4], Batch [78/157], test Loss: 0.5884\n",
            "Epoch [4], Batch [79/157], test Loss: 0.7682\n",
            "Epoch [4], Batch [80/157], test Loss: 0.8307\n",
            "Epoch [4], Batch [81/157], test Loss: 0.5476\n",
            "Epoch [4], Batch [82/157], test Loss: 0.6787\n",
            "Epoch [4], Batch [83/157], test Loss: 0.7222\n",
            "Epoch [4], Batch [84/157], test Loss: 0.4669\n",
            "Epoch [4], Batch [85/157], test Loss: 0.8149\n",
            "Epoch [4], Batch [86/157], test Loss: 0.7711\n",
            "Epoch [4], Batch [87/157], test Loss: 0.7601\n",
            "Epoch [4], Batch [88/157], test Loss: 0.7635\n",
            "Epoch [4], Batch [89/157], test Loss: 0.5953\n",
            "Epoch [4], Batch [90/157], test Loss: 0.8705\n",
            "Epoch [4], Batch [91/157], test Loss: 0.7929\n",
            "Epoch [4], Batch [92/157], test Loss: 0.5378\n",
            "Epoch [4], Batch [93/157], test Loss: 0.7309\n",
            "Epoch [4], Batch [94/157], test Loss: 0.6664\n",
            "Epoch [4], Batch [95/157], test Loss: 0.7689\n",
            "Epoch [4], Batch [96/157], test Loss: 0.8286\n",
            "Epoch [4], Batch [97/157], test Loss: 0.9353\n",
            "Epoch [4], Batch [98/157], test Loss: 0.6234\n",
            "Epoch [4], Batch [99/157], test Loss: 1.0995\n",
            "Epoch [4], Batch [100/157], test Loss: 0.8263\n",
            "Epoch [4], Batch [101/157], test Loss: 0.7341\n",
            "Epoch [4], Batch [102/157], test Loss: 0.7341\n",
            "Epoch [4], Batch [103/157], test Loss: 0.7625\n",
            "Epoch [4], Batch [104/157], test Loss: 0.6221\n",
            "Epoch [4], Batch [105/157], test Loss: 0.8218\n",
            "Epoch [4], Batch [106/157], test Loss: 0.6343\n",
            "Epoch [4], Batch [107/157], test Loss: 0.9085\n",
            "Epoch [4], Batch [108/157], test Loss: 0.7576\n",
            "Epoch [4], Batch [109/157], test Loss: 0.8376\n",
            "Epoch [4], Batch [110/157], test Loss: 0.7041\n",
            "Epoch [4], Batch [111/157], test Loss: 0.6520\n",
            "Epoch [4], Batch [112/157], test Loss: 0.9286\n",
            "Epoch [4], Batch [113/157], test Loss: 0.5858\n",
            "Epoch [4], Batch [114/157], test Loss: 0.7659\n",
            "Epoch [4], Batch [115/157], test Loss: 0.7049\n",
            "Epoch [4], Batch [116/157], test Loss: 0.5799\n",
            "Epoch [4], Batch [117/157], test Loss: 0.6114\n",
            "Epoch [4], Batch [118/157], test Loss: 0.6623\n",
            "Epoch [4], Batch [119/157], test Loss: 0.6583\n",
            "Epoch [4], Batch [120/157], test Loss: 0.7213\n",
            "Epoch [4], Batch [121/157], test Loss: 0.5732\n",
            "Epoch [4], Batch [122/157], test Loss: 0.7795\n",
            "Epoch [4], Batch [123/157], test Loss: 0.7234\n",
            "Epoch [4], Batch [124/157], test Loss: 0.7415\n",
            "Epoch [4], Batch [125/157], test Loss: 0.7559\n",
            "Epoch [4], Batch [126/157], test Loss: 0.5767\n",
            "Epoch [4], Batch [127/157], test Loss: 0.7129\n",
            "Epoch [4], Batch [128/157], test Loss: 0.7054\n",
            "Epoch [4], Batch [129/157], test Loss: 0.6535\n",
            "Epoch [4], Batch [130/157], test Loss: 0.6856\n",
            "Epoch [4], Batch [131/157], test Loss: 0.6922\n",
            "Epoch [4], Batch [132/157], test Loss: 0.6224\n",
            "Epoch [4], Batch [133/157], test Loss: 0.5915\n",
            "Epoch [4], Batch [134/157], test Loss: 0.5830\n",
            "Epoch [4], Batch [135/157], test Loss: 0.5167\n",
            "Epoch [4], Batch [136/157], test Loss: 0.6897\n",
            "Epoch [4], Batch [137/157], test Loss: 0.5700\n",
            "Epoch [4], Batch [138/157], test Loss: 0.5955\n",
            "Epoch [4], Batch [139/157], test Loss: 0.9900\n",
            "Epoch [4], Batch [140/157], test Loss: 0.6795\n",
            "Epoch [4], Batch [141/157], test Loss: 0.8859\n",
            "Epoch [4], Batch [142/157], test Loss: 0.5515\n",
            "Epoch [4], Batch [143/157], test Loss: 0.7632\n",
            "Epoch [4], Batch [144/157], test Loss: 0.5715\n",
            "Epoch [4], Batch [145/157], test Loss: 0.8037\n",
            "Epoch [4], Batch [146/157], test Loss: 0.9677\n",
            "Epoch [4], Batch [147/157], test Loss: 0.9494\n",
            "Epoch [4], Batch [148/157], test Loss: 0.5495\n",
            "Epoch [4], Batch [149/157], test Loss: 0.6605\n",
            "Epoch [4], Batch [150/157], test Loss: 0.7435\n",
            "Epoch [4], Batch [151/157], test Loss: 0.6944\n",
            "Epoch [4], Batch [152/157], test Loss: 0.7899\n",
            "Epoch [4], Batch [153/157], test Loss: 0.6839\n",
            "Epoch [4], Batch [154/157], test Loss: 0.6520\n",
            "Epoch [4], Batch [155/157], test Loss: 0.5755\n",
            "Epoch [4], Batch [156/157], test Loss: 0.6404\n",
            "Epoch [4], Batch [157/157], test Loss: 0.7838\n",
            "Accuracy of test set: 73.54%\n",
            "Epoch [5/25] - Train Loss: 0.7299, Train Accuracy: 73.72% - Test Loss: 0.7201, Test Accuracy: 73.54%\n",
            "Epoch [5], Batch [10/938], Loss: 0.6565\n",
            "Epoch [5], Batch [20/938], Loss: 0.7967\n",
            "Epoch [5], Batch [30/938], Loss: 0.7332\n",
            "Epoch [5], Batch [40/938], Loss: 0.6460\n",
            "Epoch [5], Batch [50/938], Loss: 0.7114\n",
            "Epoch [5], Batch [60/938], Loss: 0.5768\n",
            "Epoch [5], Batch [70/938], Loss: 0.6268\n",
            "Epoch [5], Batch [80/938], Loss: 0.7791\n",
            "Epoch [5], Batch [90/938], Loss: 0.8619\n",
            "Epoch [5], Batch [100/938], Loss: 0.6712\n",
            "Epoch [5], Batch [110/938], Loss: 0.7865\n",
            "Epoch [5], Batch [120/938], Loss: 0.6790\n",
            "Epoch [5], Batch [130/938], Loss: 0.8202\n",
            "Epoch [5], Batch [140/938], Loss: 0.5745\n",
            "Epoch [5], Batch [150/938], Loss: 0.6507\n",
            "Epoch [5], Batch [160/938], Loss: 0.7492\n",
            "Epoch [5], Batch [170/938], Loss: 0.6055\n",
            "Epoch [5], Batch [180/938], Loss: 0.6238\n",
            "Epoch [5], Batch [190/938], Loss: 0.5440\n",
            "Epoch [5], Batch [200/938], Loss: 0.8271\n",
            "Epoch [5], Batch [210/938], Loss: 0.6950\n",
            "Epoch [5], Batch [220/938], Loss: 0.6415\n",
            "Epoch [5], Batch [230/938], Loss: 0.6635\n",
            "Epoch [5], Batch [240/938], Loss: 0.6404\n",
            "Epoch [5], Batch [250/938], Loss: 0.8130\n",
            "Epoch [5], Batch [260/938], Loss: 0.7007\n",
            "Epoch [5], Batch [270/938], Loss: 0.4046\n",
            "Epoch [5], Batch [280/938], Loss: 0.8280\n",
            "Epoch [5], Batch [290/938], Loss: 0.4750\n",
            "Epoch [5], Batch [300/938], Loss: 0.6024\n",
            "Epoch [5], Batch [310/938], Loss: 0.6245\n",
            "Epoch [5], Batch [320/938], Loss: 0.6919\n",
            "Epoch [5], Batch [330/938], Loss: 0.8333\n",
            "Epoch [5], Batch [340/938], Loss: 0.7655\n",
            "Epoch [5], Batch [350/938], Loss: 0.5257\n",
            "Epoch [5], Batch [360/938], Loss: 0.6290\n",
            "Epoch [5], Batch [370/938], Loss: 0.7056\n",
            "Epoch [5], Batch [380/938], Loss: 0.8273\n",
            "Epoch [5], Batch [390/938], Loss: 0.6799\n",
            "Epoch [5], Batch [400/938], Loss: 0.9923\n",
            "Epoch [5], Batch [410/938], Loss: 0.7136\n",
            "Epoch [5], Batch [420/938], Loss: 0.5276\n",
            "Epoch [5], Batch [430/938], Loss: 0.6840\n",
            "Epoch [5], Batch [440/938], Loss: 0.7382\n",
            "Epoch [5], Batch [450/938], Loss: 0.6796\n",
            "Epoch [5], Batch [460/938], Loss: 0.5091\n",
            "Epoch [5], Batch [470/938], Loss: 0.6253\n",
            "Epoch [5], Batch [480/938], Loss: 0.6471\n",
            "Epoch [5], Batch [490/938], Loss: 0.6030\n",
            "Epoch [5], Batch [500/938], Loss: 0.5909\n",
            "Epoch [5], Batch [510/938], Loss: 0.6948\n",
            "Epoch [5], Batch [520/938], Loss: 0.6368\n",
            "Epoch [5], Batch [530/938], Loss: 0.5543\n",
            "Epoch [5], Batch [540/938], Loss: 0.6814\n",
            "Epoch [5], Batch [550/938], Loss: 0.7041\n",
            "Epoch [5], Batch [560/938], Loss: 0.6184\n",
            "Epoch [5], Batch [570/938], Loss: 0.5370\n",
            "Epoch [5], Batch [580/938], Loss: 0.6466\n",
            "Epoch [5], Batch [590/938], Loss: 0.5456\n",
            "Epoch [5], Batch [600/938], Loss: 0.5622\n",
            "Epoch [5], Batch [610/938], Loss: 0.5151\n",
            "Epoch [5], Batch [620/938], Loss: 0.7950\n",
            "Epoch [5], Batch [630/938], Loss: 0.7163\n",
            "Epoch [5], Batch [640/938], Loss: 0.6443\n",
            "Epoch [5], Batch [650/938], Loss: 0.5693\n",
            "Epoch [5], Batch [660/938], Loss: 0.5977\n",
            "Epoch [5], Batch [670/938], Loss: 0.6523\n",
            "Epoch [5], Batch [680/938], Loss: 0.6414\n",
            "Epoch [5], Batch [690/938], Loss: 0.5334\n",
            "Epoch [5], Batch [700/938], Loss: 0.7540\n",
            "Epoch [5], Batch [710/938], Loss: 0.5758\n",
            "Epoch [5], Batch [720/938], Loss: 0.5480\n",
            "Epoch [5], Batch [730/938], Loss: 0.8471\n",
            "Epoch [5], Batch [740/938], Loss: 1.1340\n",
            "Epoch [5], Batch [750/938], Loss: 0.5554\n",
            "Epoch [5], Batch [760/938], Loss: 0.7962\n",
            "Epoch [5], Batch [770/938], Loss: 0.6513\n",
            "Epoch [5], Batch [780/938], Loss: 0.5959\n",
            "Epoch [5], Batch [790/938], Loss: 0.6618\n",
            "Epoch [5], Batch [800/938], Loss: 0.7600\n",
            "Epoch [5], Batch [810/938], Loss: 0.6299\n",
            "Epoch [5], Batch [820/938], Loss: 0.6151\n",
            "Epoch [5], Batch [830/938], Loss: 0.5790\n",
            "Epoch [5], Batch [840/938], Loss: 0.6997\n",
            "Epoch [5], Batch [850/938], Loss: 0.7776\n",
            "Epoch [5], Batch [860/938], Loss: 0.6665\n",
            "Epoch [5], Batch [870/938], Loss: 0.5113\n",
            "Epoch [5], Batch [880/938], Loss: 0.6305\n",
            "Epoch [5], Batch [890/938], Loss: 0.6624\n",
            "Epoch [5], Batch [900/938], Loss: 0.7188\n",
            "Epoch [5], Batch [910/938], Loss: 0.5837\n",
            "Epoch [5], Batch [920/938], Loss: 0.4852\n",
            "Epoch [5], Batch [930/938], Loss: 0.5826\n",
            "Epoch [5], Batch [938/938], Loss: 0.4633\n",
            "Accuracy of train set: 76.34%\n",
            "Epoch [5], Batch [1/157], test Loss: 0.4380\n",
            "Epoch [5], Batch [2/157], test Loss: 0.7387\n",
            "Epoch [5], Batch [3/157], test Loss: 0.9014\n",
            "Epoch [5], Batch [4/157], test Loss: 0.8229\n",
            "Epoch [5], Batch [5/157], test Loss: 0.7381\n",
            "Epoch [5], Batch [6/157], test Loss: 0.6383\n",
            "Epoch [5], Batch [7/157], test Loss: 0.8924\n",
            "Epoch [5], Batch [8/157], test Loss: 0.6842\n",
            "Epoch [5], Batch [9/157], test Loss: 0.5683\n",
            "Epoch [5], Batch [10/157], test Loss: 0.7809\n",
            "Epoch [5], Batch [11/157], test Loss: 0.7273\n",
            "Epoch [5], Batch [12/157], test Loss: 0.7099\n",
            "Epoch [5], Batch [13/157], test Loss: 0.5624\n",
            "Epoch [5], Batch [14/157], test Loss: 0.5785\n",
            "Epoch [5], Batch [15/157], test Loss: 0.7167\n",
            "Epoch [5], Batch [16/157], test Loss: 0.7115\n",
            "Epoch [5], Batch [17/157], test Loss: 0.8770\n",
            "Epoch [5], Batch [18/157], test Loss: 0.7554\n",
            "Epoch [5], Batch [19/157], test Loss: 0.9010\n",
            "Epoch [5], Batch [20/157], test Loss: 0.8689\n",
            "Epoch [5], Batch [21/157], test Loss: 0.5082\n",
            "Epoch [5], Batch [22/157], test Loss: 0.8444\n",
            "Epoch [5], Batch [23/157], test Loss: 0.9819\n",
            "Epoch [5], Batch [24/157], test Loss: 0.6856\n",
            "Epoch [5], Batch [25/157], test Loss: 0.5683\n",
            "Epoch [5], Batch [26/157], test Loss: 0.5987\n",
            "Epoch [5], Batch [27/157], test Loss: 0.4365\n",
            "Epoch [5], Batch [28/157], test Loss: 0.7015\n",
            "Epoch [5], Batch [29/157], test Loss: 0.6102\n",
            "Epoch [5], Batch [30/157], test Loss: 0.8932\n",
            "Epoch [5], Batch [31/157], test Loss: 0.8574\n",
            "Epoch [5], Batch [32/157], test Loss: 0.5165\n",
            "Epoch [5], Batch [33/157], test Loss: 0.7573\n",
            "Epoch [5], Batch [34/157], test Loss: 0.6194\n",
            "Epoch [5], Batch [35/157], test Loss: 0.6846\n",
            "Epoch [5], Batch [36/157], test Loss: 0.8302\n",
            "Epoch [5], Batch [37/157], test Loss: 0.9116\n",
            "Epoch [5], Batch [38/157], test Loss: 0.6729\n",
            "Epoch [5], Batch [39/157], test Loss: 0.6080\n",
            "Epoch [5], Batch [40/157], test Loss: 0.6604\n",
            "Epoch [5], Batch [41/157], test Loss: 0.9087\n",
            "Epoch [5], Batch [42/157], test Loss: 0.6721\n",
            "Epoch [5], Batch [43/157], test Loss: 0.6887\n",
            "Epoch [5], Batch [44/157], test Loss: 0.6855\n",
            "Epoch [5], Batch [45/157], test Loss: 0.6264\n",
            "Epoch [5], Batch [46/157], test Loss: 0.5983\n",
            "Epoch [5], Batch [47/157], test Loss: 0.6626\n",
            "Epoch [5], Batch [48/157], test Loss: 0.6191\n",
            "Epoch [5], Batch [49/157], test Loss: 0.8239\n",
            "Epoch [5], Batch [50/157], test Loss: 0.8276\n",
            "Epoch [5], Batch [51/157], test Loss: 0.6088\n",
            "Epoch [5], Batch [52/157], test Loss: 0.9360\n",
            "Epoch [5], Batch [53/157], test Loss: 0.6748\n",
            "Epoch [5], Batch [54/157], test Loss: 0.8688\n",
            "Epoch [5], Batch [55/157], test Loss: 0.5559\n",
            "Epoch [5], Batch [56/157], test Loss: 0.9476\n",
            "Epoch [5], Batch [57/157], test Loss: 0.6047\n",
            "Epoch [5], Batch [58/157], test Loss: 0.7332\n",
            "Epoch [5], Batch [59/157], test Loss: 0.5470\n",
            "Epoch [5], Batch [60/157], test Loss: 0.8109\n",
            "Epoch [5], Batch [61/157], test Loss: 0.7127\n",
            "Epoch [5], Batch [62/157], test Loss: 0.7025\n",
            "Epoch [5], Batch [63/157], test Loss: 0.5011\n",
            "Epoch [5], Batch [64/157], test Loss: 0.5897\n",
            "Epoch [5], Batch [65/157], test Loss: 0.7660\n",
            "Epoch [5], Batch [66/157], test Loss: 0.5555\n",
            "Epoch [5], Batch [67/157], test Loss: 0.8235\n",
            "Epoch [5], Batch [68/157], test Loss: 0.7241\n",
            "Epoch [5], Batch [69/157], test Loss: 0.7251\n",
            "Epoch [5], Batch [70/157], test Loss: 0.5859\n",
            "Epoch [5], Batch [71/157], test Loss: 0.4467\n",
            "Epoch [5], Batch [72/157], test Loss: 0.6863\n",
            "Epoch [5], Batch [73/157], test Loss: 0.6975\n",
            "Epoch [5], Batch [74/157], test Loss: 0.6595\n",
            "Epoch [5], Batch [75/157], test Loss: 0.7701\n",
            "Epoch [5], Batch [76/157], test Loss: 0.7592\n",
            "Epoch [5], Batch [77/157], test Loss: 0.6723\n",
            "Epoch [5], Batch [78/157], test Loss: 0.8142\n",
            "Epoch [5], Batch [79/157], test Loss: 0.6272\n",
            "Epoch [5], Batch [80/157], test Loss: 0.6967\n",
            "Epoch [5], Batch [81/157], test Loss: 0.6664\n",
            "Epoch [5], Batch [82/157], test Loss: 0.8643\n",
            "Epoch [5], Batch [83/157], test Loss: 0.5154\n",
            "Epoch [5], Batch [84/157], test Loss: 0.6708\n",
            "Epoch [5], Batch [85/157], test Loss: 0.9116\n",
            "Epoch [5], Batch [86/157], test Loss: 0.6154\n",
            "Epoch [5], Batch [87/157], test Loss: 0.9234\n",
            "Epoch [5], Batch [88/157], test Loss: 0.5450\n",
            "Epoch [5], Batch [89/157], test Loss: 0.5648\n",
            "Epoch [5], Batch [90/157], test Loss: 0.5380\n",
            "Epoch [5], Batch [91/157], test Loss: 0.8766\n",
            "Epoch [5], Batch [92/157], test Loss: 0.5095\n",
            "Epoch [5], Batch [93/157], test Loss: 0.8318\n",
            "Epoch [5], Batch [94/157], test Loss: 0.6287\n",
            "Epoch [5], Batch [95/157], test Loss: 0.6718\n",
            "Epoch [5], Batch [96/157], test Loss: 0.6435\n",
            "Epoch [5], Batch [97/157], test Loss: 0.8134\n",
            "Epoch [5], Batch [98/157], test Loss: 0.5885\n",
            "Epoch [5], Batch [99/157], test Loss: 0.6904\n",
            "Epoch [5], Batch [100/157], test Loss: 0.3716\n",
            "Epoch [5], Batch [101/157], test Loss: 0.5816\n",
            "Epoch [5], Batch [102/157], test Loss: 0.8128\n",
            "Epoch [5], Batch [103/157], test Loss: 0.6800\n",
            "Epoch [5], Batch [104/157], test Loss: 0.6996\n",
            "Epoch [5], Batch [105/157], test Loss: 0.4972\n",
            "Epoch [5], Batch [106/157], test Loss: 0.6643\n",
            "Epoch [5], Batch [107/157], test Loss: 0.8185\n",
            "Epoch [5], Batch [108/157], test Loss: 0.6711\n",
            "Epoch [5], Batch [109/157], test Loss: 0.3946\n",
            "Epoch [5], Batch [110/157], test Loss: 0.7704\n",
            "Epoch [5], Batch [111/157], test Loss: 0.4797\n",
            "Epoch [5], Batch [112/157], test Loss: 0.6434\n",
            "Epoch [5], Batch [113/157], test Loss: 0.7405\n",
            "Epoch [5], Batch [114/157], test Loss: 0.7201\n",
            "Epoch [5], Batch [115/157], test Loss: 0.7326\n",
            "Epoch [5], Batch [116/157], test Loss: 0.7016\n",
            "Epoch [5], Batch [117/157], test Loss: 0.6252\n",
            "Epoch [5], Batch [118/157], test Loss: 0.5468\n",
            "Epoch [5], Batch [119/157], test Loss: 0.7114\n",
            "Epoch [5], Batch [120/157], test Loss: 0.6777\n",
            "Epoch [5], Batch [121/157], test Loss: 0.8816\n",
            "Epoch [5], Batch [122/157], test Loss: 0.7236\n",
            "Epoch [5], Batch [123/157], test Loss: 0.9778\n",
            "Epoch [5], Batch [124/157], test Loss: 0.5401\n",
            "Epoch [5], Batch [125/157], test Loss: 0.4583\n",
            "Epoch [5], Batch [126/157], test Loss: 0.6104\n",
            "Epoch [5], Batch [127/157], test Loss: 0.5829\n",
            "Epoch [5], Batch [128/157], test Loss: 0.8379\n",
            "Epoch [5], Batch [129/157], test Loss: 0.5556\n",
            "Epoch [5], Batch [130/157], test Loss: 0.6932\n",
            "Epoch [5], Batch [131/157], test Loss: 0.5782\n",
            "Epoch [5], Batch [132/157], test Loss: 0.7595\n",
            "Epoch [5], Batch [133/157], test Loss: 0.5150\n",
            "Epoch [5], Batch [134/157], test Loss: 0.6599\n",
            "Epoch [5], Batch [135/157], test Loss: 0.5543\n",
            "Epoch [5], Batch [136/157], test Loss: 0.5810\n",
            "Epoch [5], Batch [137/157], test Loss: 0.6691\n",
            "Epoch [5], Batch [138/157], test Loss: 0.6416\n",
            "Epoch [5], Batch [139/157], test Loss: 0.7508\n",
            "Epoch [5], Batch [140/157], test Loss: 0.4822\n",
            "Epoch [5], Batch [141/157], test Loss: 0.6732\n",
            "Epoch [5], Batch [142/157], test Loss: 0.8379\n",
            "Epoch [5], Batch [143/157], test Loss: 0.6623\n",
            "Epoch [5], Batch [144/157], test Loss: 0.7931\n",
            "Epoch [5], Batch [145/157], test Loss: 0.4886\n",
            "Epoch [5], Batch [146/157], test Loss: 0.5934\n",
            "Epoch [5], Batch [147/157], test Loss: 0.7773\n",
            "Epoch [5], Batch [148/157], test Loss: 0.5620\n",
            "Epoch [5], Batch [149/157], test Loss: 0.5011\n",
            "Epoch [5], Batch [150/157], test Loss: 0.6202\n",
            "Epoch [5], Batch [151/157], test Loss: 0.6484\n",
            "Epoch [5], Batch [152/157], test Loss: 0.6078\n",
            "Epoch [5], Batch [153/157], test Loss: 0.5444\n",
            "Epoch [5], Batch [154/157], test Loss: 0.6399\n",
            "Epoch [5], Batch [155/157], test Loss: 0.6667\n",
            "Epoch [5], Batch [156/157], test Loss: 0.8696\n",
            "Epoch [5], Batch [157/157], test Loss: 0.4539\n",
            "Accuracy of test set: 75.46%\n",
            "Epoch [6/25] - Train Loss: 0.6683, Train Accuracy: 76.34% - Test Loss: 0.6820, Test Accuracy: 75.46%\n",
            "Epoch [6], Batch [10/938], Loss: 0.6653\n",
            "Epoch [6], Batch [20/938], Loss: 0.5089\n",
            "Epoch [6], Batch [30/938], Loss: 0.7613\n",
            "Epoch [6], Batch [40/938], Loss: 0.6347\n",
            "Epoch [6], Batch [50/938], Loss: 0.5466\n",
            "Epoch [6], Batch [60/938], Loss: 0.7739\n",
            "Epoch [6], Batch [70/938], Loss: 0.6283\n",
            "Epoch [6], Batch [80/938], Loss: 0.4569\n",
            "Epoch [6], Batch [90/938], Loss: 0.6022\n",
            "Epoch [6], Batch [100/938], Loss: 0.5842\n",
            "Epoch [6], Batch [110/938], Loss: 0.5941\n",
            "Epoch [6], Batch [120/938], Loss: 0.7211\n",
            "Epoch [6], Batch [130/938], Loss: 0.5935\n",
            "Epoch [6], Batch [140/938], Loss: 0.6436\n",
            "Epoch [6], Batch [150/938], Loss: 0.6575\n",
            "Epoch [6], Batch [160/938], Loss: 0.6805\n",
            "Epoch [6], Batch [170/938], Loss: 0.5486\n",
            "Epoch [6], Batch [180/938], Loss: 0.5774\n",
            "Epoch [6], Batch [190/938], Loss: 0.7094\n",
            "Epoch [6], Batch [200/938], Loss: 0.6838\n",
            "Epoch [6], Batch [210/938], Loss: 0.5565\n",
            "Epoch [6], Batch [220/938], Loss: 0.5497\n",
            "Epoch [6], Batch [230/938], Loss: 0.5176\n",
            "Epoch [6], Batch [240/938], Loss: 0.6925\n",
            "Epoch [6], Batch [250/938], Loss: 0.6842\n",
            "Epoch [6], Batch [260/938], Loss: 0.5410\n",
            "Epoch [6], Batch [270/938], Loss: 0.6512\n",
            "Epoch [6], Batch [280/938], Loss: 0.5157\n",
            "Epoch [6], Batch [290/938], Loss: 0.7615\n",
            "Epoch [6], Batch [300/938], Loss: 0.7159\n",
            "Epoch [6], Batch [310/938], Loss: 0.5003\n",
            "Epoch [6], Batch [320/938], Loss: 0.4436\n",
            "Epoch [6], Batch [330/938], Loss: 0.6600\n",
            "Epoch [6], Batch [340/938], Loss: 0.7386\n",
            "Epoch [6], Batch [350/938], Loss: 0.8639\n",
            "Epoch [6], Batch [360/938], Loss: 0.4506\n",
            "Epoch [6], Batch [370/938], Loss: 0.6718\n",
            "Epoch [6], Batch [380/938], Loss: 0.5577\n",
            "Epoch [6], Batch [390/938], Loss: 0.6810\n",
            "Epoch [6], Batch [400/938], Loss: 0.5429\n",
            "Epoch [6], Batch [410/938], Loss: 0.7372\n",
            "Epoch [6], Batch [420/938], Loss: 0.5302\n",
            "Epoch [6], Batch [430/938], Loss: 0.6647\n",
            "Epoch [6], Batch [440/938], Loss: 0.7710\n",
            "Epoch [6], Batch [450/938], Loss: 0.6867\n",
            "Epoch [6], Batch [460/938], Loss: 0.5845\n",
            "Epoch [6], Batch [470/938], Loss: 0.5569\n",
            "Epoch [6], Batch [480/938], Loss: 0.6538\n",
            "Epoch [6], Batch [490/938], Loss: 0.4391\n",
            "Epoch [6], Batch [500/938], Loss: 0.5341\n",
            "Epoch [6], Batch [510/938], Loss: 0.5151\n",
            "Epoch [6], Batch [520/938], Loss: 0.4860\n",
            "Epoch [6], Batch [530/938], Loss: 0.7484\n",
            "Epoch [6], Batch [540/938], Loss: 0.8564\n",
            "Epoch [6], Batch [550/938], Loss: 0.4424\n",
            "Epoch [6], Batch [560/938], Loss: 0.5724\n",
            "Epoch [6], Batch [570/938], Loss: 0.6705\n",
            "Epoch [6], Batch [580/938], Loss: 0.6540\n",
            "Epoch [6], Batch [590/938], Loss: 0.6316\n",
            "Epoch [6], Batch [600/938], Loss: 0.6270\n",
            "Epoch [6], Batch [610/938], Loss: 0.5357\n",
            "Epoch [6], Batch [620/938], Loss: 0.4721\n",
            "Epoch [6], Batch [630/938], Loss: 0.8495\n",
            "Epoch [6], Batch [640/938], Loss: 0.6881\n",
            "Epoch [6], Batch [650/938], Loss: 0.9240\n",
            "Epoch [6], Batch [660/938], Loss: 0.6756\n",
            "Epoch [6], Batch [670/938], Loss: 0.6293\n",
            "Epoch [6], Batch [680/938], Loss: 0.4729\n",
            "Epoch [6], Batch [690/938], Loss: 0.3517\n",
            "Epoch [6], Batch [700/938], Loss: 0.4847\n",
            "Epoch [6], Batch [710/938], Loss: 0.6666\n",
            "Epoch [6], Batch [720/938], Loss: 0.5068\n",
            "Epoch [6], Batch [730/938], Loss: 0.6625\n",
            "Epoch [6], Batch [740/938], Loss: 0.6709\n",
            "Epoch [6], Batch [750/938], Loss: 0.6580\n",
            "Epoch [6], Batch [760/938], Loss: 0.8156\n",
            "Epoch [6], Batch [770/938], Loss: 0.4696\n",
            "Epoch [6], Batch [780/938], Loss: 0.4714\n",
            "Epoch [6], Batch [790/938], Loss: 0.6237\n",
            "Epoch [6], Batch [800/938], Loss: 0.3510\n",
            "Epoch [6], Batch [810/938], Loss: 0.7328\n",
            "Epoch [6], Batch [820/938], Loss: 0.6205\n",
            "Epoch [6], Batch [830/938], Loss: 0.5418\n",
            "Epoch [6], Batch [840/938], Loss: 0.7238\n",
            "Epoch [6], Batch [850/938], Loss: 0.4313\n",
            "Epoch [6], Batch [860/938], Loss: 0.5232\n",
            "Epoch [6], Batch [870/938], Loss: 0.5491\n",
            "Epoch [6], Batch [880/938], Loss: 0.7954\n",
            "Epoch [6], Batch [890/938], Loss: 0.5378\n",
            "Epoch [6], Batch [900/938], Loss: 0.7180\n",
            "Epoch [6], Batch [910/938], Loss: 0.7485\n",
            "Epoch [6], Batch [920/938], Loss: 0.7174\n",
            "Epoch [6], Batch [930/938], Loss: 0.5682\n",
            "Epoch [6], Batch [938/938], Loss: 0.4932\n",
            "Accuracy of train set: 77.78%\n",
            "Epoch [6], Batch [1/157], test Loss: 0.6176\n",
            "Epoch [6], Batch [2/157], test Loss: 0.7534\n",
            "Epoch [6], Batch [3/157], test Loss: 0.7338\n",
            "Epoch [6], Batch [4/157], test Loss: 0.6676\n",
            "Epoch [6], Batch [5/157], test Loss: 0.6726\n",
            "Epoch [6], Batch [6/157], test Loss: 0.5950\n",
            "Epoch [6], Batch [7/157], test Loss: 0.3879\n",
            "Epoch [6], Batch [8/157], test Loss: 0.5629\n",
            "Epoch [6], Batch [9/157], test Loss: 0.5966\n",
            "Epoch [6], Batch [10/157], test Loss: 0.4874\n",
            "Epoch [6], Batch [11/157], test Loss: 0.7066\n",
            "Epoch [6], Batch [12/157], test Loss: 0.8038\n",
            "Epoch [6], Batch [13/157], test Loss: 0.7345\n",
            "Epoch [6], Batch [14/157], test Loss: 0.6686\n",
            "Epoch [6], Batch [15/157], test Loss: 0.6707\n",
            "Epoch [6], Batch [16/157], test Loss: 0.6191\n",
            "Epoch [6], Batch [17/157], test Loss: 0.7730\n",
            "Epoch [6], Batch [18/157], test Loss: 0.5491\n",
            "Epoch [6], Batch [19/157], test Loss: 0.5684\n",
            "Epoch [6], Batch [20/157], test Loss: 0.6718\n",
            "Epoch [6], Batch [21/157], test Loss: 0.6124\n",
            "Epoch [6], Batch [22/157], test Loss: 0.6867\n",
            "Epoch [6], Batch [23/157], test Loss: 0.7983\n",
            "Epoch [6], Batch [24/157], test Loss: 0.7947\n",
            "Epoch [6], Batch [25/157], test Loss: 0.7338\n",
            "Epoch [6], Batch [26/157], test Loss: 0.5841\n",
            "Epoch [6], Batch [27/157], test Loss: 0.5570\n",
            "Epoch [6], Batch [28/157], test Loss: 0.5862\n",
            "Epoch [6], Batch [29/157], test Loss: 0.7671\n",
            "Epoch [6], Batch [30/157], test Loss: 0.5572\n",
            "Epoch [6], Batch [31/157], test Loss: 0.7324\n",
            "Epoch [6], Batch [32/157], test Loss: 0.5811\n",
            "Epoch [6], Batch [33/157], test Loss: 0.6770\n",
            "Epoch [6], Batch [34/157], test Loss: 0.6228\n",
            "Epoch [6], Batch [35/157], test Loss: 0.7401\n",
            "Epoch [6], Batch [36/157], test Loss: 0.6372\n",
            "Epoch [6], Batch [37/157], test Loss: 0.4122\n",
            "Epoch [6], Batch [38/157], test Loss: 0.3841\n",
            "Epoch [6], Batch [39/157], test Loss: 0.5437\n",
            "Epoch [6], Batch [40/157], test Loss: 0.6226\n",
            "Epoch [6], Batch [41/157], test Loss: 0.4900\n",
            "Epoch [6], Batch [42/157], test Loss: 0.5196\n",
            "Epoch [6], Batch [43/157], test Loss: 0.4875\n",
            "Epoch [6], Batch [44/157], test Loss: 0.5785\n",
            "Epoch [6], Batch [45/157], test Loss: 0.7003\n",
            "Epoch [6], Batch [46/157], test Loss: 0.6215\n",
            "Epoch [6], Batch [47/157], test Loss: 0.5059\n",
            "Epoch [6], Batch [48/157], test Loss: 0.5674\n",
            "Epoch [6], Batch [49/157], test Loss: 0.6374\n",
            "Epoch [6], Batch [50/157], test Loss: 0.5855\n",
            "Epoch [6], Batch [51/157], test Loss: 0.6825\n",
            "Epoch [6], Batch [52/157], test Loss: 0.6763\n",
            "Epoch [6], Batch [53/157], test Loss: 0.5596\n",
            "Epoch [6], Batch [54/157], test Loss: 0.7981\n",
            "Epoch [6], Batch [55/157], test Loss: 0.6738\n",
            "Epoch [6], Batch [56/157], test Loss: 0.5527\n",
            "Epoch [6], Batch [57/157], test Loss: 0.5191\n",
            "Epoch [6], Batch [58/157], test Loss: 0.8650\n",
            "Epoch [6], Batch [59/157], test Loss: 0.5902\n",
            "Epoch [6], Batch [60/157], test Loss: 0.8241\n",
            "Epoch [6], Batch [61/157], test Loss: 0.5487\n",
            "Epoch [6], Batch [62/157], test Loss: 0.6470\n",
            "Epoch [6], Batch [63/157], test Loss: 0.6683\n",
            "Epoch [6], Batch [64/157], test Loss: 0.5924\n",
            "Epoch [6], Batch [65/157], test Loss: 0.7542\n",
            "Epoch [6], Batch [66/157], test Loss: 0.5278\n",
            "Epoch [6], Batch [67/157], test Loss: 0.7372\n",
            "Epoch [6], Batch [68/157], test Loss: 0.6863\n",
            "Epoch [6], Batch [69/157], test Loss: 0.5547\n",
            "Epoch [6], Batch [70/157], test Loss: 0.5609\n",
            "Epoch [6], Batch [71/157], test Loss: 0.6217\n",
            "Epoch [6], Batch [72/157], test Loss: 0.8987\n",
            "Epoch [6], Batch [73/157], test Loss: 0.4411\n",
            "Epoch [6], Batch [74/157], test Loss: 0.7060\n",
            "Epoch [6], Batch [75/157], test Loss: 0.5842\n",
            "Epoch [6], Batch [76/157], test Loss: 0.3405\n",
            "Epoch [6], Batch [77/157], test Loss: 0.6431\n",
            "Epoch [6], Batch [78/157], test Loss: 0.4567\n",
            "Epoch [6], Batch [79/157], test Loss: 0.5698\n",
            "Epoch [6], Batch [80/157], test Loss: 0.8926\n",
            "Epoch [6], Batch [81/157], test Loss: 0.5827\n",
            "Epoch [6], Batch [82/157], test Loss: 0.6124\n",
            "Epoch [6], Batch [83/157], test Loss: 0.3897\n",
            "Epoch [6], Batch [84/157], test Loss: 0.4306\n",
            "Epoch [6], Batch [85/157], test Loss: 0.5344\n",
            "Epoch [6], Batch [86/157], test Loss: 0.5956\n",
            "Epoch [6], Batch [87/157], test Loss: 0.8153\n",
            "Epoch [6], Batch [88/157], test Loss: 0.7480\n",
            "Epoch [6], Batch [89/157], test Loss: 0.6287\n",
            "Epoch [6], Batch [90/157], test Loss: 0.7352\n",
            "Epoch [6], Batch [91/157], test Loss: 0.6720\n",
            "Epoch [6], Batch [92/157], test Loss: 0.5574\n",
            "Epoch [6], Batch [93/157], test Loss: 0.5038\n",
            "Epoch [6], Batch [94/157], test Loss: 0.6820\n",
            "Epoch [6], Batch [95/157], test Loss: 0.6567\n",
            "Epoch [6], Batch [96/157], test Loss: 0.6653\n",
            "Epoch [6], Batch [97/157], test Loss: 0.5029\n",
            "Epoch [6], Batch [98/157], test Loss: 0.5213\n",
            "Epoch [6], Batch [99/157], test Loss: 0.4342\n",
            "Epoch [6], Batch [100/157], test Loss: 0.7252\n",
            "Epoch [6], Batch [101/157], test Loss: 0.6365\n",
            "Epoch [6], Batch [102/157], test Loss: 0.8037\n",
            "Epoch [6], Batch [103/157], test Loss: 0.6224\n",
            "Epoch [6], Batch [104/157], test Loss: 0.7771\n",
            "Epoch [6], Batch [105/157], test Loss: 0.7029\n",
            "Epoch [6], Batch [106/157], test Loss: 0.6803\n",
            "Epoch [6], Batch [107/157], test Loss: 0.5575\n",
            "Epoch [6], Batch [108/157], test Loss: 0.6672\n",
            "Epoch [6], Batch [109/157], test Loss: 0.4591\n",
            "Epoch [6], Batch [110/157], test Loss: 0.6397\n",
            "Epoch [6], Batch [111/157], test Loss: 0.7648\n",
            "Epoch [6], Batch [112/157], test Loss: 0.6164\n",
            "Epoch [6], Batch [113/157], test Loss: 0.5854\n",
            "Epoch [6], Batch [114/157], test Loss: 0.7938\n",
            "Epoch [6], Batch [115/157], test Loss: 0.4884\n",
            "Epoch [6], Batch [116/157], test Loss: 0.6803\n",
            "Epoch [6], Batch [117/157], test Loss: 0.6141\n",
            "Epoch [6], Batch [118/157], test Loss: 0.5511\n",
            "Epoch [6], Batch [119/157], test Loss: 0.6389\n",
            "Epoch [6], Batch [120/157], test Loss: 0.7022\n",
            "Epoch [6], Batch [121/157], test Loss: 0.5586\n",
            "Epoch [6], Batch [122/157], test Loss: 0.8625\n",
            "Epoch [6], Batch [123/157], test Loss: 0.5866\n",
            "Epoch [6], Batch [124/157], test Loss: 0.6281\n",
            "Epoch [6], Batch [125/157], test Loss: 0.5248\n",
            "Epoch [6], Batch [126/157], test Loss: 0.6115\n",
            "Epoch [6], Batch [127/157], test Loss: 0.6788\n",
            "Epoch [6], Batch [128/157], test Loss: 0.6789\n",
            "Epoch [6], Batch [129/157], test Loss: 0.4792\n",
            "Epoch [6], Batch [130/157], test Loss: 0.6408\n",
            "Epoch [6], Batch [131/157], test Loss: 0.7059\n",
            "Epoch [6], Batch [132/157], test Loss: 0.7095\n",
            "Epoch [6], Batch [133/157], test Loss: 0.4937\n",
            "Epoch [6], Batch [134/157], test Loss: 0.7237\n",
            "Epoch [6], Batch [135/157], test Loss: 0.9260\n",
            "Epoch [6], Batch [136/157], test Loss: 0.6537\n",
            "Epoch [6], Batch [137/157], test Loss: 0.5668\n",
            "Epoch [6], Batch [138/157], test Loss: 0.7252\n",
            "Epoch [6], Batch [139/157], test Loss: 0.8959\n",
            "Epoch [6], Batch [140/157], test Loss: 0.5688\n",
            "Epoch [6], Batch [141/157], test Loss: 0.5241\n",
            "Epoch [6], Batch [142/157], test Loss: 0.8599\n",
            "Epoch [6], Batch [143/157], test Loss: 0.5484\n",
            "Epoch [6], Batch [144/157], test Loss: 0.6414\n",
            "Epoch [6], Batch [145/157], test Loss: 0.6949\n",
            "Epoch [6], Batch [146/157], test Loss: 0.7095\n",
            "Epoch [6], Batch [147/157], test Loss: 0.5363\n",
            "Epoch [6], Batch [148/157], test Loss: 0.3684\n",
            "Epoch [6], Batch [149/157], test Loss: 0.6362\n",
            "Epoch [6], Batch [150/157], test Loss: 0.5721\n",
            "Epoch [6], Batch [151/157], test Loss: 0.6254\n",
            "Epoch [6], Batch [152/157], test Loss: 0.6235\n",
            "Epoch [6], Batch [153/157], test Loss: 0.5625\n",
            "Epoch [6], Batch [154/157], test Loss: 0.5451\n",
            "Epoch [6], Batch [155/157], test Loss: 0.5778\n",
            "Epoch [6], Batch [156/157], test Loss: 0.5736\n",
            "Epoch [6], Batch [157/157], test Loss: 0.3269\n",
            "Accuracy of test set: 77.20%\n",
            "Epoch [7/25] - Train Loss: 0.6210, Train Accuracy: 77.78% - Test Loss: 0.6271, Test Accuracy: 77.20%\n",
            "Epoch [7], Batch [10/938], Loss: 0.4241\n",
            "Epoch [7], Batch [20/938], Loss: 1.0523\n",
            "Epoch [7], Batch [30/938], Loss: 0.6223\n",
            "Epoch [7], Batch [40/938], Loss: 0.5217\n",
            "Epoch [7], Batch [50/938], Loss: 0.6257\n",
            "Epoch [7], Batch [60/938], Loss: 0.7282\n",
            "Epoch [7], Batch [70/938], Loss: 0.4113\n",
            "Epoch [7], Batch [80/938], Loss: 0.4438\n",
            "Epoch [7], Batch [90/938], Loss: 0.7424\n",
            "Epoch [7], Batch [100/938], Loss: 0.6492\n",
            "Epoch [7], Batch [110/938], Loss: 0.7101\n",
            "Epoch [7], Batch [120/938], Loss: 0.5922\n",
            "Epoch [7], Batch [130/938], Loss: 0.5725\n",
            "Epoch [7], Batch [140/938], Loss: 0.5008\n",
            "Epoch [7], Batch [150/938], Loss: 0.4586\n",
            "Epoch [7], Batch [160/938], Loss: 0.5172\n",
            "Epoch [7], Batch [170/938], Loss: 0.6263\n",
            "Epoch [7], Batch [180/938], Loss: 0.6195\n",
            "Epoch [7], Batch [190/938], Loss: 0.5278\n",
            "Epoch [7], Batch [200/938], Loss: 0.6394\n",
            "Epoch [7], Batch [210/938], Loss: 0.6219\n",
            "Epoch [7], Batch [220/938], Loss: 0.6939\n",
            "Epoch [7], Batch [230/938], Loss: 0.7072\n",
            "Epoch [7], Batch [240/938], Loss: 0.8255\n",
            "Epoch [7], Batch [250/938], Loss: 0.5935\n",
            "Epoch [7], Batch [260/938], Loss: 0.6145\n",
            "Epoch [7], Batch [270/938], Loss: 0.7057\n",
            "Epoch [7], Batch [280/938], Loss: 0.6458\n",
            "Epoch [7], Batch [290/938], Loss: 0.6480\n",
            "Epoch [7], Batch [300/938], Loss: 0.5555\n",
            "Epoch [7], Batch [310/938], Loss: 0.4316\n",
            "Epoch [7], Batch [320/938], Loss: 0.6387\n",
            "Epoch [7], Batch [330/938], Loss: 0.6118\n",
            "Epoch [7], Batch [340/938], Loss: 0.6798\n",
            "Epoch [7], Batch [350/938], Loss: 0.5771\n",
            "Epoch [7], Batch [360/938], Loss: 0.5814\n",
            "Epoch [7], Batch [370/938], Loss: 0.7949\n",
            "Epoch [7], Batch [380/938], Loss: 0.4488\n",
            "Epoch [7], Batch [390/938], Loss: 0.5231\n",
            "Epoch [7], Batch [400/938], Loss: 0.8872\n",
            "Epoch [7], Batch [410/938], Loss: 0.5989\n",
            "Epoch [7], Batch [420/938], Loss: 0.5551\n",
            "Epoch [7], Batch [430/938], Loss: 0.5079\n",
            "Epoch [7], Batch [440/938], Loss: 0.6317\n",
            "Epoch [7], Batch [450/938], Loss: 0.6004\n",
            "Epoch [7], Batch [460/938], Loss: 0.5413\n",
            "Epoch [7], Batch [470/938], Loss: 0.4210\n",
            "Epoch [7], Batch [480/938], Loss: 0.5104\n",
            "Epoch [7], Batch [490/938], Loss: 0.7125\n",
            "Epoch [7], Batch [500/938], Loss: 0.5656\n",
            "Epoch [7], Batch [510/938], Loss: 0.4989\n",
            "Epoch [7], Batch [520/938], Loss: 0.6309\n",
            "Epoch [7], Batch [530/938], Loss: 0.5501\n",
            "Epoch [7], Batch [540/938], Loss: 0.5284\n",
            "Epoch [7], Batch [550/938], Loss: 0.5734\n",
            "Epoch [7], Batch [560/938], Loss: 0.5073\n",
            "Epoch [7], Batch [570/938], Loss: 0.5228\n",
            "Epoch [7], Batch [580/938], Loss: 0.4611\n",
            "Epoch [7], Batch [590/938], Loss: 0.5139\n",
            "Epoch [7], Batch [600/938], Loss: 0.4692\n",
            "Epoch [7], Batch [610/938], Loss: 0.5586\n",
            "Epoch [7], Batch [620/938], Loss: 0.6628\n",
            "Epoch [7], Batch [630/938], Loss: 0.3204\n",
            "Epoch [7], Batch [640/938], Loss: 0.5565\n",
            "Epoch [7], Batch [650/938], Loss: 0.5398\n",
            "Epoch [7], Batch [660/938], Loss: 0.6380\n",
            "Epoch [7], Batch [670/938], Loss: 0.5409\n",
            "Epoch [7], Batch [680/938], Loss: 0.7803\n",
            "Epoch [7], Batch [690/938], Loss: 0.5403\n",
            "Epoch [7], Batch [700/938], Loss: 0.6407\n",
            "Epoch [7], Batch [710/938], Loss: 0.6905\n",
            "Epoch [7], Batch [720/938], Loss: 0.6307\n",
            "Epoch [7], Batch [730/938], Loss: 0.4917\n",
            "Epoch [7], Batch [740/938], Loss: 0.5774\n",
            "Epoch [7], Batch [750/938], Loss: 0.5567\n",
            "Epoch [7], Batch [760/938], Loss: 0.5700\n",
            "Epoch [7], Batch [770/938], Loss: 0.4237\n",
            "Epoch [7], Batch [780/938], Loss: 0.5040\n",
            "Epoch [7], Batch [790/938], Loss: 0.6773\n",
            "Epoch [7], Batch [800/938], Loss: 0.4477\n",
            "Epoch [7], Batch [810/938], Loss: 0.5020\n",
            "Epoch [7], Batch [820/938], Loss: 0.5956\n",
            "Epoch [7], Batch [830/938], Loss: 0.5772\n",
            "Epoch [7], Batch [840/938], Loss: 0.4788\n",
            "Epoch [7], Batch [850/938], Loss: 0.7402\n",
            "Epoch [7], Batch [860/938], Loss: 0.5827\n",
            "Epoch [7], Batch [870/938], Loss: 0.6650\n",
            "Epoch [7], Batch [880/938], Loss: 0.7516\n",
            "Epoch [7], Batch [890/938], Loss: 0.5640\n",
            "Epoch [7], Batch [900/938], Loss: 0.4588\n",
            "Epoch [7], Batch [910/938], Loss: 0.6189\n",
            "Epoch [7], Batch [920/938], Loss: 0.4972\n",
            "Epoch [7], Batch [930/938], Loss: 0.5023\n",
            "Epoch [7], Batch [938/938], Loss: 0.7865\n",
            "Accuracy of train set: 79.22%\n",
            "Epoch [7], Batch [1/157], test Loss: 0.5090\n",
            "Epoch [7], Batch [2/157], test Loss: 0.6207\n",
            "Epoch [7], Batch [3/157], test Loss: 0.8285\n",
            "Epoch [7], Batch [4/157], test Loss: 0.6582\n",
            "Epoch [7], Batch [5/157], test Loss: 0.6891\n",
            "Epoch [7], Batch [6/157], test Loss: 0.6974\n",
            "Epoch [7], Batch [7/157], test Loss: 0.5273\n",
            "Epoch [7], Batch [8/157], test Loss: 0.6006\n",
            "Epoch [7], Batch [9/157], test Loss: 0.7281\n",
            "Epoch [7], Batch [10/157], test Loss: 0.7553\n",
            "Epoch [7], Batch [11/157], test Loss: 0.7431\n",
            "Epoch [7], Batch [12/157], test Loss: 0.4503\n",
            "Epoch [7], Batch [13/157], test Loss: 0.7568\n",
            "Epoch [7], Batch [14/157], test Loss: 0.7212\n",
            "Epoch [7], Batch [15/157], test Loss: 0.6361\n",
            "Epoch [7], Batch [16/157], test Loss: 0.6055\n",
            "Epoch [7], Batch [17/157], test Loss: 0.8256\n",
            "Epoch [7], Batch [18/157], test Loss: 0.6923\n",
            "Epoch [7], Batch [19/157], test Loss: 0.5509\n",
            "Epoch [7], Batch [20/157], test Loss: 0.4361\n",
            "Epoch [7], Batch [21/157], test Loss: 0.6546\n",
            "Epoch [7], Batch [22/157], test Loss: 0.5596\n",
            "Epoch [7], Batch [23/157], test Loss: 0.5787\n",
            "Epoch [7], Batch [24/157], test Loss: 0.6049\n",
            "Epoch [7], Batch [25/157], test Loss: 0.3912\n",
            "Epoch [7], Batch [26/157], test Loss: 0.6866\n",
            "Epoch [7], Batch [27/157], test Loss: 0.4721\n",
            "Epoch [7], Batch [28/157], test Loss: 0.5477\n",
            "Epoch [7], Batch [29/157], test Loss: 0.7400\n",
            "Epoch [7], Batch [30/157], test Loss: 0.5284\n",
            "Epoch [7], Batch [31/157], test Loss: 0.5248\n",
            "Epoch [7], Batch [32/157], test Loss: 0.4735\n",
            "Epoch [7], Batch [33/157], test Loss: 0.3945\n",
            "Epoch [7], Batch [34/157], test Loss: 0.7644\n",
            "Epoch [7], Batch [35/157], test Loss: 0.6505\n",
            "Epoch [7], Batch [36/157], test Loss: 0.7301\n",
            "Epoch [7], Batch [37/157], test Loss: 0.5826\n",
            "Epoch [7], Batch [38/157], test Loss: 0.6393\n",
            "Epoch [7], Batch [39/157], test Loss: 0.6076\n",
            "Epoch [7], Batch [40/157], test Loss: 0.6349\n",
            "Epoch [7], Batch [41/157], test Loss: 0.6482\n",
            "Epoch [7], Batch [42/157], test Loss: 0.5135\n",
            "Epoch [7], Batch [43/157], test Loss: 0.7079\n",
            "Epoch [7], Batch [44/157], test Loss: 0.6313\n",
            "Epoch [7], Batch [45/157], test Loss: 0.5235\n",
            "Epoch [7], Batch [46/157], test Loss: 0.3606\n",
            "Epoch [7], Batch [47/157], test Loss: 0.5394\n",
            "Epoch [7], Batch [48/157], test Loss: 0.9146\n",
            "Epoch [7], Batch [49/157], test Loss: 0.5547\n",
            "Epoch [7], Batch [50/157], test Loss: 0.7651\n",
            "Epoch [7], Batch [51/157], test Loss: 0.6188\n",
            "Epoch [7], Batch [52/157], test Loss: 0.6105\n",
            "Epoch [7], Batch [53/157], test Loss: 0.6547\n",
            "Epoch [7], Batch [54/157], test Loss: 0.8610\n",
            "Epoch [7], Batch [55/157], test Loss: 0.5699\n",
            "Epoch [7], Batch [56/157], test Loss: 0.4860\n",
            "Epoch [7], Batch [57/157], test Loss: 0.4910\n",
            "Epoch [7], Batch [58/157], test Loss: 0.5358\n",
            "Epoch [7], Batch [59/157], test Loss: 0.5309\n",
            "Epoch [7], Batch [60/157], test Loss: 0.4658\n",
            "Epoch [7], Batch [61/157], test Loss: 0.7568\n",
            "Epoch [7], Batch [62/157], test Loss: 0.5243\n",
            "Epoch [7], Batch [63/157], test Loss: 0.5573\n",
            "Epoch [7], Batch [64/157], test Loss: 0.6357\n",
            "Epoch [7], Batch [65/157], test Loss: 0.7451\n",
            "Epoch [7], Batch [66/157], test Loss: 0.5891\n",
            "Epoch [7], Batch [67/157], test Loss: 0.4915\n",
            "Epoch [7], Batch [68/157], test Loss: 0.6144\n",
            "Epoch [7], Batch [69/157], test Loss: 0.4193\n",
            "Epoch [7], Batch [70/157], test Loss: 0.5364\n",
            "Epoch [7], Batch [71/157], test Loss: 0.5381\n",
            "Epoch [7], Batch [72/157], test Loss: 0.5043\n",
            "Epoch [7], Batch [73/157], test Loss: 0.7068\n",
            "Epoch [7], Batch [74/157], test Loss: 0.5330\n",
            "Epoch [7], Batch [75/157], test Loss: 0.5929\n",
            "Epoch [7], Batch [76/157], test Loss: 0.5339\n",
            "Epoch [7], Batch [77/157], test Loss: 0.6117\n",
            "Epoch [7], Batch [78/157], test Loss: 0.5074\n",
            "Epoch [7], Batch [79/157], test Loss: 0.6276\n",
            "Epoch [7], Batch [80/157], test Loss: 0.6238\n",
            "Epoch [7], Batch [81/157], test Loss: 0.5551\n",
            "Epoch [7], Batch [82/157], test Loss: 0.5747\n",
            "Epoch [7], Batch [83/157], test Loss: 0.4638\n",
            "Epoch [7], Batch [84/157], test Loss: 0.3575\n",
            "Epoch [7], Batch [85/157], test Loss: 0.5027\n",
            "Epoch [7], Batch [86/157], test Loss: 0.4437\n",
            "Epoch [7], Batch [87/157], test Loss: 0.6237\n",
            "Epoch [7], Batch [88/157], test Loss: 0.6028\n",
            "Epoch [7], Batch [89/157], test Loss: 0.6262\n",
            "Epoch [7], Batch [90/157], test Loss: 0.7889\n",
            "Epoch [7], Batch [91/157], test Loss: 0.6295\n",
            "Epoch [7], Batch [92/157], test Loss: 0.6068\n",
            "Epoch [7], Batch [93/157], test Loss: 0.4624\n",
            "Epoch [7], Batch [94/157], test Loss: 0.6170\n",
            "Epoch [7], Batch [95/157], test Loss: 0.5404\n",
            "Epoch [7], Batch [96/157], test Loss: 0.7418\n",
            "Epoch [7], Batch [97/157], test Loss: 0.5556\n",
            "Epoch [7], Batch [98/157], test Loss: 0.5095\n",
            "Epoch [7], Batch [99/157], test Loss: 0.3721\n",
            "Epoch [7], Batch [100/157], test Loss: 0.4039\n",
            "Epoch [7], Batch [101/157], test Loss: 0.5214\n",
            "Epoch [7], Batch [102/157], test Loss: 0.5173\n",
            "Epoch [7], Batch [103/157], test Loss: 0.6155\n",
            "Epoch [7], Batch [104/157], test Loss: 0.5198\n",
            "Epoch [7], Batch [105/157], test Loss: 0.5170\n",
            "Epoch [7], Batch [106/157], test Loss: 0.6697\n",
            "Epoch [7], Batch [107/157], test Loss: 0.4950\n",
            "Epoch [7], Batch [108/157], test Loss: 0.6515\n",
            "Epoch [7], Batch [109/157], test Loss: 0.4269\n",
            "Epoch [7], Batch [110/157], test Loss: 0.4903\n",
            "Epoch [7], Batch [111/157], test Loss: 0.6409\n",
            "Epoch [7], Batch [112/157], test Loss: 0.5423\n",
            "Epoch [7], Batch [113/157], test Loss: 0.5180\n",
            "Epoch [7], Batch [114/157], test Loss: 0.6495\n",
            "Epoch [7], Batch [115/157], test Loss: 0.6319\n",
            "Epoch [7], Batch [116/157], test Loss: 0.4617\n",
            "Epoch [7], Batch [117/157], test Loss: 0.5879\n",
            "Epoch [7], Batch [118/157], test Loss: 0.6404\n",
            "Epoch [7], Batch [119/157], test Loss: 0.5303\n",
            "Epoch [7], Batch [120/157], test Loss: 0.5928\n",
            "Epoch [7], Batch [121/157], test Loss: 0.6228\n",
            "Epoch [7], Batch [122/157], test Loss: 0.6819\n",
            "Epoch [7], Batch [123/157], test Loss: 0.5865\n",
            "Epoch [7], Batch [124/157], test Loss: 0.5882\n",
            "Epoch [7], Batch [125/157], test Loss: 0.6105\n",
            "Epoch [7], Batch [126/157], test Loss: 0.7191\n",
            "Epoch [7], Batch [127/157], test Loss: 0.5560\n",
            "Epoch [7], Batch [128/157], test Loss: 0.5566\n",
            "Epoch [7], Batch [129/157], test Loss: 0.5445\n",
            "Epoch [7], Batch [130/157], test Loss: 1.2193\n",
            "Epoch [7], Batch [131/157], test Loss: 0.6203\n",
            "Epoch [7], Batch [132/157], test Loss: 0.4809\n",
            "Epoch [7], Batch [133/157], test Loss: 0.6039\n",
            "Epoch [7], Batch [134/157], test Loss: 0.5472\n",
            "Epoch [7], Batch [135/157], test Loss: 0.7391\n",
            "Epoch [7], Batch [136/157], test Loss: 0.6432\n",
            "Epoch [7], Batch [137/157], test Loss: 0.6588\n",
            "Epoch [7], Batch [138/157], test Loss: 0.6340\n",
            "Epoch [7], Batch [139/157], test Loss: 0.6190\n",
            "Epoch [7], Batch [140/157], test Loss: 0.5681\n",
            "Epoch [7], Batch [141/157], test Loss: 0.7584\n",
            "Epoch [7], Batch [142/157], test Loss: 0.7647\n",
            "Epoch [7], Batch [143/157], test Loss: 0.5029\n",
            "Epoch [7], Batch [144/157], test Loss: 0.4771\n",
            "Epoch [7], Batch [145/157], test Loss: 0.5286\n",
            "Epoch [7], Batch [146/157], test Loss: 0.8706\n",
            "Epoch [7], Batch [147/157], test Loss: 1.1138\n",
            "Epoch [7], Batch [148/157], test Loss: 0.6583\n",
            "Epoch [7], Batch [149/157], test Loss: 0.6554\n",
            "Epoch [7], Batch [150/157], test Loss: 0.5951\n",
            "Epoch [7], Batch [151/157], test Loss: 0.6364\n",
            "Epoch [7], Batch [152/157], test Loss: 0.7043\n",
            "Epoch [7], Batch [153/157], test Loss: 0.5695\n",
            "Epoch [7], Batch [154/157], test Loss: 0.6906\n",
            "Epoch [7], Batch [155/157], test Loss: 0.6760\n",
            "Epoch [7], Batch [156/157], test Loss: 0.4648\n",
            "Epoch [7], Batch [157/157], test Loss: 0.2702\n",
            "Accuracy of test set: 78.26%\n",
            "Epoch [8/25] - Train Loss: 0.5874, Train Accuracy: 79.22% - Test Loss: 0.6011, Test Accuracy: 78.26%\n",
            "Epoch [8], Batch [10/938], Loss: 0.5175\n",
            "Epoch [8], Batch [20/938], Loss: 0.5700\n",
            "Epoch [8], Batch [30/938], Loss: 0.6517\n",
            "Epoch [8], Batch [40/938], Loss: 0.4071\n",
            "Epoch [8], Batch [50/938], Loss: 0.6369\n",
            "Epoch [8], Batch [60/938], Loss: 0.7378\n",
            "Epoch [8], Batch [70/938], Loss: 0.4044\n",
            "Epoch [8], Batch [80/938], Loss: 0.5305\n",
            "Epoch [8], Batch [90/938], Loss: 0.5899\n",
            "Epoch [8], Batch [100/938], Loss: 0.6014\n",
            "Epoch [8], Batch [110/938], Loss: 0.4425\n",
            "Epoch [8], Batch [120/938], Loss: 0.8824\n",
            "Epoch [8], Batch [130/938], Loss: 0.4858\n",
            "Epoch [8], Batch [140/938], Loss: 0.5268\n",
            "Epoch [8], Batch [150/938], Loss: 0.6986\n",
            "Epoch [8], Batch [160/938], Loss: 0.7261\n",
            "Epoch [8], Batch [170/938], Loss: 0.3881\n",
            "Epoch [8], Batch [180/938], Loss: 0.6091\n",
            "Epoch [8], Batch [190/938], Loss: 0.5192\n",
            "Epoch [8], Batch [200/938], Loss: 0.3873\n",
            "Epoch [8], Batch [210/938], Loss: 0.7190\n",
            "Epoch [8], Batch [220/938], Loss: 0.3984\n",
            "Epoch [8], Batch [230/938], Loss: 0.6689\n",
            "Epoch [8], Batch [240/938], Loss: 0.5114\n",
            "Epoch [8], Batch [250/938], Loss: 0.5297\n",
            "Epoch [8], Batch [260/938], Loss: 0.5617\n",
            "Epoch [8], Batch [270/938], Loss: 0.5702\n",
            "Epoch [8], Batch [280/938], Loss: 0.5231\n",
            "Epoch [8], Batch [290/938], Loss: 0.5344\n",
            "Epoch [8], Batch [300/938], Loss: 0.6084\n",
            "Epoch [8], Batch [310/938], Loss: 0.6078\n",
            "Epoch [8], Batch [320/938], Loss: 0.7110\n",
            "Epoch [8], Batch [330/938], Loss: 0.8389\n",
            "Epoch [8], Batch [340/938], Loss: 0.6141\n",
            "Epoch [8], Batch [350/938], Loss: 0.4783\n",
            "Epoch [8], Batch [360/938], Loss: 0.3887\n",
            "Epoch [8], Batch [370/938], Loss: 0.5454\n",
            "Epoch [8], Batch [380/938], Loss: 0.7588\n",
            "Epoch [8], Batch [390/938], Loss: 0.6497\n",
            "Epoch [8], Batch [400/938], Loss: 0.4040\n",
            "Epoch [8], Batch [410/938], Loss: 0.7326\n",
            "Epoch [8], Batch [420/938], Loss: 0.5201\n",
            "Epoch [8], Batch [430/938], Loss: 0.7573\n",
            "Epoch [8], Batch [440/938], Loss: 0.6532\n",
            "Epoch [8], Batch [450/938], Loss: 0.7645\n",
            "Epoch [8], Batch [460/938], Loss: 0.5662\n",
            "Epoch [8], Batch [470/938], Loss: 0.3620\n",
            "Epoch [8], Batch [480/938], Loss: 0.7612\n",
            "Epoch [8], Batch [490/938], Loss: 0.7234\n",
            "Epoch [8], Batch [500/938], Loss: 0.4317\n",
            "Epoch [8], Batch [510/938], Loss: 0.5279\n",
            "Epoch [8], Batch [520/938], Loss: 0.4455\n",
            "Epoch [8], Batch [530/938], Loss: 0.5336\n",
            "Epoch [8], Batch [540/938], Loss: 0.4054\n",
            "Epoch [8], Batch [550/938], Loss: 0.4491\n",
            "Epoch [8], Batch [560/938], Loss: 0.4847\n",
            "Epoch [8], Batch [570/938], Loss: 0.5811\n",
            "Epoch [8], Batch [580/938], Loss: 0.4639\n",
            "Epoch [8], Batch [590/938], Loss: 0.5203\n",
            "Epoch [8], Batch [600/938], Loss: 0.3973\n",
            "Epoch [8], Batch [610/938], Loss: 0.8082\n",
            "Epoch [8], Batch [620/938], Loss: 0.4370\n",
            "Epoch [8], Batch [630/938], Loss: 0.6239\n",
            "Epoch [8], Batch [640/938], Loss: 0.5163\n",
            "Epoch [8], Batch [650/938], Loss: 0.5631\n",
            "Epoch [8], Batch [660/938], Loss: 0.6041\n",
            "Epoch [8], Batch [670/938], Loss: 0.4903\n",
            "Epoch [8], Batch [680/938], Loss: 0.4111\n",
            "Epoch [8], Batch [690/938], Loss: 0.5928\n",
            "Epoch [8], Batch [700/938], Loss: 0.4938\n",
            "Epoch [8], Batch [710/938], Loss: 0.3854\n",
            "Epoch [8], Batch [720/938], Loss: 0.4100\n",
            "Epoch [8], Batch [730/938], Loss: 0.5583\n",
            "Epoch [8], Batch [740/938], Loss: 0.5362\n",
            "Epoch [8], Batch [750/938], Loss: 0.5401\n",
            "Epoch [8], Batch [760/938], Loss: 0.4836\n",
            "Epoch [8], Batch [770/938], Loss: 0.5361\n",
            "Epoch [8], Batch [780/938], Loss: 0.4129\n",
            "Epoch [8], Batch [790/938], Loss: 0.6197\n",
            "Epoch [8], Batch [800/938], Loss: 0.4550\n",
            "Epoch [8], Batch [810/938], Loss: 0.6902\n",
            "Epoch [8], Batch [820/938], Loss: 0.4926\n",
            "Epoch [8], Batch [830/938], Loss: 0.7352\n",
            "Epoch [8], Batch [840/938], Loss: 0.8674\n",
            "Epoch [8], Batch [850/938], Loss: 0.4925\n",
            "Epoch [8], Batch [860/938], Loss: 0.5657\n",
            "Epoch [8], Batch [870/938], Loss: 0.5220\n",
            "Epoch [8], Batch [880/938], Loss: 0.6109\n",
            "Epoch [8], Batch [890/938], Loss: 0.5517\n",
            "Epoch [8], Batch [900/938], Loss: 0.5067\n",
            "Epoch [8], Batch [910/938], Loss: 0.6451\n",
            "Epoch [8], Batch [920/938], Loss: 0.5217\n",
            "Epoch [8], Batch [930/938], Loss: 0.5856\n",
            "Epoch [8], Batch [938/938], Loss: 0.5517\n",
            "Accuracy of train set: 80.40%\n",
            "Epoch [8], Batch [1/157], test Loss: 0.4369\n",
            "Epoch [8], Batch [2/157], test Loss: 0.5844\n",
            "Epoch [8], Batch [3/157], test Loss: 0.5582\n",
            "Epoch [8], Batch [4/157], test Loss: 0.5933\n",
            "Epoch [8], Batch [5/157], test Loss: 0.8085\n",
            "Epoch [8], Batch [6/157], test Loss: 0.6709\n",
            "Epoch [8], Batch [7/157], test Loss: 0.8737\n",
            "Epoch [8], Batch [8/157], test Loss: 0.7602\n",
            "Epoch [8], Batch [9/157], test Loss: 0.5458\n",
            "Epoch [8], Batch [10/157], test Loss: 0.5529\n",
            "Epoch [8], Batch [11/157], test Loss: 0.6859\n",
            "Epoch [8], Batch [12/157], test Loss: 0.5846\n",
            "Epoch [8], Batch [13/157], test Loss: 0.3467\n",
            "Epoch [8], Batch [14/157], test Loss: 0.6406\n",
            "Epoch [8], Batch [15/157], test Loss: 0.3649\n",
            "Epoch [8], Batch [16/157], test Loss: 0.4926\n",
            "Epoch [8], Batch [17/157], test Loss: 0.4228\n",
            "Epoch [8], Batch [18/157], test Loss: 0.5701\n",
            "Epoch [8], Batch [19/157], test Loss: 0.7655\n",
            "Epoch [8], Batch [20/157], test Loss: 0.7116\n",
            "Epoch [8], Batch [21/157], test Loss: 0.4876\n",
            "Epoch [8], Batch [22/157], test Loss: 0.6642\n",
            "Epoch [8], Batch [23/157], test Loss: 0.6929\n",
            "Epoch [8], Batch [24/157], test Loss: 0.7925\n",
            "Epoch [8], Batch [25/157], test Loss: 0.4360\n",
            "Epoch [8], Batch [26/157], test Loss: 0.6129\n",
            "Epoch [8], Batch [27/157], test Loss: 0.6140\n",
            "Epoch [8], Batch [28/157], test Loss: 0.4807\n",
            "Epoch [8], Batch [29/157], test Loss: 0.6215\n",
            "Epoch [8], Batch [30/157], test Loss: 0.6422\n",
            "Epoch [8], Batch [31/157], test Loss: 0.5306\n",
            "Epoch [8], Batch [32/157], test Loss: 0.5311\n",
            "Epoch [8], Batch [33/157], test Loss: 0.6161\n",
            "Epoch [8], Batch [34/157], test Loss: 0.8290\n",
            "Epoch [8], Batch [35/157], test Loss: 0.5097\n",
            "Epoch [8], Batch [36/157], test Loss: 0.9450\n",
            "Epoch [8], Batch [37/157], test Loss: 0.6138\n",
            "Epoch [8], Batch [38/157], test Loss: 0.5124\n",
            "Epoch [8], Batch [39/157], test Loss: 0.6134\n",
            "Epoch [8], Batch [40/157], test Loss: 0.3142\n",
            "Epoch [8], Batch [41/157], test Loss: 0.6785\n",
            "Epoch [8], Batch [42/157], test Loss: 0.5950\n",
            "Epoch [8], Batch [43/157], test Loss: 0.6601\n",
            "Epoch [8], Batch [44/157], test Loss: 0.6088\n",
            "Epoch [8], Batch [45/157], test Loss: 0.5773\n",
            "Epoch [8], Batch [46/157], test Loss: 0.5654\n",
            "Epoch [8], Batch [47/157], test Loss: 0.5467\n",
            "Epoch [8], Batch [48/157], test Loss: 0.5571\n",
            "Epoch [8], Batch [49/157], test Loss: 0.5819\n",
            "Epoch [8], Batch [50/157], test Loss: 0.4577\n",
            "Epoch [8], Batch [51/157], test Loss: 0.5174\n",
            "Epoch [8], Batch [52/157], test Loss: 0.6627\n",
            "Epoch [8], Batch [53/157], test Loss: 0.5357\n",
            "Epoch [8], Batch [54/157], test Loss: 0.5249\n",
            "Epoch [8], Batch [55/157], test Loss: 0.5215\n",
            "Epoch [8], Batch [56/157], test Loss: 0.8059\n",
            "Epoch [8], Batch [57/157], test Loss: 0.4149\n",
            "Epoch [8], Batch [58/157], test Loss: 0.3668\n",
            "Epoch [8], Batch [59/157], test Loss: 0.5955\n",
            "Epoch [8], Batch [60/157], test Loss: 0.6349\n",
            "Epoch [8], Batch [61/157], test Loss: 0.6193\n",
            "Epoch [8], Batch [62/157], test Loss: 0.3481\n",
            "Epoch [8], Batch [63/157], test Loss: 0.5785\n",
            "Epoch [8], Batch [64/157], test Loss: 0.6590\n",
            "Epoch [8], Batch [65/157], test Loss: 0.4873\n",
            "Epoch [8], Batch [66/157], test Loss: 0.4764\n",
            "Epoch [8], Batch [67/157], test Loss: 0.6241\n",
            "Epoch [8], Batch [68/157], test Loss: 0.4492\n",
            "Epoch [8], Batch [69/157], test Loss: 0.6754\n",
            "Epoch [8], Batch [70/157], test Loss: 0.3753\n",
            "Epoch [8], Batch [71/157], test Loss: 0.5950\n",
            "Epoch [8], Batch [72/157], test Loss: 0.8476\n",
            "Epoch [8], Batch [73/157], test Loss: 0.6183\n",
            "Epoch [8], Batch [74/157], test Loss: 0.4930\n",
            "Epoch [8], Batch [75/157], test Loss: 0.5152\n",
            "Epoch [8], Batch [76/157], test Loss: 0.6140\n",
            "Epoch [8], Batch [77/157], test Loss: 0.5372\n",
            "Epoch [8], Batch [78/157], test Loss: 0.5635\n",
            "Epoch [8], Batch [79/157], test Loss: 0.6108\n",
            "Epoch [8], Batch [80/157], test Loss: 0.4573\n",
            "Epoch [8], Batch [81/157], test Loss: 0.5177\n",
            "Epoch [8], Batch [82/157], test Loss: 0.5756\n",
            "Epoch [8], Batch [83/157], test Loss: 0.4123\n",
            "Epoch [8], Batch [84/157], test Loss: 0.6131\n",
            "Epoch [8], Batch [85/157], test Loss: 0.7340\n",
            "Epoch [8], Batch [86/157], test Loss: 0.5656\n",
            "Epoch [8], Batch [87/157], test Loss: 0.5196\n",
            "Epoch [8], Batch [88/157], test Loss: 0.5235\n",
            "Epoch [8], Batch [89/157], test Loss: 0.4339\n",
            "Epoch [8], Batch [90/157], test Loss: 0.5669\n",
            "Epoch [8], Batch [91/157], test Loss: 0.6873\n",
            "Epoch [8], Batch [92/157], test Loss: 0.4469\n",
            "Epoch [8], Batch [93/157], test Loss: 0.5664\n",
            "Epoch [8], Batch [94/157], test Loss: 0.5114\n",
            "Epoch [8], Batch [95/157], test Loss: 0.4674\n",
            "Epoch [8], Batch [96/157], test Loss: 0.5840\n",
            "Epoch [8], Batch [97/157], test Loss: 0.6948\n",
            "Epoch [8], Batch [98/157], test Loss: 0.6222\n",
            "Epoch [8], Batch [99/157], test Loss: 0.6204\n",
            "Epoch [8], Batch [100/157], test Loss: 0.6497\n",
            "Epoch [8], Batch [101/157], test Loss: 0.7432\n",
            "Epoch [8], Batch [102/157], test Loss: 0.6278\n",
            "Epoch [8], Batch [103/157], test Loss: 0.6943\n",
            "Epoch [8], Batch [104/157], test Loss: 0.6274\n",
            "Epoch [8], Batch [105/157], test Loss: 0.6130\n",
            "Epoch [8], Batch [106/157], test Loss: 0.3657\n",
            "Epoch [8], Batch [107/157], test Loss: 0.6360\n",
            "Epoch [8], Batch [108/157], test Loss: 0.4788\n",
            "Epoch [8], Batch [109/157], test Loss: 0.6904\n",
            "Epoch [8], Batch [110/157], test Loss: 0.5766\n",
            "Epoch [8], Batch [111/157], test Loss: 0.4255\n",
            "Epoch [8], Batch [112/157], test Loss: 0.6624\n",
            "Epoch [8], Batch [113/157], test Loss: 0.5660\n",
            "Epoch [8], Batch [114/157], test Loss: 0.6449\n",
            "Epoch [8], Batch [115/157], test Loss: 0.7277\n",
            "Epoch [8], Batch [116/157], test Loss: 0.5497\n",
            "Epoch [8], Batch [117/157], test Loss: 0.7674\n",
            "Epoch [8], Batch [118/157], test Loss: 0.3774\n",
            "Epoch [8], Batch [119/157], test Loss: 0.4532\n",
            "Epoch [8], Batch [120/157], test Loss: 0.7104\n",
            "Epoch [8], Batch [121/157], test Loss: 0.6986\n",
            "Epoch [8], Batch [122/157], test Loss: 0.4409\n",
            "Epoch [8], Batch [123/157], test Loss: 0.6057\n",
            "Epoch [8], Batch [124/157], test Loss: 0.6635\n",
            "Epoch [8], Batch [125/157], test Loss: 0.4556\n",
            "Epoch [8], Batch [126/157], test Loss: 0.5552\n",
            "Epoch [8], Batch [127/157], test Loss: 0.6622\n",
            "Epoch [8], Batch [128/157], test Loss: 0.6583\n",
            "Epoch [8], Batch [129/157], test Loss: 0.5699\n",
            "Epoch [8], Batch [130/157], test Loss: 0.4786\n",
            "Epoch [8], Batch [131/157], test Loss: 0.5500\n",
            "Epoch [8], Batch [132/157], test Loss: 0.7232\n",
            "Epoch [8], Batch [133/157], test Loss: 0.6212\n",
            "Epoch [8], Batch [134/157], test Loss: 0.7898\n",
            "Epoch [8], Batch [135/157], test Loss: 0.7638\n",
            "Epoch [8], Batch [136/157], test Loss: 1.1096\n",
            "Epoch [8], Batch [137/157], test Loss: 0.6455\n",
            "Epoch [8], Batch [138/157], test Loss: 0.7147\n",
            "Epoch [8], Batch [139/157], test Loss: 0.6818\n",
            "Epoch [8], Batch [140/157], test Loss: 0.4565\n",
            "Epoch [8], Batch [141/157], test Loss: 0.5209\n",
            "Epoch [8], Batch [142/157], test Loss: 0.5693\n",
            "Epoch [8], Batch [143/157], test Loss: 0.4104\n",
            "Epoch [8], Batch [144/157], test Loss: 0.5243\n",
            "Epoch [8], Batch [145/157], test Loss: 0.4160\n",
            "Epoch [8], Batch [146/157], test Loss: 0.4782\n",
            "Epoch [8], Batch [147/157], test Loss: 0.5466\n",
            "Epoch [8], Batch [148/157], test Loss: 0.4907\n",
            "Epoch [8], Batch [149/157], test Loss: 0.4427\n",
            "Epoch [8], Batch [150/157], test Loss: 0.5611\n",
            "Epoch [8], Batch [151/157], test Loss: 0.4742\n",
            "Epoch [8], Batch [152/157], test Loss: 0.3769\n",
            "Epoch [8], Batch [153/157], test Loss: 0.6821\n",
            "Epoch [8], Batch [154/157], test Loss: 0.4426\n",
            "Epoch [8], Batch [155/157], test Loss: 0.6235\n",
            "Epoch [8], Batch [156/157], test Loss: 0.7308\n",
            "Epoch [8], Batch [157/157], test Loss: 0.5711\n",
            "Accuracy of test set: 79.60%\n",
            "Epoch [9/25] - Train Loss: 0.5619, Train Accuracy: 80.40% - Test Loss: 0.5831, Test Accuracy: 79.60%\n",
            "Epoch [9], Batch [10/938], Loss: 0.4865\n",
            "Epoch [9], Batch [20/938], Loss: 0.5272\n",
            "Epoch [9], Batch [30/938], Loss: 0.6515\n",
            "Epoch [9], Batch [40/938], Loss: 0.6206\n",
            "Epoch [9], Batch [50/938], Loss: 0.5947\n",
            "Epoch [9], Batch [60/938], Loss: 0.5438\n",
            "Epoch [9], Batch [70/938], Loss: 0.4417\n",
            "Epoch [9], Batch [80/938], Loss: 0.4981\n",
            "Epoch [9], Batch [90/938], Loss: 0.7045\n",
            "Epoch [9], Batch [100/938], Loss: 0.6440\n",
            "Epoch [9], Batch [110/938], Loss: 0.5021\n",
            "Epoch [9], Batch [120/938], Loss: 0.4735\n",
            "Epoch [9], Batch [130/938], Loss: 0.5317\n",
            "Epoch [9], Batch [140/938], Loss: 0.4039\n",
            "Epoch [9], Batch [150/938], Loss: 0.5652\n",
            "Epoch [9], Batch [160/938], Loss: 0.6519\n",
            "Epoch [9], Batch [170/938], Loss: 0.6248\n",
            "Epoch [9], Batch [180/938], Loss: 0.7012\n",
            "Epoch [9], Batch [190/938], Loss: 0.4353\n",
            "Epoch [9], Batch [200/938], Loss: 0.6937\n",
            "Epoch [9], Batch [210/938], Loss: 0.4630\n",
            "Epoch [9], Batch [220/938], Loss: 0.5886\n",
            "Epoch [9], Batch [230/938], Loss: 0.5651\n",
            "Epoch [9], Batch [240/938], Loss: 0.9159\n",
            "Epoch [9], Batch [250/938], Loss: 0.4910\n",
            "Epoch [9], Batch [260/938], Loss: 0.6063\n",
            "Epoch [9], Batch [270/938], Loss: 0.5164\n",
            "Epoch [9], Batch [280/938], Loss: 0.5777\n",
            "Epoch [9], Batch [290/938], Loss: 0.5765\n",
            "Epoch [9], Batch [300/938], Loss: 0.6834\n",
            "Epoch [9], Batch [310/938], Loss: 0.4536\n",
            "Epoch [9], Batch [320/938], Loss: 0.5485\n",
            "Epoch [9], Batch [330/938], Loss: 0.4131\n",
            "Epoch [9], Batch [340/938], Loss: 0.4071\n",
            "Epoch [9], Batch [350/938], Loss: 0.6835\n",
            "Epoch [9], Batch [360/938], Loss: 0.5428\n",
            "Epoch [9], Batch [370/938], Loss: 0.6409\n",
            "Epoch [9], Batch [380/938], Loss: 0.4017\n",
            "Epoch [9], Batch [390/938], Loss: 0.4715\n",
            "Epoch [9], Batch [400/938], Loss: 0.4336\n",
            "Epoch [9], Batch [410/938], Loss: 0.5267\n",
            "Epoch [9], Batch [420/938], Loss: 0.6025\n",
            "Epoch [9], Batch [430/938], Loss: 0.6448\n",
            "Epoch [9], Batch [440/938], Loss: 0.2984\n",
            "Epoch [9], Batch [450/938], Loss: 0.5467\n",
            "Epoch [9], Batch [460/938], Loss: 0.4064\n",
            "Epoch [9], Batch [470/938], Loss: 0.3558\n",
            "Epoch [9], Batch [480/938], Loss: 0.4169\n",
            "Epoch [9], Batch [490/938], Loss: 0.4436\n",
            "Epoch [9], Batch [500/938], Loss: 0.4857\n",
            "Epoch [9], Batch [510/938], Loss: 0.3824\n",
            "Epoch [9], Batch [520/938], Loss: 0.6970\n",
            "Epoch [9], Batch [530/938], Loss: 0.4650\n",
            "Epoch [9], Batch [540/938], Loss: 0.4236\n",
            "Epoch [9], Batch [550/938], Loss: 0.5772\n",
            "Epoch [9], Batch [560/938], Loss: 0.5732\n",
            "Epoch [9], Batch [570/938], Loss: 0.3698\n",
            "Epoch [9], Batch [580/938], Loss: 0.4427\n",
            "Epoch [9], Batch [590/938], Loss: 0.4702\n",
            "Epoch [9], Batch [600/938], Loss: 0.4344\n",
            "Epoch [9], Batch [610/938], Loss: 0.4303\n",
            "Epoch [9], Batch [620/938], Loss: 0.5386\n",
            "Epoch [9], Batch [630/938], Loss: 0.3972\n",
            "Epoch [9], Batch [640/938], Loss: 0.5633\n",
            "Epoch [9], Batch [650/938], Loss: 0.5878\n",
            "Epoch [9], Batch [660/938], Loss: 0.4839\n",
            "Epoch [9], Batch [670/938], Loss: 0.4879\n",
            "Epoch [9], Batch [680/938], Loss: 0.5782\n",
            "Epoch [9], Batch [690/938], Loss: 0.4911\n",
            "Epoch [9], Batch [700/938], Loss: 0.6820\n",
            "Epoch [9], Batch [710/938], Loss: 0.4898\n",
            "Epoch [9], Batch [720/938], Loss: 0.4983\n",
            "Epoch [9], Batch [730/938], Loss: 0.5495\n",
            "Epoch [9], Batch [740/938], Loss: 0.4015\n",
            "Epoch [9], Batch [750/938], Loss: 0.6476\n",
            "Epoch [9], Batch [760/938], Loss: 0.5062\n",
            "Epoch [9], Batch [770/938], Loss: 0.5324\n",
            "Epoch [9], Batch [780/938], Loss: 0.5502\n",
            "Epoch [9], Batch [790/938], Loss: 0.4492\n",
            "Epoch [9], Batch [800/938], Loss: 0.5128\n",
            "Epoch [9], Batch [810/938], Loss: 0.5587\n",
            "Epoch [9], Batch [820/938], Loss: 0.4808\n",
            "Epoch [9], Batch [830/938], Loss: 0.7381\n",
            "Epoch [9], Batch [840/938], Loss: 0.6012\n",
            "Epoch [9], Batch [850/938], Loss: 0.4912\n",
            "Epoch [9], Batch [860/938], Loss: 0.4127\n",
            "Epoch [9], Batch [870/938], Loss: 0.5934\n",
            "Epoch [9], Batch [880/938], Loss: 0.3878\n",
            "Epoch [9], Batch [890/938], Loss: 0.5104\n",
            "Epoch [9], Batch [900/938], Loss: 0.6167\n",
            "Epoch [9], Batch [910/938], Loss: 0.4667\n",
            "Epoch [9], Batch [920/938], Loss: 0.7246\n",
            "Epoch [9], Batch [930/938], Loss: 0.4802\n",
            "Epoch [9], Batch [938/938], Loss: 0.3848\n",
            "Accuracy of train set: 81.27%\n",
            "Epoch [9], Batch [1/157], test Loss: 0.5946\n",
            "Epoch [9], Batch [2/157], test Loss: 0.3670\n",
            "Epoch [9], Batch [3/157], test Loss: 0.5678\n",
            "Epoch [9], Batch [4/157], test Loss: 0.8117\n",
            "Epoch [9], Batch [5/157], test Loss: 0.5103\n",
            "Epoch [9], Batch [6/157], test Loss: 0.4228\n",
            "Epoch [9], Batch [7/157], test Loss: 0.5718\n",
            "Epoch [9], Batch [8/157], test Loss: 0.3819\n",
            "Epoch [9], Batch [9/157], test Loss: 0.5918\n",
            "Epoch [9], Batch [10/157], test Loss: 0.5995\n",
            "Epoch [9], Batch [11/157], test Loss: 0.6693\n",
            "Epoch [9], Batch [12/157], test Loss: 0.5512\n",
            "Epoch [9], Batch [13/157], test Loss: 0.4182\n",
            "Epoch [9], Batch [14/157], test Loss: 0.7534\n",
            "Epoch [9], Batch [15/157], test Loss: 0.6190\n",
            "Epoch [9], Batch [16/157], test Loss: 0.7301\n",
            "Epoch [9], Batch [17/157], test Loss: 0.4708\n",
            "Epoch [9], Batch [18/157], test Loss: 0.7624\n",
            "Epoch [9], Batch [19/157], test Loss: 0.6518\n",
            "Epoch [9], Batch [20/157], test Loss: 0.3650\n",
            "Epoch [9], Batch [21/157], test Loss: 0.6510\n",
            "Epoch [9], Batch [22/157], test Loss: 0.3774\n",
            "Epoch [9], Batch [23/157], test Loss: 0.7551\n",
            "Epoch [9], Batch [24/157], test Loss: 0.7267\n",
            "Epoch [9], Batch [25/157], test Loss: 0.5240\n",
            "Epoch [9], Batch [26/157], test Loss: 0.6390\n",
            "Epoch [9], Batch [27/157], test Loss: 0.4281\n",
            "Epoch [9], Batch [28/157], test Loss: 0.5433\n",
            "Epoch [9], Batch [29/157], test Loss: 0.4855\n",
            "Epoch [9], Batch [30/157], test Loss: 0.4793\n",
            "Epoch [9], Batch [31/157], test Loss: 0.4452\n",
            "Epoch [9], Batch [32/157], test Loss: 0.3692\n",
            "Epoch [9], Batch [33/157], test Loss: 0.6678\n",
            "Epoch [9], Batch [34/157], test Loss: 0.6691\n",
            "Epoch [9], Batch [35/157], test Loss: 0.6393\n",
            "Epoch [9], Batch [36/157], test Loss: 0.6290\n",
            "Epoch [9], Batch [37/157], test Loss: 0.5980\n",
            "Epoch [9], Batch [38/157], test Loss: 0.6877\n",
            "Epoch [9], Batch [39/157], test Loss: 0.7683\n",
            "Epoch [9], Batch [40/157], test Loss: 0.4616\n",
            "Epoch [9], Batch [41/157], test Loss: 0.5539\n",
            "Epoch [9], Batch [42/157], test Loss: 0.6861\n",
            "Epoch [9], Batch [43/157], test Loss: 0.3930\n",
            "Epoch [9], Batch [44/157], test Loss: 0.4426\n",
            "Epoch [9], Batch [45/157], test Loss: 0.4838\n",
            "Epoch [9], Batch [46/157], test Loss: 0.3909\n",
            "Epoch [9], Batch [47/157], test Loss: 0.5460\n",
            "Epoch [9], Batch [48/157], test Loss: 0.4554\n",
            "Epoch [9], Batch [49/157], test Loss: 0.5905\n",
            "Epoch [9], Batch [50/157], test Loss: 0.6186\n",
            "Epoch [9], Batch [51/157], test Loss: 0.4348\n",
            "Epoch [9], Batch [52/157], test Loss: 0.5582\n",
            "Epoch [9], Batch [53/157], test Loss: 0.4572\n",
            "Epoch [9], Batch [54/157], test Loss: 0.7081\n",
            "Epoch [9], Batch [55/157], test Loss: 0.6963\n",
            "Epoch [9], Batch [56/157], test Loss: 0.6484\n",
            "Epoch [9], Batch [57/157], test Loss: 0.5825\n",
            "Epoch [9], Batch [58/157], test Loss: 0.5286\n",
            "Epoch [9], Batch [59/157], test Loss: 0.5711\n",
            "Epoch [9], Batch [60/157], test Loss: 0.3675\n",
            "Epoch [9], Batch [61/157], test Loss: 0.4459\n",
            "Epoch [9], Batch [62/157], test Loss: 0.5828\n",
            "Epoch [9], Batch [63/157], test Loss: 0.7734\n",
            "Epoch [9], Batch [64/157], test Loss: 0.5837\n",
            "Epoch [9], Batch [65/157], test Loss: 0.7499\n",
            "Epoch [9], Batch [66/157], test Loss: 0.3271\n",
            "Epoch [9], Batch [67/157], test Loss: 0.5088\n",
            "Epoch [9], Batch [68/157], test Loss: 0.5729\n",
            "Epoch [9], Batch [69/157], test Loss: 0.7150\n",
            "Epoch [9], Batch [70/157], test Loss: 0.5820\n",
            "Epoch [9], Batch [71/157], test Loss: 0.7435\n",
            "Epoch [9], Batch [72/157], test Loss: 0.5768\n",
            "Epoch [9], Batch [73/157], test Loss: 0.6031\n",
            "Epoch [9], Batch [74/157], test Loss: 0.6460\n",
            "Epoch [9], Batch [75/157], test Loss: 0.5180\n",
            "Epoch [9], Batch [76/157], test Loss: 0.4750\n",
            "Epoch [9], Batch [77/157], test Loss: 0.4610\n",
            "Epoch [9], Batch [78/157], test Loss: 0.4023\n",
            "Epoch [9], Batch [79/157], test Loss: 0.4539\n",
            "Epoch [9], Batch [80/157], test Loss: 0.4700\n",
            "Epoch [9], Batch [81/157], test Loss: 0.4095\n",
            "Epoch [9], Batch [82/157], test Loss: 0.5380\n",
            "Epoch [9], Batch [83/157], test Loss: 0.4506\n",
            "Epoch [9], Batch [84/157], test Loss: 0.7594\n",
            "Epoch [9], Batch [85/157], test Loss: 0.4478\n",
            "Epoch [9], Batch [86/157], test Loss: 0.5018\n",
            "Epoch [9], Batch [87/157], test Loss: 0.6890\n",
            "Epoch [9], Batch [88/157], test Loss: 0.4970\n",
            "Epoch [9], Batch [89/157], test Loss: 0.4560\n",
            "Epoch [9], Batch [90/157], test Loss: 0.5063\n",
            "Epoch [9], Batch [91/157], test Loss: 0.6051\n",
            "Epoch [9], Batch [92/157], test Loss: 0.4924\n",
            "Epoch [9], Batch [93/157], test Loss: 0.3816\n",
            "Epoch [9], Batch [94/157], test Loss: 0.5478\n",
            "Epoch [9], Batch [95/157], test Loss: 0.5078\n",
            "Epoch [9], Batch [96/157], test Loss: 0.5774\n",
            "Epoch [9], Batch [97/157], test Loss: 0.5881\n",
            "Epoch [9], Batch [98/157], test Loss: 0.7068\n",
            "Epoch [9], Batch [99/157], test Loss: 0.5036\n",
            "Epoch [9], Batch [100/157], test Loss: 0.6053\n",
            "Epoch [9], Batch [101/157], test Loss: 0.3898\n",
            "Epoch [9], Batch [102/157], test Loss: 0.6682\n",
            "Epoch [9], Batch [103/157], test Loss: 0.7804\n",
            "Epoch [9], Batch [104/157], test Loss: 0.5227\n",
            "Epoch [9], Batch [105/157], test Loss: 0.4616\n",
            "Epoch [9], Batch [106/157], test Loss: 0.5083\n",
            "Epoch [9], Batch [107/157], test Loss: 0.4647\n",
            "Epoch [9], Batch [108/157], test Loss: 0.5408\n",
            "Epoch [9], Batch [109/157], test Loss: 0.5815\n",
            "Epoch [9], Batch [110/157], test Loss: 0.5804\n",
            "Epoch [9], Batch [111/157], test Loss: 0.5277\n",
            "Epoch [9], Batch [112/157], test Loss: 0.4898\n",
            "Epoch [9], Batch [113/157], test Loss: 0.4479\n",
            "Epoch [9], Batch [114/157], test Loss: 0.7404\n",
            "Epoch [9], Batch [115/157], test Loss: 0.5280\n",
            "Epoch [9], Batch [116/157], test Loss: 0.5590\n",
            "Epoch [9], Batch [117/157], test Loss: 0.6752\n",
            "Epoch [9], Batch [118/157], test Loss: 0.6633\n",
            "Epoch [9], Batch [119/157], test Loss: 0.5047\n",
            "Epoch [9], Batch [120/157], test Loss: 0.5960\n",
            "Epoch [9], Batch [121/157], test Loss: 0.6111\n",
            "Epoch [9], Batch [122/157], test Loss: 0.4532\n",
            "Epoch [9], Batch [123/157], test Loss: 0.5210\n",
            "Epoch [9], Batch [124/157], test Loss: 0.6319\n",
            "Epoch [9], Batch [125/157], test Loss: 0.5936\n",
            "Epoch [9], Batch [126/157], test Loss: 0.4526\n",
            "Epoch [9], Batch [127/157], test Loss: 0.4969\n",
            "Epoch [9], Batch [128/157], test Loss: 0.7004\n",
            "Epoch [9], Batch [129/157], test Loss: 0.4278\n",
            "Epoch [9], Batch [130/157], test Loss: 0.4660\n",
            "Epoch [9], Batch [131/157], test Loss: 0.7027\n",
            "Epoch [9], Batch [132/157], test Loss: 0.4466\n",
            "Epoch [9], Batch [133/157], test Loss: 0.3873\n",
            "Epoch [9], Batch [134/157], test Loss: 0.6030\n",
            "Epoch [9], Batch [135/157], test Loss: 0.6071\n",
            "Epoch [9], Batch [136/157], test Loss: 0.5692\n",
            "Epoch [9], Batch [137/157], test Loss: 0.4237\n",
            "Epoch [9], Batch [138/157], test Loss: 0.5724\n",
            "Epoch [9], Batch [139/157], test Loss: 0.7058\n",
            "Epoch [9], Batch [140/157], test Loss: 0.3082\n",
            "Epoch [9], Batch [141/157], test Loss: 0.4134\n",
            "Epoch [9], Batch [142/157], test Loss: 0.5789\n",
            "Epoch [9], Batch [143/157], test Loss: 0.7089\n",
            "Epoch [9], Batch [144/157], test Loss: 0.5453\n",
            "Epoch [9], Batch [145/157], test Loss: 1.0349\n",
            "Epoch [9], Batch [146/157], test Loss: 0.5145\n",
            "Epoch [9], Batch [147/157], test Loss: 0.3522\n",
            "Epoch [9], Batch [148/157], test Loss: 0.6520\n",
            "Epoch [9], Batch [149/157], test Loss: 0.5025\n",
            "Epoch [9], Batch [150/157], test Loss: 0.7258\n",
            "Epoch [9], Batch [151/157], test Loss: 0.5786\n",
            "Epoch [9], Batch [152/157], test Loss: 0.5294\n",
            "Epoch [9], Batch [153/157], test Loss: 0.6927\n",
            "Epoch [9], Batch [154/157], test Loss: 0.5627\n",
            "Epoch [9], Batch [155/157], test Loss: 0.6321\n",
            "Epoch [9], Batch [156/157], test Loss: 0.5761\n",
            "Epoch [9], Batch [157/157], test Loss: 0.8859\n",
            "Accuracy of test set: 80.52%\n",
            "Epoch [10/25] - Train Loss: 0.5390, Train Accuracy: 81.27% - Test Loss: 0.5607, Test Accuracy: 80.52%\n",
            "Epoch [10], Batch [10/938], Loss: 0.7386\n",
            "Epoch [10], Batch [20/938], Loss: 0.5854\n",
            "Epoch [10], Batch [30/938], Loss: 0.5373\n",
            "Epoch [10], Batch [40/938], Loss: 0.6322\n",
            "Epoch [10], Batch [50/938], Loss: 0.7483\n",
            "Epoch [10], Batch [60/938], Loss: 0.5236\n",
            "Epoch [10], Batch [70/938], Loss: 0.4496\n",
            "Epoch [10], Batch [80/938], Loss: 0.6734\n",
            "Epoch [10], Batch [90/938], Loss: 0.5199\n",
            "Epoch [10], Batch [100/938], Loss: 0.5517\n",
            "Epoch [10], Batch [110/938], Loss: 0.6447\n",
            "Epoch [10], Batch [120/938], Loss: 0.4708\n",
            "Epoch [10], Batch [130/938], Loss: 0.4210\n",
            "Epoch [10], Batch [140/938], Loss: 0.3688\n",
            "Epoch [10], Batch [150/938], Loss: 0.6051\n",
            "Epoch [10], Batch [160/938], Loss: 0.3991\n",
            "Epoch [10], Batch [170/938], Loss: 0.5957\n",
            "Epoch [10], Batch [180/938], Loss: 0.5695\n",
            "Epoch [10], Batch [190/938], Loss: 0.2700\n",
            "Epoch [10], Batch [200/938], Loss: 0.4511\n",
            "Epoch [10], Batch [210/938], Loss: 0.5299\n",
            "Epoch [10], Batch [220/938], Loss: 0.5024\n",
            "Epoch [10], Batch [230/938], Loss: 0.8123\n",
            "Epoch [10], Batch [240/938], Loss: 0.4666\n",
            "Epoch [10], Batch [250/938], Loss: 0.6871\n",
            "Epoch [10], Batch [260/938], Loss: 0.5264\n",
            "Epoch [10], Batch [270/938], Loss: 0.5445\n",
            "Epoch [10], Batch [280/938], Loss: 0.5896\n",
            "Epoch [10], Batch [290/938], Loss: 0.5498\n",
            "Epoch [10], Batch [300/938], Loss: 0.5549\n",
            "Epoch [10], Batch [310/938], Loss: 0.7991\n",
            "Epoch [10], Batch [320/938], Loss: 0.5794\n",
            "Epoch [10], Batch [330/938], Loss: 0.7074\n",
            "Epoch [10], Batch [340/938], Loss: 0.5285\n",
            "Epoch [10], Batch [350/938], Loss: 0.4030\n",
            "Epoch [10], Batch [360/938], Loss: 0.4409\n",
            "Epoch [10], Batch [370/938], Loss: 0.5269\n",
            "Epoch [10], Batch [380/938], Loss: 0.3897\n",
            "Epoch [10], Batch [390/938], Loss: 0.6259\n",
            "Epoch [10], Batch [400/938], Loss: 0.5386\n",
            "Epoch [10], Batch [410/938], Loss: 0.5657\n",
            "Epoch [10], Batch [420/938], Loss: 0.4824\n",
            "Epoch [10], Batch [430/938], Loss: 0.5658\n",
            "Epoch [10], Batch [440/938], Loss: 0.4441\n",
            "Epoch [10], Batch [450/938], Loss: 0.6278\n",
            "Epoch [10], Batch [460/938], Loss: 0.5521\n",
            "Epoch [10], Batch [470/938], Loss: 0.5088\n",
            "Epoch [10], Batch [480/938], Loss: 0.4596\n",
            "Epoch [10], Batch [490/938], Loss: 0.7214\n",
            "Epoch [10], Batch [500/938], Loss: 0.6541\n",
            "Epoch [10], Batch [510/938], Loss: 0.4327\n",
            "Epoch [10], Batch [520/938], Loss: 0.5271\n",
            "Epoch [10], Batch [530/938], Loss: 0.5625\n",
            "Epoch [10], Batch [540/938], Loss: 0.5853\n",
            "Epoch [10], Batch [550/938], Loss: 0.6701\n",
            "Epoch [10], Batch [560/938], Loss: 0.5033\n",
            "Epoch [10], Batch [570/938], Loss: 0.4269\n",
            "Epoch [10], Batch [580/938], Loss: 0.3518\n",
            "Epoch [10], Batch [590/938], Loss: 0.4804\n",
            "Epoch [10], Batch [600/938], Loss: 0.5351\n",
            "Epoch [10], Batch [610/938], Loss: 0.6787\n",
            "Epoch [10], Batch [620/938], Loss: 0.4440\n",
            "Epoch [10], Batch [630/938], Loss: 0.5108\n",
            "Epoch [10], Batch [640/938], Loss: 0.8045\n",
            "Epoch [10], Batch [650/938], Loss: 0.7144\n",
            "Epoch [10], Batch [660/938], Loss: 0.6103\n",
            "Epoch [10], Batch [670/938], Loss: 0.4279\n",
            "Epoch [10], Batch [680/938], Loss: 0.3598\n",
            "Epoch [10], Batch [690/938], Loss: 0.4781\n",
            "Epoch [10], Batch [700/938], Loss: 0.4955\n",
            "Epoch [10], Batch [710/938], Loss: 0.6167\n",
            "Epoch [10], Batch [720/938], Loss: 0.3612\n",
            "Epoch [10], Batch [730/938], Loss: 0.5647\n",
            "Epoch [10], Batch [740/938], Loss: 0.4684\n",
            "Epoch [10], Batch [750/938], Loss: 0.3369\n",
            "Epoch [10], Batch [760/938], Loss: 0.4822\n",
            "Epoch [10], Batch [770/938], Loss: 0.7547\n",
            "Epoch [10], Batch [780/938], Loss: 0.4807\n",
            "Epoch [10], Batch [790/938], Loss: 0.4799\n",
            "Epoch [10], Batch [800/938], Loss: 0.3193\n",
            "Epoch [10], Batch [810/938], Loss: 0.5380\n",
            "Epoch [10], Batch [820/938], Loss: 0.7725\n",
            "Epoch [10], Batch [830/938], Loss: 0.5262\n",
            "Epoch [10], Batch [840/938], Loss: 0.5215\n",
            "Epoch [10], Batch [850/938], Loss: 0.6706\n",
            "Epoch [10], Batch [860/938], Loss: 0.3191\n",
            "Epoch [10], Batch [870/938], Loss: 0.4206\n",
            "Epoch [10], Batch [880/938], Loss: 0.6390\n",
            "Epoch [10], Batch [890/938], Loss: 0.6683\n",
            "Epoch [10], Batch [900/938], Loss: 0.4263\n",
            "Epoch [10], Batch [910/938], Loss: 0.5226\n",
            "Epoch [10], Batch [920/938], Loss: 0.7832\n",
            "Epoch [10], Batch [930/938], Loss: 0.5636\n",
            "Epoch [10], Batch [938/938], Loss: 0.5717\n",
            "Accuracy of train set: 81.88%\n",
            "Epoch [10], Batch [1/157], test Loss: 0.4012\n",
            "Epoch [10], Batch [2/157], test Loss: 0.5965\n",
            "Epoch [10], Batch [3/157], test Loss: 0.3398\n",
            "Epoch [10], Batch [4/157], test Loss: 0.3643\n",
            "Epoch [10], Batch [5/157], test Loss: 0.4826\n",
            "Epoch [10], Batch [6/157], test Loss: 0.3968\n",
            "Epoch [10], Batch [7/157], test Loss: 0.3828\n",
            "Epoch [10], Batch [8/157], test Loss: 0.5470\n",
            "Epoch [10], Batch [9/157], test Loss: 0.4843\n",
            "Epoch [10], Batch [10/157], test Loss: 0.4314\n",
            "Epoch [10], Batch [11/157], test Loss: 0.5019\n",
            "Epoch [10], Batch [12/157], test Loss: 0.4030\n",
            "Epoch [10], Batch [13/157], test Loss: 0.6168\n",
            "Epoch [10], Batch [14/157], test Loss: 0.9610\n",
            "Epoch [10], Batch [15/157], test Loss: 0.8193\n",
            "Epoch [10], Batch [16/157], test Loss: 0.5092\n",
            "Epoch [10], Batch [17/157], test Loss: 0.4378\n",
            "Epoch [10], Batch [18/157], test Loss: 0.4733\n",
            "Epoch [10], Batch [19/157], test Loss: 0.5953\n",
            "Epoch [10], Batch [20/157], test Loss: 0.5454\n",
            "Epoch [10], Batch [21/157], test Loss: 0.7418\n",
            "Epoch [10], Batch [22/157], test Loss: 0.8246\n",
            "Epoch [10], Batch [23/157], test Loss: 0.4751\n",
            "Epoch [10], Batch [24/157], test Loss: 0.4949\n",
            "Epoch [10], Batch [25/157], test Loss: 0.3740\n",
            "Epoch [10], Batch [26/157], test Loss: 0.4330\n",
            "Epoch [10], Batch [27/157], test Loss: 0.5454\n",
            "Epoch [10], Batch [28/157], test Loss: 0.4242\n",
            "Epoch [10], Batch [29/157], test Loss: 0.6027\n",
            "Epoch [10], Batch [30/157], test Loss: 0.5110\n",
            "Epoch [10], Batch [31/157], test Loss: 0.7184\n",
            "Epoch [10], Batch [32/157], test Loss: 0.5958\n",
            "Epoch [10], Batch [33/157], test Loss: 0.6031\n",
            "Epoch [10], Batch [34/157], test Loss: 0.4698\n",
            "Epoch [10], Batch [35/157], test Loss: 0.5502\n",
            "Epoch [10], Batch [36/157], test Loss: 0.5418\n",
            "Epoch [10], Batch [37/157], test Loss: 0.6457\n",
            "Epoch [10], Batch [38/157], test Loss: 0.4067\n",
            "Epoch [10], Batch [39/157], test Loss: 0.3878\n",
            "Epoch [10], Batch [40/157], test Loss: 0.5482\n",
            "Epoch [10], Batch [41/157], test Loss: 0.4827\n",
            "Epoch [10], Batch [42/157], test Loss: 0.7811\n",
            "Epoch [10], Batch [43/157], test Loss: 0.6593\n",
            "Epoch [10], Batch [44/157], test Loss: 0.4155\n",
            "Epoch [10], Batch [45/157], test Loss: 0.4560\n",
            "Epoch [10], Batch [46/157], test Loss: 0.7011\n",
            "Epoch [10], Batch [47/157], test Loss: 0.5824\n",
            "Epoch [10], Batch [48/157], test Loss: 0.6251\n",
            "Epoch [10], Batch [49/157], test Loss: 0.5642\n",
            "Epoch [10], Batch [50/157], test Loss: 0.5551\n",
            "Epoch [10], Batch [51/157], test Loss: 0.7034\n",
            "Epoch [10], Batch [52/157], test Loss: 0.6198\n",
            "Epoch [10], Batch [53/157], test Loss: 0.5795\n",
            "Epoch [10], Batch [54/157], test Loss: 0.5663\n",
            "Epoch [10], Batch [55/157], test Loss: 0.6149\n",
            "Epoch [10], Batch [56/157], test Loss: 0.4695\n",
            "Epoch [10], Batch [57/157], test Loss: 0.4235\n",
            "Epoch [10], Batch [58/157], test Loss: 0.4234\n",
            "Epoch [10], Batch [59/157], test Loss: 0.3826\n",
            "Epoch [10], Batch [60/157], test Loss: 0.4223\n",
            "Epoch [10], Batch [61/157], test Loss: 0.5534\n",
            "Epoch [10], Batch [62/157], test Loss: 0.5413\n",
            "Epoch [10], Batch [63/157], test Loss: 0.4957\n",
            "Epoch [10], Batch [64/157], test Loss: 0.4683\n",
            "Epoch [10], Batch [65/157], test Loss: 0.4302\n",
            "Epoch [10], Batch [66/157], test Loss: 0.7159\n",
            "Epoch [10], Batch [67/157], test Loss: 0.5248\n",
            "Epoch [10], Batch [68/157], test Loss: 0.6124\n",
            "Epoch [10], Batch [69/157], test Loss: 0.4322\n",
            "Epoch [10], Batch [70/157], test Loss: 0.5809\n",
            "Epoch [10], Batch [71/157], test Loss: 0.5771\n",
            "Epoch [10], Batch [72/157], test Loss: 0.6366\n",
            "Epoch [10], Batch [73/157], test Loss: 0.7147\n",
            "Epoch [10], Batch [74/157], test Loss: 0.6880\n",
            "Epoch [10], Batch [75/157], test Loss: 0.4262\n",
            "Epoch [10], Batch [76/157], test Loss: 0.4037\n",
            "Epoch [10], Batch [77/157], test Loss: 0.6740\n",
            "Epoch [10], Batch [78/157], test Loss: 0.4490\n",
            "Epoch [10], Batch [79/157], test Loss: 0.6020\n",
            "Epoch [10], Batch [80/157], test Loss: 0.8309\n",
            "Epoch [10], Batch [81/157], test Loss: 0.5725\n",
            "Epoch [10], Batch [82/157], test Loss: 0.5157\n",
            "Epoch [10], Batch [83/157], test Loss: 0.5841\n",
            "Epoch [10], Batch [84/157], test Loss: 0.6417\n",
            "Epoch [10], Batch [85/157], test Loss: 0.6013\n",
            "Epoch [10], Batch [86/157], test Loss: 0.4071\n",
            "Epoch [10], Batch [87/157], test Loss: 0.4327\n",
            "Epoch [10], Batch [88/157], test Loss: 0.6016\n",
            "Epoch [10], Batch [89/157], test Loss: 0.5717\n",
            "Epoch [10], Batch [90/157], test Loss: 0.5631\n",
            "Epoch [10], Batch [91/157], test Loss: 0.4527\n",
            "Epoch [10], Batch [92/157], test Loss: 0.5184\n",
            "Epoch [10], Batch [93/157], test Loss: 0.5780\n",
            "Epoch [10], Batch [94/157], test Loss: 0.6171\n",
            "Epoch [10], Batch [95/157], test Loss: 0.4855\n",
            "Epoch [10], Batch [96/157], test Loss: 0.4443\n",
            "Epoch [10], Batch [97/157], test Loss: 0.5045\n",
            "Epoch [10], Batch [98/157], test Loss: 0.5313\n",
            "Epoch [10], Batch [99/157], test Loss: 0.4898\n",
            "Epoch [10], Batch [100/157], test Loss: 0.5883\n",
            "Epoch [10], Batch [101/157], test Loss: 0.5487\n",
            "Epoch [10], Batch [102/157], test Loss: 0.4369\n",
            "Epoch [10], Batch [103/157], test Loss: 0.5671\n",
            "Epoch [10], Batch [104/157], test Loss: 0.8859\n",
            "Epoch [10], Batch [105/157], test Loss: 0.6413\n",
            "Epoch [10], Batch [106/157], test Loss: 0.6357\n",
            "Epoch [10], Batch [107/157], test Loss: 0.6281\n",
            "Epoch [10], Batch [108/157], test Loss: 0.5786\n",
            "Epoch [10], Batch [109/157], test Loss: 0.4139\n",
            "Epoch [10], Batch [110/157], test Loss: 0.4520\n",
            "Epoch [10], Batch [111/157], test Loss: 0.6364\n",
            "Epoch [10], Batch [112/157], test Loss: 0.6524\n",
            "Epoch [10], Batch [113/157], test Loss: 0.6391\n",
            "Epoch [10], Batch [114/157], test Loss: 0.4900\n",
            "Epoch [10], Batch [115/157], test Loss: 0.5247\n",
            "Epoch [10], Batch [116/157], test Loss: 0.4718\n",
            "Epoch [10], Batch [117/157], test Loss: 0.5915\n",
            "Epoch [10], Batch [118/157], test Loss: 0.7198\n",
            "Epoch [10], Batch [119/157], test Loss: 0.7670\n",
            "Epoch [10], Batch [120/157], test Loss: 0.6554\n",
            "Epoch [10], Batch [121/157], test Loss: 0.4440\n",
            "Epoch [10], Batch [122/157], test Loss: 0.6189\n",
            "Epoch [10], Batch [123/157], test Loss: 0.6499\n",
            "Epoch [10], Batch [124/157], test Loss: 0.6597\n",
            "Epoch [10], Batch [125/157], test Loss: 0.6014\n",
            "Epoch [10], Batch [126/157], test Loss: 0.4847\n",
            "Epoch [10], Batch [127/157], test Loss: 0.3929\n",
            "Epoch [10], Batch [128/157], test Loss: 0.4042\n",
            "Epoch [10], Batch [129/157], test Loss: 0.4539\n",
            "Epoch [10], Batch [130/157], test Loss: 0.5001\n",
            "Epoch [10], Batch [131/157], test Loss: 0.6391\n",
            "Epoch [10], Batch [132/157], test Loss: 0.5405\n",
            "Epoch [10], Batch [133/157], test Loss: 0.5393\n",
            "Epoch [10], Batch [134/157], test Loss: 0.3748\n",
            "Epoch [10], Batch [135/157], test Loss: 0.6977\n",
            "Epoch [10], Batch [136/157], test Loss: 0.5884\n",
            "Epoch [10], Batch [137/157], test Loss: 0.6309\n",
            "Epoch [10], Batch [138/157], test Loss: 0.4567\n",
            "Epoch [10], Batch [139/157], test Loss: 0.6172\n",
            "Epoch [10], Batch [140/157], test Loss: 0.4368\n",
            "Epoch [10], Batch [141/157], test Loss: 0.4535\n",
            "Epoch [10], Batch [142/157], test Loss: 0.5414\n",
            "Epoch [10], Batch [143/157], test Loss: 0.4826\n",
            "Epoch [10], Batch [144/157], test Loss: 0.4472\n",
            "Epoch [10], Batch [145/157], test Loss: 0.5516\n",
            "Epoch [10], Batch [146/157], test Loss: 0.5504\n",
            "Epoch [10], Batch [147/157], test Loss: 0.6735\n",
            "Epoch [10], Batch [148/157], test Loss: 0.5812\n",
            "Epoch [10], Batch [149/157], test Loss: 0.5982\n",
            "Epoch [10], Batch [150/157], test Loss: 0.5890\n",
            "Epoch [10], Batch [151/157], test Loss: 0.4381\n",
            "Epoch [10], Batch [152/157], test Loss: 0.5240\n",
            "Epoch [10], Batch [153/157], test Loss: 0.4785\n",
            "Epoch [10], Batch [154/157], test Loss: 0.3947\n",
            "Epoch [10], Batch [155/157], test Loss: 0.6476\n",
            "Epoch [10], Batch [156/157], test Loss: 0.2841\n",
            "Epoch [10], Batch [157/157], test Loss: 0.8276\n",
            "Accuracy of test set: 80.60%\n",
            "Epoch [11/25] - Train Loss: 0.5211, Train Accuracy: 81.88% - Test Loss: 0.5478, Test Accuracy: 80.60%\n",
            "Epoch [11], Batch [10/938], Loss: 0.5774\n",
            "Epoch [11], Batch [20/938], Loss: 0.3973\n",
            "Epoch [11], Batch [30/938], Loss: 0.6570\n",
            "Epoch [11], Batch [40/938], Loss: 0.2050\n",
            "Epoch [11], Batch [50/938], Loss: 0.5821\n",
            "Epoch [11], Batch [60/938], Loss: 0.4844\n",
            "Epoch [11], Batch [70/938], Loss: 0.5747\n",
            "Epoch [11], Batch [80/938], Loss: 0.2896\n",
            "Epoch [11], Batch [90/938], Loss: 0.4646\n",
            "Epoch [11], Batch [100/938], Loss: 0.6018\n",
            "Epoch [11], Batch [110/938], Loss: 0.4358\n",
            "Epoch [11], Batch [120/938], Loss: 0.3601\n",
            "Epoch [11], Batch [130/938], Loss: 0.4286\n",
            "Epoch [11], Batch [140/938], Loss: 0.4752\n",
            "Epoch [11], Batch [150/938], Loss: 0.3144\n",
            "Epoch [11], Batch [160/938], Loss: 0.4806\n",
            "Epoch [11], Batch [170/938], Loss: 0.4798\n",
            "Epoch [11], Batch [180/938], Loss: 0.7944\n",
            "Epoch [11], Batch [190/938], Loss: 0.4857\n",
            "Epoch [11], Batch [200/938], Loss: 0.4452\n",
            "Epoch [11], Batch [210/938], Loss: 0.6257\n",
            "Epoch [11], Batch [220/938], Loss: 0.4870\n",
            "Epoch [11], Batch [230/938], Loss: 0.5113\n",
            "Epoch [11], Batch [240/938], Loss: 0.4303\n",
            "Epoch [11], Batch [250/938], Loss: 0.7472\n",
            "Epoch [11], Batch [260/938], Loss: 0.5990\n",
            "Epoch [11], Batch [270/938], Loss: 0.5788\n",
            "Epoch [11], Batch [280/938], Loss: 0.6208\n",
            "Epoch [11], Batch [290/938], Loss: 0.4422\n",
            "Epoch [11], Batch [300/938], Loss: 0.4572\n",
            "Epoch [11], Batch [310/938], Loss: 0.4334\n",
            "Epoch [11], Batch [320/938], Loss: 0.3855\n",
            "Epoch [11], Batch [330/938], Loss: 0.5192\n",
            "Epoch [11], Batch [340/938], Loss: 0.7257\n",
            "Epoch [11], Batch [350/938], Loss: 0.4483\n",
            "Epoch [11], Batch [360/938], Loss: 0.5485\n",
            "Epoch [11], Batch [370/938], Loss: 0.6948\n",
            "Epoch [11], Batch [380/938], Loss: 0.4774\n",
            "Epoch [11], Batch [390/938], Loss: 0.6013\n",
            "Epoch [11], Batch [400/938], Loss: 0.4670\n",
            "Epoch [11], Batch [410/938], Loss: 0.5238\n",
            "Epoch [11], Batch [420/938], Loss: 0.5135\n",
            "Epoch [11], Batch [430/938], Loss: 0.4450\n",
            "Epoch [11], Batch [440/938], Loss: 0.5105\n",
            "Epoch [11], Batch [450/938], Loss: 0.6333\n",
            "Epoch [11], Batch [460/938], Loss: 0.4096\n",
            "Epoch [11], Batch [470/938], Loss: 0.5442\n",
            "Epoch [11], Batch [480/938], Loss: 0.5520\n",
            "Epoch [11], Batch [490/938], Loss: 0.5533\n",
            "Epoch [11], Batch [500/938], Loss: 0.6097\n",
            "Epoch [11], Batch [510/938], Loss: 0.6018\n",
            "Epoch [11], Batch [520/938], Loss: 0.6947\n",
            "Epoch [11], Batch [530/938], Loss: 0.3887\n",
            "Epoch [11], Batch [540/938], Loss: 0.6329\n",
            "Epoch [11], Batch [550/938], Loss: 0.5340\n",
            "Epoch [11], Batch [560/938], Loss: 0.4989\n",
            "Epoch [11], Batch [570/938], Loss: 0.3888\n",
            "Epoch [11], Batch [580/938], Loss: 0.5997\n",
            "Epoch [11], Batch [590/938], Loss: 0.4601\n",
            "Epoch [11], Batch [600/938], Loss: 0.3766\n",
            "Epoch [11], Batch [610/938], Loss: 0.5564\n",
            "Epoch [11], Batch [620/938], Loss: 0.5195\n",
            "Epoch [11], Batch [630/938], Loss: 0.4610\n",
            "Epoch [11], Batch [640/938], Loss: 0.6859\n",
            "Epoch [11], Batch [650/938], Loss: 0.3323\n",
            "Epoch [11], Batch [660/938], Loss: 0.4673\n",
            "Epoch [11], Batch [670/938], Loss: 0.4031\n",
            "Epoch [11], Batch [680/938], Loss: 0.5449\n",
            "Epoch [11], Batch [690/938], Loss: 0.5714\n",
            "Epoch [11], Batch [700/938], Loss: 0.4163\n",
            "Epoch [11], Batch [710/938], Loss: 0.3862\n",
            "Epoch [11], Batch [720/938], Loss: 0.4352\n",
            "Epoch [11], Batch [730/938], Loss: 0.5202\n",
            "Epoch [11], Batch [740/938], Loss: 0.2728\n",
            "Epoch [11], Batch [750/938], Loss: 0.4642\n",
            "Epoch [11], Batch [760/938], Loss: 0.4623\n",
            "Epoch [11], Batch [770/938], Loss: 0.4573\n",
            "Epoch [11], Batch [780/938], Loss: 0.3573\n",
            "Epoch [11], Batch [790/938], Loss: 0.7552\n",
            "Epoch [11], Batch [800/938], Loss: 0.5597\n",
            "Epoch [11], Batch [810/938], Loss: 0.4481\n",
            "Epoch [11], Batch [820/938], Loss: 0.4294\n",
            "Epoch [11], Batch [830/938], Loss: 0.3910\n",
            "Epoch [11], Batch [840/938], Loss: 0.5221\n",
            "Epoch [11], Batch [850/938], Loss: 0.5253\n",
            "Epoch [11], Batch [860/938], Loss: 0.4891\n",
            "Epoch [11], Batch [870/938], Loss: 0.5700\n",
            "Epoch [11], Batch [880/938], Loss: 0.5651\n",
            "Epoch [11], Batch [890/938], Loss: 0.5420\n",
            "Epoch [11], Batch [900/938], Loss: 0.6042\n",
            "Epoch [11], Batch [910/938], Loss: 0.3989\n",
            "Epoch [11], Batch [920/938], Loss: 0.3825\n",
            "Epoch [11], Batch [930/938], Loss: 0.3791\n",
            "Epoch [11], Batch [938/938], Loss: 0.2139\n",
            "Accuracy of train set: 82.46%\n",
            "Epoch [11], Batch [1/157], test Loss: 0.7406\n",
            "Epoch [11], Batch [2/157], test Loss: 0.4733\n",
            "Epoch [11], Batch [3/157], test Loss: 0.4126\n",
            "Epoch [11], Batch [4/157], test Loss: 0.3259\n",
            "Epoch [11], Batch [5/157], test Loss: 0.5834\n",
            "Epoch [11], Batch [6/157], test Loss: 0.4559\n",
            "Epoch [11], Batch [7/157], test Loss: 0.5576\n",
            "Epoch [11], Batch [8/157], test Loss: 0.7818\n",
            "Epoch [11], Batch [9/157], test Loss: 0.5446\n",
            "Epoch [11], Batch [10/157], test Loss: 0.5590\n",
            "Epoch [11], Batch [11/157], test Loss: 0.5894\n",
            "Epoch [11], Batch [12/157], test Loss: 0.4114\n",
            "Epoch [11], Batch [13/157], test Loss: 0.4238\n",
            "Epoch [11], Batch [14/157], test Loss: 0.5227\n",
            "Epoch [11], Batch [15/157], test Loss: 0.3235\n",
            "Epoch [11], Batch [16/157], test Loss: 0.4504\n",
            "Epoch [11], Batch [17/157], test Loss: 0.3464\n",
            "Epoch [11], Batch [18/157], test Loss: 0.4403\n",
            "Epoch [11], Batch [19/157], test Loss: 0.4792\n",
            "Epoch [11], Batch [20/157], test Loss: 0.5777\n",
            "Epoch [11], Batch [21/157], test Loss: 0.5007\n",
            "Epoch [11], Batch [22/157], test Loss: 0.5761\n",
            "Epoch [11], Batch [23/157], test Loss: 0.5672\n",
            "Epoch [11], Batch [24/157], test Loss: 0.6896\n",
            "Epoch [11], Batch [25/157], test Loss: 0.4506\n",
            "Epoch [11], Batch [26/157], test Loss: 0.4604\n",
            "Epoch [11], Batch [27/157], test Loss: 0.5905\n",
            "Epoch [11], Batch [28/157], test Loss: 0.4596\n",
            "Epoch [11], Batch [29/157], test Loss: 0.4831\n",
            "Epoch [11], Batch [30/157], test Loss: 0.5945\n",
            "Epoch [11], Batch [31/157], test Loss: 0.4571\n",
            "Epoch [11], Batch [32/157], test Loss: 0.3758\n",
            "Epoch [11], Batch [33/157], test Loss: 0.4245\n",
            "Epoch [11], Batch [34/157], test Loss: 0.6608\n",
            "Epoch [11], Batch [35/157], test Loss: 0.4156\n",
            "Epoch [11], Batch [36/157], test Loss: 0.6616\n",
            "Epoch [11], Batch [37/157], test Loss: 0.4157\n",
            "Epoch [11], Batch [38/157], test Loss: 0.4684\n",
            "Epoch [11], Batch [39/157], test Loss: 0.2687\n",
            "Epoch [11], Batch [40/157], test Loss: 0.6098\n",
            "Epoch [11], Batch [41/157], test Loss: 0.2724\n",
            "Epoch [11], Batch [42/157], test Loss: 0.5910\n",
            "Epoch [11], Batch [43/157], test Loss: 0.7819\n",
            "Epoch [11], Batch [44/157], test Loss: 0.7161\n",
            "Epoch [11], Batch [45/157], test Loss: 0.6385\n",
            "Epoch [11], Batch [46/157], test Loss: 0.4505\n",
            "Epoch [11], Batch [47/157], test Loss: 0.5675\n",
            "Epoch [11], Batch [48/157], test Loss: 0.5042\n",
            "Epoch [11], Batch [49/157], test Loss: 0.5375\n",
            "Epoch [11], Batch [50/157], test Loss: 0.5158\n",
            "Epoch [11], Batch [51/157], test Loss: 0.5872\n",
            "Epoch [11], Batch [52/157], test Loss: 0.6559\n",
            "Epoch [11], Batch [53/157], test Loss: 0.6421\n",
            "Epoch [11], Batch [54/157], test Loss: 0.6035\n",
            "Epoch [11], Batch [55/157], test Loss: 0.5422\n",
            "Epoch [11], Batch [56/157], test Loss: 0.5310\n",
            "Epoch [11], Batch [57/157], test Loss: 0.5591\n",
            "Epoch [11], Batch [58/157], test Loss: 0.4406\n",
            "Epoch [11], Batch [59/157], test Loss: 0.4722\n",
            "Epoch [11], Batch [60/157], test Loss: 0.6192\n",
            "Epoch [11], Batch [61/157], test Loss: 0.5243\n",
            "Epoch [11], Batch [62/157], test Loss: 0.4650\n",
            "Epoch [11], Batch [63/157], test Loss: 0.5657\n",
            "Epoch [11], Batch [64/157], test Loss: 0.5796\n",
            "Epoch [11], Batch [65/157], test Loss: 0.6405\n",
            "Epoch [11], Batch [66/157], test Loss: 0.7004\n",
            "Epoch [11], Batch [67/157], test Loss: 0.5003\n",
            "Epoch [11], Batch [68/157], test Loss: 0.8353\n",
            "Epoch [11], Batch [69/157], test Loss: 0.3435\n",
            "Epoch [11], Batch [70/157], test Loss: 0.6799\n",
            "Epoch [11], Batch [71/157], test Loss: 0.3510\n",
            "Epoch [11], Batch [72/157], test Loss: 0.4302\n",
            "Epoch [11], Batch [73/157], test Loss: 0.5324\n",
            "Epoch [11], Batch [74/157], test Loss: 0.3932\n",
            "Epoch [11], Batch [75/157], test Loss: 0.6595\n",
            "Epoch [11], Batch [76/157], test Loss: 0.5654\n",
            "Epoch [11], Batch [77/157], test Loss: 0.5974\n",
            "Epoch [11], Batch [78/157], test Loss: 0.3177\n",
            "Epoch [11], Batch [79/157], test Loss: 0.8023\n",
            "Epoch [11], Batch [80/157], test Loss: 0.4018\n",
            "Epoch [11], Batch [81/157], test Loss: 0.4126\n",
            "Epoch [11], Batch [82/157], test Loss: 0.6527\n",
            "Epoch [11], Batch [83/157], test Loss: 0.3490\n",
            "Epoch [11], Batch [84/157], test Loss: 0.5253\n",
            "Epoch [11], Batch [85/157], test Loss: 0.6396\n",
            "Epoch [11], Batch [86/157], test Loss: 0.5421\n",
            "Epoch [11], Batch [87/157], test Loss: 0.6846\n",
            "Epoch [11], Batch [88/157], test Loss: 0.4702\n",
            "Epoch [11], Batch [89/157], test Loss: 0.4239\n",
            "Epoch [11], Batch [90/157], test Loss: 0.6149\n",
            "Epoch [11], Batch [91/157], test Loss: 0.4239\n",
            "Epoch [11], Batch [92/157], test Loss: 0.4909\n",
            "Epoch [11], Batch [93/157], test Loss: 0.4662\n",
            "Epoch [11], Batch [94/157], test Loss: 0.3758\n",
            "Epoch [11], Batch [95/157], test Loss: 0.4089\n",
            "Epoch [11], Batch [96/157], test Loss: 0.6233\n",
            "Epoch [11], Batch [97/157], test Loss: 0.6080\n",
            "Epoch [11], Batch [98/157], test Loss: 0.5325\n",
            "Epoch [11], Batch [99/157], test Loss: 0.5311\n",
            "Epoch [11], Batch [100/157], test Loss: 0.4114\n",
            "Epoch [11], Batch [101/157], test Loss: 0.6712\n",
            "Epoch [11], Batch [102/157], test Loss: 0.3708\n",
            "Epoch [11], Batch [103/157], test Loss: 0.5110\n",
            "Epoch [11], Batch [104/157], test Loss: 0.4874\n",
            "Epoch [11], Batch [105/157], test Loss: 0.4247\n",
            "Epoch [11], Batch [106/157], test Loss: 0.4337\n",
            "Epoch [11], Batch [107/157], test Loss: 0.4150\n",
            "Epoch [11], Batch [108/157], test Loss: 0.4250\n",
            "Epoch [11], Batch [109/157], test Loss: 0.5493\n",
            "Epoch [11], Batch [110/157], test Loss: 0.4980\n",
            "Epoch [11], Batch [111/157], test Loss: 0.5082\n",
            "Epoch [11], Batch [112/157], test Loss: 0.6152\n",
            "Epoch [11], Batch [113/157], test Loss: 0.4325\n",
            "Epoch [11], Batch [114/157], test Loss: 0.5669\n",
            "Epoch [11], Batch [115/157], test Loss: 0.7415\n",
            "Epoch [11], Batch [116/157], test Loss: 0.5586\n",
            "Epoch [11], Batch [117/157], test Loss: 0.8739\n",
            "Epoch [11], Batch [118/157], test Loss: 0.3629\n",
            "Epoch [11], Batch [119/157], test Loss: 0.5582\n",
            "Epoch [11], Batch [120/157], test Loss: 0.5847\n",
            "Epoch [11], Batch [121/157], test Loss: 0.5662\n",
            "Epoch [11], Batch [122/157], test Loss: 0.4409\n",
            "Epoch [11], Batch [123/157], test Loss: 0.4426\n",
            "Epoch [11], Batch [124/157], test Loss: 0.5822\n",
            "Epoch [11], Batch [125/157], test Loss: 0.5301\n",
            "Epoch [11], Batch [126/157], test Loss: 0.3663\n",
            "Epoch [11], Batch [127/157], test Loss: 0.6884\n",
            "Epoch [11], Batch [128/157], test Loss: 0.5221\n",
            "Epoch [11], Batch [129/157], test Loss: 0.4531\n",
            "Epoch [11], Batch [130/157], test Loss: 0.6310\n",
            "Epoch [11], Batch [131/157], test Loss: 0.5997\n",
            "Epoch [11], Batch [132/157], test Loss: 0.3097\n",
            "Epoch [11], Batch [133/157], test Loss: 0.5871\n",
            "Epoch [11], Batch [134/157], test Loss: 0.5786\n",
            "Epoch [11], Batch [135/157], test Loss: 0.6093\n",
            "Epoch [11], Batch [136/157], test Loss: 0.4483\n",
            "Epoch [11], Batch [137/157], test Loss: 0.3495\n",
            "Epoch [11], Batch [138/157], test Loss: 0.4777\n",
            "Epoch [11], Batch [139/157], test Loss: 0.6920\n",
            "Epoch [11], Batch [140/157], test Loss: 0.4060\n",
            "Epoch [11], Batch [141/157], test Loss: 0.4514\n",
            "Epoch [11], Batch [142/157], test Loss: 0.7147\n",
            "Epoch [11], Batch [143/157], test Loss: 0.3992\n",
            "Epoch [11], Batch [144/157], test Loss: 0.3334\n",
            "Epoch [11], Batch [145/157], test Loss: 0.5094\n",
            "Epoch [11], Batch [146/157], test Loss: 0.5064\n",
            "Epoch [11], Batch [147/157], test Loss: 0.5262\n",
            "Epoch [11], Batch [148/157], test Loss: 0.4309\n",
            "Epoch [11], Batch [149/157], test Loss: 0.5344\n",
            "Epoch [11], Batch [150/157], test Loss: 0.5079\n",
            "Epoch [11], Batch [151/157], test Loss: 0.5025\n",
            "Epoch [11], Batch [152/157], test Loss: 0.5937\n",
            "Epoch [11], Batch [153/157], test Loss: 0.5684\n",
            "Epoch [11], Batch [154/157], test Loss: 0.6552\n",
            "Epoch [11], Batch [155/157], test Loss: 0.7382\n",
            "Epoch [11], Batch [156/157], test Loss: 0.4633\n",
            "Epoch [11], Batch [157/157], test Loss: 0.3255\n",
            "Accuracy of test set: 81.48%\n",
            "Epoch [12/25] - Train Loss: 0.5037, Train Accuracy: 82.46% - Test Loss: 0.5227, Test Accuracy: 81.48%\n",
            "Epoch [12], Batch [10/938], Loss: 0.4286\n",
            "Epoch [12], Batch [20/938], Loss: 0.4924\n",
            "Epoch [12], Batch [30/938], Loss: 0.4537\n",
            "Epoch [12], Batch [40/938], Loss: 0.3768\n",
            "Epoch [12], Batch [50/938], Loss: 0.4002\n",
            "Epoch [12], Batch [60/938], Loss: 0.4795\n",
            "Epoch [12], Batch [70/938], Loss: 0.4704\n",
            "Epoch [12], Batch [80/938], Loss: 0.5883\n",
            "Epoch [12], Batch [90/938], Loss: 0.4555\n",
            "Epoch [12], Batch [100/938], Loss: 0.6406\n",
            "Epoch [12], Batch [110/938], Loss: 0.5747\n",
            "Epoch [12], Batch [120/938], Loss: 0.4659\n",
            "Epoch [12], Batch [130/938], Loss: 0.4523\n",
            "Epoch [12], Batch [140/938], Loss: 0.6453\n",
            "Epoch [12], Batch [150/938], Loss: 0.2590\n",
            "Epoch [12], Batch [160/938], Loss: 0.4581\n",
            "Epoch [12], Batch [170/938], Loss: 0.5734\n",
            "Epoch [12], Batch [180/938], Loss: 0.4311\n",
            "Epoch [12], Batch [190/938], Loss: 0.5342\n",
            "Epoch [12], Batch [200/938], Loss: 0.7774\n",
            "Epoch [12], Batch [210/938], Loss: 0.4524\n",
            "Epoch [12], Batch [220/938], Loss: 0.6094\n",
            "Epoch [12], Batch [230/938], Loss: 0.4927\n",
            "Epoch [12], Batch [240/938], Loss: 0.5126\n",
            "Epoch [12], Batch [250/938], Loss: 0.4562\n",
            "Epoch [12], Batch [260/938], Loss: 0.5861\n",
            "Epoch [12], Batch [270/938], Loss: 0.6866\n",
            "Epoch [12], Batch [280/938], Loss: 0.4042\n",
            "Epoch [12], Batch [290/938], Loss: 0.4699\n",
            "Epoch [12], Batch [300/938], Loss: 0.5587\n",
            "Epoch [12], Batch [310/938], Loss: 0.3651\n",
            "Epoch [12], Batch [320/938], Loss: 0.2904\n",
            "Epoch [12], Batch [330/938], Loss: 0.6102\n",
            "Epoch [12], Batch [340/938], Loss: 0.6239\n",
            "Epoch [12], Batch [350/938], Loss: 0.4658\n",
            "Epoch [12], Batch [360/938], Loss: 0.5663\n",
            "Epoch [12], Batch [370/938], Loss: 0.3466\n",
            "Epoch [12], Batch [380/938], Loss: 0.5776\n",
            "Epoch [12], Batch [390/938], Loss: 0.3983\n",
            "Epoch [12], Batch [400/938], Loss: 0.3763\n",
            "Epoch [12], Batch [410/938], Loss: 0.3077\n",
            "Epoch [12], Batch [420/938], Loss: 0.4347\n",
            "Epoch [12], Batch [430/938], Loss: 0.6594\n",
            "Epoch [12], Batch [440/938], Loss: 0.3659\n",
            "Epoch [12], Batch [450/938], Loss: 0.4307\n",
            "Epoch [12], Batch [460/938], Loss: 0.3960\n",
            "Epoch [12], Batch [470/938], Loss: 0.3462\n",
            "Epoch [12], Batch [480/938], Loss: 0.6320\n",
            "Epoch [12], Batch [490/938], Loss: 0.5058\n",
            "Epoch [12], Batch [500/938], Loss: 0.2741\n",
            "Epoch [12], Batch [510/938], Loss: 0.8162\n",
            "Epoch [12], Batch [520/938], Loss: 0.5129\n",
            "Epoch [12], Batch [530/938], Loss: 0.4570\n",
            "Epoch [12], Batch [540/938], Loss: 0.2688\n",
            "Epoch [12], Batch [550/938], Loss: 0.4394\n",
            "Epoch [12], Batch [560/938], Loss: 0.4330\n",
            "Epoch [12], Batch [570/938], Loss: 0.4283\n",
            "Epoch [12], Batch [580/938], Loss: 0.8084\n",
            "Epoch [12], Batch [590/938], Loss: 0.5333\n",
            "Epoch [12], Batch [600/938], Loss: 0.4643\n",
            "Epoch [12], Batch [610/938], Loss: 0.4625\n",
            "Epoch [12], Batch [620/938], Loss: 0.3103\n",
            "Epoch [12], Batch [630/938], Loss: 0.4536\n",
            "Epoch [12], Batch [640/938], Loss: 0.3139\n",
            "Epoch [12], Batch [650/938], Loss: 0.5863\n",
            "Epoch [12], Batch [660/938], Loss: 0.2748\n",
            "Epoch [12], Batch [670/938], Loss: 0.4785\n",
            "Epoch [12], Batch [680/938], Loss: 0.4722\n",
            "Epoch [12], Batch [690/938], Loss: 0.5560\n",
            "Epoch [12], Batch [700/938], Loss: 0.4156\n",
            "Epoch [12], Batch [710/938], Loss: 0.4608\n",
            "Epoch [12], Batch [720/938], Loss: 0.5484\n",
            "Epoch [12], Batch [730/938], Loss: 0.5112\n",
            "Epoch [12], Batch [740/938], Loss: 0.4382\n",
            "Epoch [12], Batch [750/938], Loss: 0.4479\n",
            "Epoch [12], Batch [760/938], Loss: 0.5797\n",
            "Epoch [12], Batch [770/938], Loss: 0.4373\n",
            "Epoch [12], Batch [780/938], Loss: 0.4983\n",
            "Epoch [12], Batch [790/938], Loss: 0.4698\n",
            "Epoch [12], Batch [800/938], Loss: 0.4143\n",
            "Epoch [12], Batch [810/938], Loss: 0.5844\n",
            "Epoch [12], Batch [820/938], Loss: 0.4116\n",
            "Epoch [12], Batch [830/938], Loss: 0.5877\n",
            "Epoch [12], Batch [840/938], Loss: 0.4829\n",
            "Epoch [12], Batch [850/938], Loss: 0.5456\n",
            "Epoch [12], Batch [860/938], Loss: 0.4995\n",
            "Epoch [12], Batch [870/938], Loss: 0.4899\n",
            "Epoch [12], Batch [880/938], Loss: 0.4676\n",
            "Epoch [12], Batch [890/938], Loss: 0.6817\n",
            "Epoch [12], Batch [900/938], Loss: 0.5025\n",
            "Epoch [12], Batch [910/938], Loss: 0.3958\n",
            "Epoch [12], Batch [920/938], Loss: 0.5438\n",
            "Epoch [12], Batch [930/938], Loss: 0.5205\n",
            "Epoch [12], Batch [938/938], Loss: 0.7288\n",
            "Accuracy of train set: 82.83%\n",
            "Epoch [12], Batch [1/157], test Loss: 0.6232\n",
            "Epoch [12], Batch [2/157], test Loss: 0.5256\n",
            "Epoch [12], Batch [3/157], test Loss: 0.4827\n",
            "Epoch [12], Batch [4/157], test Loss: 0.8285\n",
            "Epoch [12], Batch [5/157], test Loss: 0.7768\n",
            "Epoch [12], Batch [6/157], test Loss: 0.5334\n",
            "Epoch [12], Batch [7/157], test Loss: 0.3775\n",
            "Epoch [12], Batch [8/157], test Loss: 0.4802\n",
            "Epoch [12], Batch [9/157], test Loss: 0.5286\n",
            "Epoch [12], Batch [10/157], test Loss: 0.3110\n",
            "Epoch [12], Batch [11/157], test Loss: 0.4043\n",
            "Epoch [12], Batch [12/157], test Loss: 0.7284\n",
            "Epoch [12], Batch [13/157], test Loss: 0.4972\n",
            "Epoch [12], Batch [14/157], test Loss: 0.5146\n",
            "Epoch [12], Batch [15/157], test Loss: 0.5356\n",
            "Epoch [12], Batch [16/157], test Loss: 0.7630\n",
            "Epoch [12], Batch [17/157], test Loss: 0.4336\n",
            "Epoch [12], Batch [18/157], test Loss: 0.5151\n",
            "Epoch [12], Batch [19/157], test Loss: 0.4088\n",
            "Epoch [12], Batch [20/157], test Loss: 0.6359\n",
            "Epoch [12], Batch [21/157], test Loss: 0.4731\n",
            "Epoch [12], Batch [22/157], test Loss: 0.7736\n",
            "Epoch [12], Batch [23/157], test Loss: 0.6998\n",
            "Epoch [12], Batch [24/157], test Loss: 0.5793\n",
            "Epoch [12], Batch [25/157], test Loss: 0.7352\n",
            "Epoch [12], Batch [26/157], test Loss: 0.6057\n",
            "Epoch [12], Batch [27/157], test Loss: 0.4333\n",
            "Epoch [12], Batch [28/157], test Loss: 0.5654\n",
            "Epoch [12], Batch [29/157], test Loss: 0.7617\n",
            "Epoch [12], Batch [30/157], test Loss: 0.4280\n",
            "Epoch [12], Batch [31/157], test Loss: 0.8619\n",
            "Epoch [12], Batch [32/157], test Loss: 0.6289\n",
            "Epoch [12], Batch [33/157], test Loss: 0.6506\n",
            "Epoch [12], Batch [34/157], test Loss: 0.4551\n",
            "Epoch [12], Batch [35/157], test Loss: 0.6670\n",
            "Epoch [12], Batch [36/157], test Loss: 0.7812\n",
            "Epoch [12], Batch [37/157], test Loss: 0.7081\n",
            "Epoch [12], Batch [38/157], test Loss: 0.5161\n",
            "Epoch [12], Batch [39/157], test Loss: 0.8174\n",
            "Epoch [12], Batch [40/157], test Loss: 0.7215\n",
            "Epoch [12], Batch [41/157], test Loss: 0.4313\n",
            "Epoch [12], Batch [42/157], test Loss: 0.5839\n",
            "Epoch [12], Batch [43/157], test Loss: 0.5462\n",
            "Epoch [12], Batch [44/157], test Loss: 0.5491\n",
            "Epoch [12], Batch [45/157], test Loss: 0.5968\n",
            "Epoch [12], Batch [46/157], test Loss: 0.6130\n",
            "Epoch [12], Batch [47/157], test Loss: 0.5430\n",
            "Epoch [12], Batch [48/157], test Loss: 0.8451\n",
            "Epoch [12], Batch [49/157], test Loss: 0.8456\n",
            "Epoch [12], Batch [50/157], test Loss: 0.3934\n",
            "Epoch [12], Batch [51/157], test Loss: 0.5114\n",
            "Epoch [12], Batch [52/157], test Loss: 0.6090\n",
            "Epoch [12], Batch [53/157], test Loss: 0.5566\n",
            "Epoch [12], Batch [54/157], test Loss: 0.4855\n",
            "Epoch [12], Batch [55/157], test Loss: 0.5834\n",
            "Epoch [12], Batch [56/157], test Loss: 0.4962\n",
            "Epoch [12], Batch [57/157], test Loss: 0.4939\n",
            "Epoch [12], Batch [58/157], test Loss: 0.5916\n",
            "Epoch [12], Batch [59/157], test Loss: 0.4237\n",
            "Epoch [12], Batch [60/157], test Loss: 0.5781\n",
            "Epoch [12], Batch [61/157], test Loss: 0.4980\n",
            "Epoch [12], Batch [62/157], test Loss: 0.5103\n",
            "Epoch [12], Batch [63/157], test Loss: 0.6073\n",
            "Epoch [12], Batch [64/157], test Loss: 0.7506\n",
            "Epoch [12], Batch [65/157], test Loss: 0.7058\n",
            "Epoch [12], Batch [66/157], test Loss: 0.7393\n",
            "Epoch [12], Batch [67/157], test Loss: 0.9022\n",
            "Epoch [12], Batch [68/157], test Loss: 0.7440\n",
            "Epoch [12], Batch [69/157], test Loss: 0.4743\n",
            "Epoch [12], Batch [70/157], test Loss: 0.5435\n",
            "Epoch [12], Batch [71/157], test Loss: 0.5073\n",
            "Epoch [12], Batch [72/157], test Loss: 0.4266\n",
            "Epoch [12], Batch [73/157], test Loss: 0.6689\n",
            "Epoch [12], Batch [74/157], test Loss: 0.7609\n",
            "Epoch [12], Batch [75/157], test Loss: 0.5932\n",
            "Epoch [12], Batch [76/157], test Loss: 0.5654\n",
            "Epoch [12], Batch [77/157], test Loss: 0.4707\n",
            "Epoch [12], Batch [78/157], test Loss: 0.5177\n",
            "Epoch [12], Batch [79/157], test Loss: 0.6262\n",
            "Epoch [12], Batch [80/157], test Loss: 0.5058\n",
            "Epoch [12], Batch [81/157], test Loss: 0.4456\n",
            "Epoch [12], Batch [82/157], test Loss: 0.4279\n",
            "Epoch [12], Batch [83/157], test Loss: 0.5589\n",
            "Epoch [12], Batch [84/157], test Loss: 0.5833\n",
            "Epoch [12], Batch [85/157], test Loss: 0.6242\n",
            "Epoch [12], Batch [86/157], test Loss: 0.5711\n",
            "Epoch [12], Batch [87/157], test Loss: 0.4482\n",
            "Epoch [12], Batch [88/157], test Loss: 0.5198\n",
            "Epoch [12], Batch [89/157], test Loss: 0.6934\n",
            "Epoch [12], Batch [90/157], test Loss: 0.8357\n",
            "Epoch [12], Batch [91/157], test Loss: 0.5376\n",
            "Epoch [12], Batch [92/157], test Loss: 0.4828\n",
            "Epoch [12], Batch [93/157], test Loss: 0.7063\n",
            "Epoch [12], Batch [94/157], test Loss: 0.5436\n",
            "Epoch [12], Batch [95/157], test Loss: 0.5357\n",
            "Epoch [12], Batch [96/157], test Loss: 0.6966\n",
            "Epoch [12], Batch [97/157], test Loss: 0.5906\n",
            "Epoch [12], Batch [98/157], test Loss: 0.3886\n",
            "Epoch [12], Batch [99/157], test Loss: 0.7089\n",
            "Epoch [12], Batch [100/157], test Loss: 0.3502\n",
            "Epoch [12], Batch [101/157], test Loss: 0.6286\n",
            "Epoch [12], Batch [102/157], test Loss: 0.7388\n",
            "Epoch [12], Batch [103/157], test Loss: 0.4524\n",
            "Epoch [12], Batch [104/157], test Loss: 0.4548\n",
            "Epoch [12], Batch [105/157], test Loss: 0.6940\n",
            "Epoch [12], Batch [106/157], test Loss: 0.5951\n",
            "Epoch [12], Batch [107/157], test Loss: 0.4699\n",
            "Epoch [12], Batch [108/157], test Loss: 0.6302\n",
            "Epoch [12], Batch [109/157], test Loss: 0.6924\n",
            "Epoch [12], Batch [110/157], test Loss: 0.6134\n",
            "Epoch [12], Batch [111/157], test Loss: 0.4260\n",
            "Epoch [12], Batch [112/157], test Loss: 0.4784\n",
            "Epoch [12], Batch [113/157], test Loss: 0.5710\n",
            "Epoch [12], Batch [114/157], test Loss: 0.6305\n",
            "Epoch [12], Batch [115/157], test Loss: 0.5960\n",
            "Epoch [12], Batch [116/157], test Loss: 0.6148\n",
            "Epoch [12], Batch [117/157], test Loss: 0.4687\n",
            "Epoch [12], Batch [118/157], test Loss: 0.6512\n",
            "Epoch [12], Batch [119/157], test Loss: 0.4944\n",
            "Epoch [12], Batch [120/157], test Loss: 0.7025\n",
            "Epoch [12], Batch [121/157], test Loss: 0.5334\n",
            "Epoch [12], Batch [122/157], test Loss: 0.5266\n",
            "Epoch [12], Batch [123/157], test Loss: 0.4811\n",
            "Epoch [12], Batch [124/157], test Loss: 0.5269\n",
            "Epoch [12], Batch [125/157], test Loss: 0.5843\n",
            "Epoch [12], Batch [126/157], test Loss: 0.7311\n",
            "Epoch [12], Batch [127/157], test Loss: 0.5653\n",
            "Epoch [12], Batch [128/157], test Loss: 0.6520\n",
            "Epoch [12], Batch [129/157], test Loss: 0.4106\n",
            "Epoch [12], Batch [130/157], test Loss: 0.5912\n",
            "Epoch [12], Batch [131/157], test Loss: 0.7614\n",
            "Epoch [12], Batch [132/157], test Loss: 0.6552\n",
            "Epoch [12], Batch [133/157], test Loss: 0.6316\n",
            "Epoch [12], Batch [134/157], test Loss: 0.4401\n",
            "Epoch [12], Batch [135/157], test Loss: 0.6541\n",
            "Epoch [12], Batch [136/157], test Loss: 0.5847\n",
            "Epoch [12], Batch [137/157], test Loss: 0.3847\n",
            "Epoch [12], Batch [138/157], test Loss: 0.4106\n",
            "Epoch [12], Batch [139/157], test Loss: 0.6878\n",
            "Epoch [12], Batch [140/157], test Loss: 0.6992\n",
            "Epoch [12], Batch [141/157], test Loss: 1.1499\n",
            "Epoch [12], Batch [142/157], test Loss: 0.5350\n",
            "Epoch [12], Batch [143/157], test Loss: 0.4770\n",
            "Epoch [12], Batch [144/157], test Loss: 0.7794\n",
            "Epoch [12], Batch [145/157], test Loss: 0.4907\n",
            "Epoch [12], Batch [146/157], test Loss: 0.6995\n",
            "Epoch [12], Batch [147/157], test Loss: 0.5710\n",
            "Epoch [12], Batch [148/157], test Loss: 0.4587\n",
            "Epoch [12], Batch [149/157], test Loss: 0.6296\n",
            "Epoch [12], Batch [150/157], test Loss: 0.5211\n",
            "Epoch [12], Batch [151/157], test Loss: 0.5069\n",
            "Epoch [12], Batch [152/157], test Loss: 0.5525\n",
            "Epoch [12], Batch [153/157], test Loss: 0.6983\n",
            "Epoch [12], Batch [154/157], test Loss: 0.6108\n",
            "Epoch [12], Batch [155/157], test Loss: 0.5279\n",
            "Epoch [12], Batch [156/157], test Loss: 0.4101\n",
            "Epoch [12], Batch [157/157], test Loss: 0.1241\n",
            "Accuracy of test set: 79.01%\n",
            "Epoch [13/25] - Train Loss: 0.4889, Train Accuracy: 82.83% - Test Loss: 0.5813, Test Accuracy: 79.01%\n",
            "Epoch [13], Batch [10/938], Loss: 0.3513\n",
            "Epoch [13], Batch [20/938], Loss: 0.3856\n",
            "Epoch [13], Batch [30/938], Loss: 0.4819\n",
            "Epoch [13], Batch [40/938], Loss: 0.3143\n",
            "Epoch [13], Batch [50/938], Loss: 0.7416\n",
            "Epoch [13], Batch [60/938], Loss: 0.5736\n",
            "Epoch [13], Batch [70/938], Loss: 0.4825\n",
            "Epoch [13], Batch [80/938], Loss: 0.3897\n",
            "Epoch [13], Batch [90/938], Loss: 0.5173\n",
            "Epoch [13], Batch [100/938], Loss: 0.3936\n",
            "Epoch [13], Batch [110/938], Loss: 0.4984\n",
            "Epoch [13], Batch [120/938], Loss: 0.5548\n",
            "Epoch [13], Batch [130/938], Loss: 0.5480\n",
            "Epoch [13], Batch [140/938], Loss: 0.5089\n",
            "Epoch [13], Batch [150/938], Loss: 0.4773\n",
            "Epoch [13], Batch [160/938], Loss: 0.4980\n",
            "Epoch [13], Batch [170/938], Loss: 0.3492\n",
            "Epoch [13], Batch [180/938], Loss: 0.5082\n",
            "Epoch [13], Batch [190/938], Loss: 0.4341\n",
            "Epoch [13], Batch [200/938], Loss: 0.5708\n",
            "Epoch [13], Batch [210/938], Loss: 0.4211\n",
            "Epoch [13], Batch [220/938], Loss: 0.3573\n",
            "Epoch [13], Batch [230/938], Loss: 0.4959\n",
            "Epoch [13], Batch [240/938], Loss: 0.4942\n",
            "Epoch [13], Batch [250/938], Loss: 0.4281\n",
            "Epoch [13], Batch [260/938], Loss: 0.5544\n",
            "Epoch [13], Batch [270/938], Loss: 0.3422\n",
            "Epoch [13], Batch [280/938], Loss: 0.4348\n",
            "Epoch [13], Batch [290/938], Loss: 0.3765\n",
            "Epoch [13], Batch [300/938], Loss: 0.4600\n",
            "Epoch [13], Batch [310/938], Loss: 0.3357\n",
            "Epoch [13], Batch [320/938], Loss: 0.4154\n",
            "Epoch [13], Batch [330/938], Loss: 0.5714\n",
            "Epoch [13], Batch [340/938], Loss: 0.5532\n",
            "Epoch [13], Batch [350/938], Loss: 0.4063\n",
            "Epoch [13], Batch [360/938], Loss: 0.3794\n",
            "Epoch [13], Batch [370/938], Loss: 0.5096\n",
            "Epoch [13], Batch [380/938], Loss: 0.4723\n",
            "Epoch [13], Batch [390/938], Loss: 0.5669\n",
            "Epoch [13], Batch [400/938], Loss: 0.6665\n",
            "Epoch [13], Batch [410/938], Loss: 0.4234\n",
            "Epoch [13], Batch [420/938], Loss: 0.6106\n",
            "Epoch [13], Batch [430/938], Loss: 0.6764\n",
            "Epoch [13], Batch [440/938], Loss: 0.3664\n",
            "Epoch [13], Batch [450/938], Loss: 0.4223\n",
            "Epoch [13], Batch [460/938], Loss: 0.4279\n",
            "Epoch [13], Batch [470/938], Loss: 0.5824\n",
            "Epoch [13], Batch [480/938], Loss: 0.3104\n",
            "Epoch [13], Batch [490/938], Loss: 0.4879\n",
            "Epoch [13], Batch [500/938], Loss: 0.3425\n",
            "Epoch [13], Batch [510/938], Loss: 0.4278\n",
            "Epoch [13], Batch [520/938], Loss: 0.3941\n",
            "Epoch [13], Batch [530/938], Loss: 0.4548\n",
            "Epoch [13], Batch [540/938], Loss: 0.4374\n",
            "Epoch [13], Batch [550/938], Loss: 0.4756\n",
            "Epoch [13], Batch [560/938], Loss: 0.3529\n",
            "Epoch [13], Batch [570/938], Loss: 0.4341\n",
            "Epoch [13], Batch [580/938], Loss: 0.3993\n",
            "Epoch [13], Batch [590/938], Loss: 0.3708\n",
            "Epoch [13], Batch [600/938], Loss: 0.3766\n",
            "Epoch [13], Batch [610/938], Loss: 0.5937\n",
            "Epoch [13], Batch [620/938], Loss: 0.2715\n",
            "Epoch [13], Batch [630/938], Loss: 0.4372\n",
            "Epoch [13], Batch [640/938], Loss: 0.5544\n",
            "Epoch [13], Batch [650/938], Loss: 0.5518\n",
            "Epoch [13], Batch [660/938], Loss: 0.5154\n",
            "Epoch [13], Batch [670/938], Loss: 0.6760\n",
            "Epoch [13], Batch [680/938], Loss: 0.3815\n",
            "Epoch [13], Batch [690/938], Loss: 0.4417\n",
            "Epoch [13], Batch [700/938], Loss: 0.3272\n",
            "Epoch [13], Batch [710/938], Loss: 0.4775\n",
            "Epoch [13], Batch [720/938], Loss: 0.6472\n",
            "Epoch [13], Batch [730/938], Loss: 0.4864\n",
            "Epoch [13], Batch [740/938], Loss: 0.6212\n",
            "Epoch [13], Batch [750/938], Loss: 0.3578\n",
            "Epoch [13], Batch [760/938], Loss: 0.6237\n",
            "Epoch [13], Batch [770/938], Loss: 0.2567\n",
            "Epoch [13], Batch [780/938], Loss: 0.6176\n",
            "Epoch [13], Batch [790/938], Loss: 0.6288\n",
            "Epoch [13], Batch [800/938], Loss: 0.5837\n",
            "Epoch [13], Batch [810/938], Loss: 0.4619\n",
            "Epoch [13], Batch [820/938], Loss: 0.3650\n",
            "Epoch [13], Batch [830/938], Loss: 0.4859\n",
            "Epoch [13], Batch [840/938], Loss: 0.5249\n",
            "Epoch [13], Batch [850/938], Loss: 0.5735\n",
            "Epoch [13], Batch [860/938], Loss: 0.4724\n",
            "Epoch [13], Batch [870/938], Loss: 0.4695\n",
            "Epoch [13], Batch [880/938], Loss: 0.6023\n",
            "Epoch [13], Batch [890/938], Loss: 0.4133\n",
            "Epoch [13], Batch [900/938], Loss: 0.7090\n",
            "Epoch [13], Batch [910/938], Loss: 0.5118\n",
            "Epoch [13], Batch [920/938], Loss: 0.5387\n",
            "Epoch [13], Batch [930/938], Loss: 0.3930\n",
            "Epoch [13], Batch [938/938], Loss: 0.3110\n",
            "Accuracy of train set: 83.34%\n",
            "Epoch [13], Batch [1/157], test Loss: 0.5164\n",
            "Epoch [13], Batch [2/157], test Loss: 0.5429\n",
            "Epoch [13], Batch [3/157], test Loss: 0.6125\n",
            "Epoch [13], Batch [4/157], test Loss: 0.5802\n",
            "Epoch [13], Batch [5/157], test Loss: 0.3733\n",
            "Epoch [13], Batch [6/157], test Loss: 0.4831\n",
            "Epoch [13], Batch [7/157], test Loss: 0.3916\n",
            "Epoch [13], Batch [8/157], test Loss: 0.2916\n",
            "Epoch [13], Batch [9/157], test Loss: 0.5567\n",
            "Epoch [13], Batch [10/157], test Loss: 0.4642\n",
            "Epoch [13], Batch [11/157], test Loss: 0.5795\n",
            "Epoch [13], Batch [12/157], test Loss: 0.5012\n",
            "Epoch [13], Batch [13/157], test Loss: 0.4758\n",
            "Epoch [13], Batch [14/157], test Loss: 0.7555\n",
            "Epoch [13], Batch [15/157], test Loss: 0.4483\n",
            "Epoch [13], Batch [16/157], test Loss: 0.4677\n",
            "Epoch [13], Batch [17/157], test Loss: 0.5034\n",
            "Epoch [13], Batch [18/157], test Loss: 0.4987\n",
            "Epoch [13], Batch [19/157], test Loss: 0.5318\n",
            "Epoch [13], Batch [20/157], test Loss: 0.6602\n",
            "Epoch [13], Batch [21/157], test Loss: 0.5890\n",
            "Epoch [13], Batch [22/157], test Loss: 0.4769\n",
            "Epoch [13], Batch [23/157], test Loss: 0.3778\n",
            "Epoch [13], Batch [24/157], test Loss: 0.3141\n",
            "Epoch [13], Batch [25/157], test Loss: 0.4561\n",
            "Epoch [13], Batch [26/157], test Loss: 0.6917\n",
            "Epoch [13], Batch [27/157], test Loss: 0.5977\n",
            "Epoch [13], Batch [28/157], test Loss: 0.4529\n",
            "Epoch [13], Batch [29/157], test Loss: 0.6990\n",
            "Epoch [13], Batch [30/157], test Loss: 0.4748\n",
            "Epoch [13], Batch [31/157], test Loss: 0.4518\n",
            "Epoch [13], Batch [32/157], test Loss: 0.4970\n",
            "Epoch [13], Batch [33/157], test Loss: 0.4652\n",
            "Epoch [13], Batch [34/157], test Loss: 0.2908\n",
            "Epoch [13], Batch [35/157], test Loss: 0.5774\n",
            "Epoch [13], Batch [36/157], test Loss: 0.6203\n",
            "Epoch [13], Batch [37/157], test Loss: 0.5409\n",
            "Epoch [13], Batch [38/157], test Loss: 0.4960\n",
            "Epoch [13], Batch [39/157], test Loss: 0.5563\n",
            "Epoch [13], Batch [40/157], test Loss: 0.4248\n",
            "Epoch [13], Batch [41/157], test Loss: 0.5111\n",
            "Epoch [13], Batch [42/157], test Loss: 0.5448\n",
            "Epoch [13], Batch [43/157], test Loss: 0.4622\n",
            "Epoch [13], Batch [44/157], test Loss: 0.4331\n",
            "Epoch [13], Batch [45/157], test Loss: 0.5305\n",
            "Epoch [13], Batch [46/157], test Loss: 0.6111\n",
            "Epoch [13], Batch [47/157], test Loss: 0.4388\n",
            "Epoch [13], Batch [48/157], test Loss: 0.5570\n",
            "Epoch [13], Batch [49/157], test Loss: 0.5625\n",
            "Epoch [13], Batch [50/157], test Loss: 0.4663\n",
            "Epoch [13], Batch [51/157], test Loss: 0.5651\n",
            "Epoch [13], Batch [52/157], test Loss: 0.5312\n",
            "Epoch [13], Batch [53/157], test Loss: 0.5336\n",
            "Epoch [13], Batch [54/157], test Loss: 0.5736\n",
            "Epoch [13], Batch [55/157], test Loss: 0.4745\n",
            "Epoch [13], Batch [56/157], test Loss: 0.3711\n",
            "Epoch [13], Batch [57/157], test Loss: 0.6322\n",
            "Epoch [13], Batch [58/157], test Loss: 0.4774\n",
            "Epoch [13], Batch [59/157], test Loss: 0.4317\n",
            "Epoch [13], Batch [60/157], test Loss: 0.5682\n",
            "Epoch [13], Batch [61/157], test Loss: 0.2630\n",
            "Epoch [13], Batch [62/157], test Loss: 0.4408\n",
            "Epoch [13], Batch [63/157], test Loss: 0.4641\n",
            "Epoch [13], Batch [64/157], test Loss: 0.6761\n",
            "Epoch [13], Batch [65/157], test Loss: 0.5310\n",
            "Epoch [13], Batch [66/157], test Loss: 0.5462\n",
            "Epoch [13], Batch [67/157], test Loss: 0.4555\n",
            "Epoch [13], Batch [68/157], test Loss: 0.5540\n",
            "Epoch [13], Batch [69/157], test Loss: 0.5713\n",
            "Epoch [13], Batch [70/157], test Loss: 0.4262\n",
            "Epoch [13], Batch [71/157], test Loss: 0.7100\n",
            "Epoch [13], Batch [72/157], test Loss: 0.5785\n",
            "Epoch [13], Batch [73/157], test Loss: 0.3352\n",
            "Epoch [13], Batch [74/157], test Loss: 0.3792\n",
            "Epoch [13], Batch [75/157], test Loss: 0.5532\n",
            "Epoch [13], Batch [76/157], test Loss: 0.5173\n",
            "Epoch [13], Batch [77/157], test Loss: 0.3989\n",
            "Epoch [13], Batch [78/157], test Loss: 0.5262\n",
            "Epoch [13], Batch [79/157], test Loss: 0.4316\n",
            "Epoch [13], Batch [80/157], test Loss: 0.3869\n",
            "Epoch [13], Batch [81/157], test Loss: 0.4401\n",
            "Epoch [13], Batch [82/157], test Loss: 0.3010\n",
            "Epoch [13], Batch [83/157], test Loss: 0.3744\n",
            "Epoch [13], Batch [84/157], test Loss: 0.6553\n",
            "Epoch [13], Batch [85/157], test Loss: 0.4174\n",
            "Epoch [13], Batch [86/157], test Loss: 0.4026\n",
            "Epoch [13], Batch [87/157], test Loss: 0.6050\n",
            "Epoch [13], Batch [88/157], test Loss: 0.6257\n",
            "Epoch [13], Batch [89/157], test Loss: 0.5165\n",
            "Epoch [13], Batch [90/157], test Loss: 0.3039\n",
            "Epoch [13], Batch [91/157], test Loss: 0.4015\n",
            "Epoch [13], Batch [92/157], test Loss: 0.3997\n",
            "Epoch [13], Batch [93/157], test Loss: 0.3866\n",
            "Epoch [13], Batch [94/157], test Loss: 0.4959\n",
            "Epoch [13], Batch [95/157], test Loss: 0.5379\n",
            "Epoch [13], Batch [96/157], test Loss: 0.4404\n",
            "Epoch [13], Batch [97/157], test Loss: 0.4389\n",
            "Epoch [13], Batch [98/157], test Loss: 0.4695\n",
            "Epoch [13], Batch [99/157], test Loss: 0.5923\n",
            "Epoch [13], Batch [100/157], test Loss: 0.4564\n",
            "Epoch [13], Batch [101/157], test Loss: 0.5398\n",
            "Epoch [13], Batch [102/157], test Loss: 0.4514\n",
            "Epoch [13], Batch [103/157], test Loss: 0.6538\n",
            "Epoch [13], Batch [104/157], test Loss: 0.5999\n",
            "Epoch [13], Batch [105/157], test Loss: 0.6392\n",
            "Epoch [13], Batch [106/157], test Loss: 0.5309\n",
            "Epoch [13], Batch [107/157], test Loss: 0.3411\n",
            "Epoch [13], Batch [108/157], test Loss: 0.4147\n",
            "Epoch [13], Batch [109/157], test Loss: 0.6053\n",
            "Epoch [13], Batch [110/157], test Loss: 0.4554\n",
            "Epoch [13], Batch [111/157], test Loss: 0.5141\n",
            "Epoch [13], Batch [112/157], test Loss: 0.6139\n",
            "Epoch [13], Batch [113/157], test Loss: 0.3187\n",
            "Epoch [13], Batch [114/157], test Loss: 0.5905\n",
            "Epoch [13], Batch [115/157], test Loss: 0.5368\n",
            "Epoch [13], Batch [116/157], test Loss: 0.6472\n",
            "Epoch [13], Batch [117/157], test Loss: 0.5329\n",
            "Epoch [13], Batch [118/157], test Loss: 0.4730\n",
            "Epoch [13], Batch [119/157], test Loss: 0.6773\n",
            "Epoch [13], Batch [120/157], test Loss: 0.4454\n",
            "Epoch [13], Batch [121/157], test Loss: 0.3752\n",
            "Epoch [13], Batch [122/157], test Loss: 0.3964\n",
            "Epoch [13], Batch [123/157], test Loss: 0.5293\n",
            "Epoch [13], Batch [124/157], test Loss: 0.4084\n",
            "Epoch [13], Batch [125/157], test Loss: 0.4138\n",
            "Epoch [13], Batch [126/157], test Loss: 0.6995\n",
            "Epoch [13], Batch [127/157], test Loss: 0.5280\n",
            "Epoch [13], Batch [128/157], test Loss: 0.4403\n",
            "Epoch [13], Batch [129/157], test Loss: 0.7368\n",
            "Epoch [13], Batch [130/157], test Loss: 0.6405\n",
            "Epoch [13], Batch [131/157], test Loss: 0.4886\n",
            "Epoch [13], Batch [132/157], test Loss: 0.4573\n",
            "Epoch [13], Batch [133/157], test Loss: 0.5306\n",
            "Epoch [13], Batch [134/157], test Loss: 0.3752\n",
            "Epoch [13], Batch [135/157], test Loss: 0.5784\n",
            "Epoch [13], Batch [136/157], test Loss: 0.5131\n",
            "Epoch [13], Batch [137/157], test Loss: 0.4490\n",
            "Epoch [13], Batch [138/157], test Loss: 0.5893\n",
            "Epoch [13], Batch [139/157], test Loss: 0.3821\n",
            "Epoch [13], Batch [140/157], test Loss: 0.4011\n",
            "Epoch [13], Batch [141/157], test Loss: 0.5494\n",
            "Epoch [13], Batch [142/157], test Loss: 0.5958\n",
            "Epoch [13], Batch [143/157], test Loss: 0.5270\n",
            "Epoch [13], Batch [144/157], test Loss: 0.5444\n",
            "Epoch [13], Batch [145/157], test Loss: 0.5142\n",
            "Epoch [13], Batch [146/157], test Loss: 0.5219\n",
            "Epoch [13], Batch [147/157], test Loss: 0.5034\n",
            "Epoch [13], Batch [148/157], test Loss: 0.7953\n",
            "Epoch [13], Batch [149/157], test Loss: 0.7221\n",
            "Epoch [13], Batch [150/157], test Loss: 0.5140\n",
            "Epoch [13], Batch [151/157], test Loss: 0.9818\n",
            "Epoch [13], Batch [152/157], test Loss: 0.3030\n",
            "Epoch [13], Batch [153/157], test Loss: 0.4110\n",
            "Epoch [13], Batch [154/157], test Loss: 0.3995\n",
            "Epoch [13], Batch [155/157], test Loss: 0.6103\n",
            "Epoch [13], Batch [156/157], test Loss: 0.4356\n",
            "Epoch [13], Batch [157/157], test Loss: 0.3895\n",
            "Accuracy of test set: 81.66%\n",
            "Epoch [14/25] - Train Loss: 0.4753, Train Accuracy: 83.34% - Test Loss: 0.5057, Test Accuracy: 81.66%\n",
            "Epoch [14], Batch [10/938], Loss: 0.3156\n",
            "Epoch [14], Batch [20/938], Loss: 0.6501\n",
            "Epoch [14], Batch [30/938], Loss: 0.6015\n",
            "Epoch [14], Batch [40/938], Loss: 0.4876\n",
            "Epoch [14], Batch [50/938], Loss: 0.4198\n",
            "Epoch [14], Batch [60/938], Loss: 0.3270\n",
            "Epoch [14], Batch [70/938], Loss: 0.5006\n",
            "Epoch [14], Batch [80/938], Loss: 0.4886\n",
            "Epoch [14], Batch [90/938], Loss: 0.6109\n",
            "Epoch [14], Batch [100/938], Loss: 0.4906\n",
            "Epoch [14], Batch [110/938], Loss: 0.5769\n",
            "Epoch [14], Batch [120/938], Loss: 0.6473\n",
            "Epoch [14], Batch [130/938], Loss: 0.3903\n",
            "Epoch [14], Batch [140/938], Loss: 0.5108\n",
            "Epoch [14], Batch [150/938], Loss: 0.4103\n",
            "Epoch [14], Batch [160/938], Loss: 0.4460\n",
            "Epoch [14], Batch [170/938], Loss: 0.5647\n",
            "Epoch [14], Batch [180/938], Loss: 0.3249\n",
            "Epoch [14], Batch [190/938], Loss: 0.3488\n",
            "Epoch [14], Batch [200/938], Loss: 0.6406\n",
            "Epoch [14], Batch [210/938], Loss: 0.3328\n",
            "Epoch [14], Batch [220/938], Loss: 0.3825\n",
            "Epoch [14], Batch [230/938], Loss: 0.4056\n",
            "Epoch [14], Batch [240/938], Loss: 0.3217\n",
            "Epoch [14], Batch [250/938], Loss: 0.5936\n",
            "Epoch [14], Batch [260/938], Loss: 0.4463\n",
            "Epoch [14], Batch [270/938], Loss: 0.3954\n",
            "Epoch [14], Batch [280/938], Loss: 0.4013\n",
            "Epoch [14], Batch [290/938], Loss: 0.3988\n",
            "Epoch [14], Batch [300/938], Loss: 0.5110\n",
            "Epoch [14], Batch [310/938], Loss: 0.3523\n",
            "Epoch [14], Batch [320/938], Loss: 0.4090\n",
            "Epoch [14], Batch [330/938], Loss: 0.2531\n",
            "Epoch [14], Batch [340/938], Loss: 0.5035\n",
            "Epoch [14], Batch [350/938], Loss: 0.4529\n",
            "Epoch [14], Batch [360/938], Loss: 0.5173\n",
            "Epoch [14], Batch [370/938], Loss: 0.4495\n",
            "Epoch [14], Batch [380/938], Loss: 0.5052\n",
            "Epoch [14], Batch [390/938], Loss: 0.4447\n",
            "Epoch [14], Batch [400/938], Loss: 0.4358\n",
            "Epoch [14], Batch [410/938], Loss: 0.3985\n",
            "Epoch [14], Batch [420/938], Loss: 0.4233\n",
            "Epoch [14], Batch [430/938], Loss: 0.5706\n",
            "Epoch [14], Batch [440/938], Loss: 0.3482\n",
            "Epoch [14], Batch [450/938], Loss: 0.4455\n",
            "Epoch [14], Batch [460/938], Loss: 0.4090\n",
            "Epoch [14], Batch [470/938], Loss: 0.4575\n",
            "Epoch [14], Batch [480/938], Loss: 0.4908\n",
            "Epoch [14], Batch [490/938], Loss: 0.6248\n",
            "Epoch [14], Batch [500/938], Loss: 0.5552\n",
            "Epoch [14], Batch [510/938], Loss: 0.5120\n",
            "Epoch [14], Batch [520/938], Loss: 0.5257\n",
            "Epoch [14], Batch [530/938], Loss: 0.5183\n",
            "Epoch [14], Batch [540/938], Loss: 0.3069\n",
            "Epoch [14], Batch [550/938], Loss: 0.3414\n",
            "Epoch [14], Batch [560/938], Loss: 0.3506\n",
            "Epoch [14], Batch [570/938], Loss: 0.4449\n",
            "Epoch [14], Batch [580/938], Loss: 0.4599\n",
            "Epoch [14], Batch [590/938], Loss: 0.3293\n",
            "Epoch [14], Batch [600/938], Loss: 0.4587\n",
            "Epoch [14], Batch [610/938], Loss: 0.4887\n",
            "Epoch [14], Batch [620/938], Loss: 0.3076\n",
            "Epoch [14], Batch [630/938], Loss: 0.4013\n",
            "Epoch [14], Batch [640/938], Loss: 0.5008\n",
            "Epoch [14], Batch [650/938], Loss: 0.4791\n",
            "Epoch [14], Batch [660/938], Loss: 0.4161\n",
            "Epoch [14], Batch [670/938], Loss: 0.4774\n",
            "Epoch [14], Batch [680/938], Loss: 0.4270\n",
            "Epoch [14], Batch [690/938], Loss: 0.4687\n",
            "Epoch [14], Batch [700/938], Loss: 0.3417\n",
            "Epoch [14], Batch [710/938], Loss: 0.7261\n",
            "Epoch [14], Batch [720/938], Loss: 0.6700\n",
            "Epoch [14], Batch [730/938], Loss: 0.4721\n",
            "Epoch [14], Batch [740/938], Loss: 0.3883\n",
            "Epoch [14], Batch [750/938], Loss: 0.3803\n",
            "Epoch [14], Batch [760/938], Loss: 0.4734\n",
            "Epoch [14], Batch [770/938], Loss: 0.4698\n",
            "Epoch [14], Batch [780/938], Loss: 0.4263\n",
            "Epoch [14], Batch [790/938], Loss: 0.3501\n",
            "Epoch [14], Batch [800/938], Loss: 0.6072\n",
            "Epoch [14], Batch [810/938], Loss: 0.4162\n",
            "Epoch [14], Batch [820/938], Loss: 0.6150\n",
            "Epoch [14], Batch [830/938], Loss: 0.4926\n",
            "Epoch [14], Batch [840/938], Loss: 0.4114\n",
            "Epoch [14], Batch [850/938], Loss: 0.7291\n",
            "Epoch [14], Batch [860/938], Loss: 0.4568\n",
            "Epoch [14], Batch [870/938], Loss: 0.4723\n",
            "Epoch [14], Batch [880/938], Loss: 0.5161\n",
            "Epoch [14], Batch [890/938], Loss: 0.4899\n",
            "Epoch [14], Batch [900/938], Loss: 0.7013\n",
            "Epoch [14], Batch [910/938], Loss: 0.7065\n",
            "Epoch [14], Batch [920/938], Loss: 0.4356\n",
            "Epoch [14], Batch [930/938], Loss: 0.4324\n",
            "Epoch [14], Batch [938/938], Loss: 0.3770\n",
            "Accuracy of train set: 83.73%\n",
            "Epoch [14], Batch [1/157], test Loss: 0.3588\n",
            "Epoch [14], Batch [2/157], test Loss: 0.2580\n",
            "Epoch [14], Batch [3/157], test Loss: 0.6444\n",
            "Epoch [14], Batch [4/157], test Loss: 0.4217\n",
            "Epoch [14], Batch [5/157], test Loss: 0.4260\n",
            "Epoch [14], Batch [6/157], test Loss: 0.5069\n",
            "Epoch [14], Batch [7/157], test Loss: 0.5669\n",
            "Epoch [14], Batch [8/157], test Loss: 0.3992\n",
            "Epoch [14], Batch [9/157], test Loss: 0.3970\n",
            "Epoch [14], Batch [10/157], test Loss: 0.5125\n",
            "Epoch [14], Batch [11/157], test Loss: 0.4387\n",
            "Epoch [14], Batch [12/157], test Loss: 0.4807\n",
            "Epoch [14], Batch [13/157], test Loss: 0.3612\n",
            "Epoch [14], Batch [14/157], test Loss: 0.5034\n",
            "Epoch [14], Batch [15/157], test Loss: 0.3017\n",
            "Epoch [14], Batch [16/157], test Loss: 0.6526\n",
            "Epoch [14], Batch [17/157], test Loss: 0.5254\n",
            "Epoch [14], Batch [18/157], test Loss: 0.6635\n",
            "Epoch [14], Batch [19/157], test Loss: 0.3828\n",
            "Epoch [14], Batch [20/157], test Loss: 0.4958\n",
            "Epoch [14], Batch [21/157], test Loss: 0.4577\n",
            "Epoch [14], Batch [22/157], test Loss: 0.4890\n",
            "Epoch [14], Batch [23/157], test Loss: 0.5809\n",
            "Epoch [14], Batch [24/157], test Loss: 0.3412\n",
            "Epoch [14], Batch [25/157], test Loss: 0.4147\n",
            "Epoch [14], Batch [26/157], test Loss: 0.4906\n",
            "Epoch [14], Batch [27/157], test Loss: 0.2858\n",
            "Epoch [14], Batch [28/157], test Loss: 0.5187\n",
            "Epoch [14], Batch [29/157], test Loss: 0.6408\n",
            "Epoch [14], Batch [30/157], test Loss: 0.5473\n",
            "Epoch [14], Batch [31/157], test Loss: 0.4858\n",
            "Epoch [14], Batch [32/157], test Loss: 0.5399\n",
            "Epoch [14], Batch [33/157], test Loss: 0.4391\n",
            "Epoch [14], Batch [34/157], test Loss: 0.8296\n",
            "Epoch [14], Batch [35/157], test Loss: 0.5338\n",
            "Epoch [14], Batch [36/157], test Loss: 0.5556\n",
            "Epoch [14], Batch [37/157], test Loss: 0.5110\n",
            "Epoch [14], Batch [38/157], test Loss: 0.3687\n",
            "Epoch [14], Batch [39/157], test Loss: 0.4505\n",
            "Epoch [14], Batch [40/157], test Loss: 0.5506\n",
            "Epoch [14], Batch [41/157], test Loss: 0.5872\n",
            "Epoch [14], Batch [42/157], test Loss: 0.4909\n",
            "Epoch [14], Batch [43/157], test Loss: 0.6018\n",
            "Epoch [14], Batch [44/157], test Loss: 0.5169\n",
            "Epoch [14], Batch [45/157], test Loss: 0.5238\n",
            "Epoch [14], Batch [46/157], test Loss: 0.3999\n",
            "Epoch [14], Batch [47/157], test Loss: 0.3891\n",
            "Epoch [14], Batch [48/157], test Loss: 0.4554\n",
            "Epoch [14], Batch [49/157], test Loss: 0.6636\n",
            "Epoch [14], Batch [50/157], test Loss: 0.5845\n",
            "Epoch [14], Batch [51/157], test Loss: 0.5327\n",
            "Epoch [14], Batch [52/157], test Loss: 0.5101\n",
            "Epoch [14], Batch [53/157], test Loss: 0.7002\n",
            "Epoch [14], Batch [54/157], test Loss: 0.3275\n",
            "Epoch [14], Batch [55/157], test Loss: 0.5088\n",
            "Epoch [14], Batch [56/157], test Loss: 0.5624\n",
            "Epoch [14], Batch [57/157], test Loss: 0.5237\n",
            "Epoch [14], Batch [58/157], test Loss: 0.4435\n",
            "Epoch [14], Batch [59/157], test Loss: 0.7251\n",
            "Epoch [14], Batch [60/157], test Loss: 0.4014\n",
            "Epoch [14], Batch [61/157], test Loss: 0.9444\n",
            "Epoch [14], Batch [62/157], test Loss: 0.6207\n",
            "Epoch [14], Batch [63/157], test Loss: 0.4859\n",
            "Epoch [14], Batch [64/157], test Loss: 0.4560\n",
            "Epoch [14], Batch [65/157], test Loss: 0.4653\n",
            "Epoch [14], Batch [66/157], test Loss: 0.3403\n",
            "Epoch [14], Batch [67/157], test Loss: 0.3460\n",
            "Epoch [14], Batch [68/157], test Loss: 0.3769\n",
            "Epoch [14], Batch [69/157], test Loss: 0.6448\n",
            "Epoch [14], Batch [70/157], test Loss: 0.6168\n",
            "Epoch [14], Batch [71/157], test Loss: 0.6861\n",
            "Epoch [14], Batch [72/157], test Loss: 0.4902\n",
            "Epoch [14], Batch [73/157], test Loss: 0.4064\n",
            "Epoch [14], Batch [74/157], test Loss: 0.5350\n",
            "Epoch [14], Batch [75/157], test Loss: 0.4838\n",
            "Epoch [14], Batch [76/157], test Loss: 0.5556\n",
            "Epoch [14], Batch [77/157], test Loss: 0.4026\n",
            "Epoch [14], Batch [78/157], test Loss: 0.3086\n",
            "Epoch [14], Batch [79/157], test Loss: 0.5583\n",
            "Epoch [14], Batch [80/157], test Loss: 0.5136\n",
            "Epoch [14], Batch [81/157], test Loss: 0.5065\n",
            "Epoch [14], Batch [82/157], test Loss: 0.3183\n",
            "Epoch [14], Batch [83/157], test Loss: 0.5953\n",
            "Epoch [14], Batch [84/157], test Loss: 0.4324\n",
            "Epoch [14], Batch [85/157], test Loss: 0.5650\n",
            "Epoch [14], Batch [86/157], test Loss: 0.5256\n",
            "Epoch [14], Batch [87/157], test Loss: 0.5210\n",
            "Epoch [14], Batch [88/157], test Loss: 0.4334\n",
            "Epoch [14], Batch [89/157], test Loss: 0.5503\n",
            "Epoch [14], Batch [90/157], test Loss: 0.5274\n",
            "Epoch [14], Batch [91/157], test Loss: 0.4461\n",
            "Epoch [14], Batch [92/157], test Loss: 0.5171\n",
            "Epoch [14], Batch [93/157], test Loss: 0.7198\n",
            "Epoch [14], Batch [94/157], test Loss: 0.4119\n",
            "Epoch [14], Batch [95/157], test Loss: 0.4458\n",
            "Epoch [14], Batch [96/157], test Loss: 0.5346\n",
            "Epoch [14], Batch [97/157], test Loss: 0.5883\n",
            "Epoch [14], Batch [98/157], test Loss: 0.3766\n",
            "Epoch [14], Batch [99/157], test Loss: 0.5348\n",
            "Epoch [14], Batch [100/157], test Loss: 0.5030\n",
            "Epoch [14], Batch [101/157], test Loss: 0.4601\n",
            "Epoch [14], Batch [102/157], test Loss: 0.4043\n",
            "Epoch [14], Batch [103/157], test Loss: 0.7672\n",
            "Epoch [14], Batch [104/157], test Loss: 0.5440\n",
            "Epoch [14], Batch [105/157], test Loss: 0.7368\n",
            "Epoch [14], Batch [106/157], test Loss: 0.4708\n",
            "Epoch [14], Batch [107/157], test Loss: 0.5851\n",
            "Epoch [14], Batch [108/157], test Loss: 0.6299\n",
            "Epoch [14], Batch [109/157], test Loss: 0.4818\n",
            "Epoch [14], Batch [110/157], test Loss: 0.5033\n",
            "Epoch [14], Batch [111/157], test Loss: 0.8729\n",
            "Epoch [14], Batch [112/157], test Loss: 0.4461\n",
            "Epoch [14], Batch [113/157], test Loss: 0.4822\n",
            "Epoch [14], Batch [114/157], test Loss: 0.3100\n",
            "Epoch [14], Batch [115/157], test Loss: 0.5299\n",
            "Epoch [14], Batch [116/157], test Loss: 0.6115\n",
            "Epoch [14], Batch [117/157], test Loss: 0.5751\n",
            "Epoch [14], Batch [118/157], test Loss: 0.4613\n",
            "Epoch [14], Batch [119/157], test Loss: 0.5032\n",
            "Epoch [14], Batch [120/157], test Loss: 0.3552\n",
            "Epoch [14], Batch [121/157], test Loss: 0.4748\n",
            "Epoch [14], Batch [122/157], test Loss: 0.5509\n",
            "Epoch [14], Batch [123/157], test Loss: 0.6306\n",
            "Epoch [14], Batch [124/157], test Loss: 0.4366\n",
            "Epoch [14], Batch [125/157], test Loss: 0.3799\n",
            "Epoch [14], Batch [126/157], test Loss: 0.6164\n",
            "Epoch [14], Batch [127/157], test Loss: 0.2323\n",
            "Epoch [14], Batch [128/157], test Loss: 0.5287\n",
            "Epoch [14], Batch [129/157], test Loss: 0.8503\n",
            "Epoch [14], Batch [130/157], test Loss: 0.4237\n",
            "Epoch [14], Batch [131/157], test Loss: 0.4217\n",
            "Epoch [14], Batch [132/157], test Loss: 0.4026\n",
            "Epoch [14], Batch [133/157], test Loss: 0.2980\n",
            "Epoch [14], Batch [134/157], test Loss: 0.5699\n",
            "Epoch [14], Batch [135/157], test Loss: 0.6160\n",
            "Epoch [14], Batch [136/157], test Loss: 0.4598\n",
            "Epoch [14], Batch [137/157], test Loss: 0.5786\n",
            "Epoch [14], Batch [138/157], test Loss: 0.6137\n",
            "Epoch [14], Batch [139/157], test Loss: 0.5066\n",
            "Epoch [14], Batch [140/157], test Loss: 0.6440\n",
            "Epoch [14], Batch [141/157], test Loss: 0.4088\n",
            "Epoch [14], Batch [142/157], test Loss: 0.3977\n",
            "Epoch [14], Batch [143/157], test Loss: 0.4695\n",
            "Epoch [14], Batch [144/157], test Loss: 0.5167\n",
            "Epoch [14], Batch [145/157], test Loss: 0.6312\n",
            "Epoch [14], Batch [146/157], test Loss: 0.4336\n",
            "Epoch [14], Batch [147/157], test Loss: 0.5240\n",
            "Epoch [14], Batch [148/157], test Loss: 0.4392\n",
            "Epoch [14], Batch [149/157], test Loss: 0.4795\n",
            "Epoch [14], Batch [150/157], test Loss: 0.6033\n",
            "Epoch [14], Batch [151/157], test Loss: 0.3912\n",
            "Epoch [14], Batch [152/157], test Loss: 0.5045\n",
            "Epoch [14], Batch [153/157], test Loss: 0.3217\n",
            "Epoch [14], Batch [154/157], test Loss: 0.4110\n",
            "Epoch [14], Batch [155/157], test Loss: 0.7584\n",
            "Epoch [14], Batch [156/157], test Loss: 0.5065\n",
            "Epoch [14], Batch [157/157], test Loss: 0.2752\n",
            "Accuracy of test set: 81.48%\n",
            "Epoch [15/25] - Train Loss: 0.4621, Train Accuracy: 83.73% - Test Loss: 0.5039, Test Accuracy: 81.48%\n",
            "Epoch [15], Batch [10/938], Loss: 0.5865\n",
            "Epoch [15], Batch [20/938], Loss: 0.7406\n",
            "Epoch [15], Batch [30/938], Loss: 0.2638\n",
            "Epoch [15], Batch [40/938], Loss: 0.3387\n",
            "Epoch [15], Batch [50/938], Loss: 0.3947\n",
            "Epoch [15], Batch [60/938], Loss: 0.6101\n",
            "Epoch [15], Batch [70/938], Loss: 0.4951\n",
            "Epoch [15], Batch [80/938], Loss: 0.5361\n",
            "Epoch [15], Batch [90/938], Loss: 0.4209\n",
            "Epoch [15], Batch [100/938], Loss: 0.5028\n",
            "Epoch [15], Batch [110/938], Loss: 0.4746\n",
            "Epoch [15], Batch [120/938], Loss: 0.6911\n",
            "Epoch [15], Batch [130/938], Loss: 0.5017\n",
            "Epoch [15], Batch [140/938], Loss: 0.2953\n",
            "Epoch [15], Batch [150/938], Loss: 0.4218\n",
            "Epoch [15], Batch [160/938], Loss: 0.2878\n",
            "Epoch [15], Batch [170/938], Loss: 0.4710\n",
            "Epoch [15], Batch [180/938], Loss: 0.4714\n",
            "Epoch [15], Batch [190/938], Loss: 0.4675\n",
            "Epoch [15], Batch [200/938], Loss: 0.3400\n",
            "Epoch [15], Batch [210/938], Loss: 0.4902\n",
            "Epoch [15], Batch [220/938], Loss: 0.5113\n",
            "Epoch [15], Batch [230/938], Loss: 0.5550\n",
            "Epoch [15], Batch [240/938], Loss: 0.3796\n",
            "Epoch [15], Batch [250/938], Loss: 0.6616\n",
            "Epoch [15], Batch [260/938], Loss: 0.3898\n",
            "Epoch [15], Batch [270/938], Loss: 0.7313\n",
            "Epoch [15], Batch [280/938], Loss: 0.4518\n",
            "Epoch [15], Batch [290/938], Loss: 0.2957\n",
            "Epoch [15], Batch [300/938], Loss: 0.3045\n",
            "Epoch [15], Batch [310/938], Loss: 0.4654\n",
            "Epoch [15], Batch [320/938], Loss: 0.6025\n",
            "Epoch [15], Batch [330/938], Loss: 0.4685\n",
            "Epoch [15], Batch [340/938], Loss: 0.5590\n",
            "Epoch [15], Batch [350/938], Loss: 0.4979\n",
            "Epoch [15], Batch [360/938], Loss: 0.5296\n",
            "Epoch [15], Batch [370/938], Loss: 0.4223\n",
            "Epoch [15], Batch [380/938], Loss: 0.4596\n",
            "Epoch [15], Batch [390/938], Loss: 0.4931\n",
            "Epoch [15], Batch [400/938], Loss: 0.3512\n",
            "Epoch [15], Batch [410/938], Loss: 0.3734\n",
            "Epoch [15], Batch [420/938], Loss: 0.4866\n",
            "Epoch [15], Batch [430/938], Loss: 0.5391\n",
            "Epoch [15], Batch [440/938], Loss: 0.5129\n",
            "Epoch [15], Batch [450/938], Loss: 0.4223\n",
            "Epoch [15], Batch [460/938], Loss: 0.6030\n",
            "Epoch [15], Batch [470/938], Loss: 0.4580\n",
            "Epoch [15], Batch [480/938], Loss: 0.3058\n",
            "Epoch [15], Batch [490/938], Loss: 0.2402\n",
            "Epoch [15], Batch [500/938], Loss: 0.4868\n",
            "Epoch [15], Batch [510/938], Loss: 0.4592\n",
            "Epoch [15], Batch [520/938], Loss: 0.6120\n",
            "Epoch [15], Batch [530/938], Loss: 0.3819\n",
            "Epoch [15], Batch [540/938], Loss: 0.3494\n",
            "Epoch [15], Batch [550/938], Loss: 0.3599\n",
            "Epoch [15], Batch [560/938], Loss: 0.4132\n",
            "Epoch [15], Batch [570/938], Loss: 0.4607\n",
            "Epoch [15], Batch [580/938], Loss: 0.4954\n",
            "Epoch [15], Batch [590/938], Loss: 0.4951\n",
            "Epoch [15], Batch [600/938], Loss: 0.5047\n",
            "Epoch [15], Batch [610/938], Loss: 0.4431\n",
            "Epoch [15], Batch [620/938], Loss: 0.2582\n",
            "Epoch [15], Batch [630/938], Loss: 0.3642\n",
            "Epoch [15], Batch [640/938], Loss: 0.3452\n",
            "Epoch [15], Batch [650/938], Loss: 0.5905\n",
            "Epoch [15], Batch [660/938], Loss: 0.5055\n",
            "Epoch [15], Batch [670/938], Loss: 0.4439\n",
            "Epoch [15], Batch [680/938], Loss: 0.6496\n",
            "Epoch [15], Batch [690/938], Loss: 0.4976\n",
            "Epoch [15], Batch [700/938], Loss: 0.3178\n",
            "Epoch [15], Batch [710/938], Loss: 0.3585\n",
            "Epoch [15], Batch [720/938], Loss: 0.4272\n",
            "Epoch [15], Batch [730/938], Loss: 0.2663\n",
            "Epoch [15], Batch [740/938], Loss: 0.4075\n",
            "Epoch [15], Batch [750/938], Loss: 0.4181\n",
            "Epoch [15], Batch [760/938], Loss: 0.4254\n",
            "Epoch [15], Batch [770/938], Loss: 0.4099\n",
            "Epoch [15], Batch [780/938], Loss: 0.3342\n",
            "Epoch [15], Batch [790/938], Loss: 0.3784\n",
            "Epoch [15], Batch [800/938], Loss: 0.3413\n",
            "Epoch [15], Batch [810/938], Loss: 0.2979\n",
            "Epoch [15], Batch [820/938], Loss: 0.5518\n",
            "Epoch [15], Batch [830/938], Loss: 0.5183\n",
            "Epoch [15], Batch [840/938], Loss: 0.3670\n",
            "Epoch [15], Batch [850/938], Loss: 0.3891\n",
            "Epoch [15], Batch [860/938], Loss: 0.4688\n",
            "Epoch [15], Batch [870/938], Loss: 0.3583\n",
            "Epoch [15], Batch [880/938], Loss: 0.3685\n",
            "Epoch [15], Batch [890/938], Loss: 0.3136\n",
            "Epoch [15], Batch [900/938], Loss: 0.6468\n",
            "Epoch [15], Batch [910/938], Loss: 0.5849\n",
            "Epoch [15], Batch [920/938], Loss: 0.3888\n",
            "Epoch [15], Batch [930/938], Loss: 0.4360\n",
            "Epoch [15], Batch [938/938], Loss: 0.3118\n",
            "Accuracy of train set: 84.12%\n",
            "Epoch [15], Batch [1/157], test Loss: 0.6279\n",
            "Epoch [15], Batch [2/157], test Loss: 0.4052\n",
            "Epoch [15], Batch [3/157], test Loss: 0.5289\n",
            "Epoch [15], Batch [4/157], test Loss: 0.3813\n",
            "Epoch [15], Batch [5/157], test Loss: 0.4371\n",
            "Epoch [15], Batch [6/157], test Loss: 0.4129\n",
            "Epoch [15], Batch [7/157], test Loss: 0.3338\n",
            "Epoch [15], Batch [8/157], test Loss: 0.3845\n",
            "Epoch [15], Batch [9/157], test Loss: 0.3809\n",
            "Epoch [15], Batch [10/157], test Loss: 0.4281\n",
            "Epoch [15], Batch [11/157], test Loss: 0.4599\n",
            "Epoch [15], Batch [12/157], test Loss: 0.3952\n",
            "Epoch [15], Batch [13/157], test Loss: 0.4114\n",
            "Epoch [15], Batch [14/157], test Loss: 0.3418\n",
            "Epoch [15], Batch [15/157], test Loss: 0.2711\n",
            "Epoch [15], Batch [16/157], test Loss: 0.3347\n",
            "Epoch [15], Batch [17/157], test Loss: 0.3373\n",
            "Epoch [15], Batch [18/157], test Loss: 0.6378\n",
            "Epoch [15], Batch [19/157], test Loss: 0.3070\n",
            "Epoch [15], Batch [20/157], test Loss: 0.4947\n",
            "Epoch [15], Batch [21/157], test Loss: 0.5514\n",
            "Epoch [15], Batch [22/157], test Loss: 0.5877\n",
            "Epoch [15], Batch [23/157], test Loss: 0.4154\n",
            "Epoch [15], Batch [24/157], test Loss: 0.3558\n",
            "Epoch [15], Batch [25/157], test Loss: 0.5249\n",
            "Epoch [15], Batch [26/157], test Loss: 0.4434\n",
            "Epoch [15], Batch [27/157], test Loss: 0.4878\n",
            "Epoch [15], Batch [28/157], test Loss: 0.6058\n",
            "Epoch [15], Batch [29/157], test Loss: 0.5860\n",
            "Epoch [15], Batch [30/157], test Loss: 0.3905\n",
            "Epoch [15], Batch [31/157], test Loss: 0.5169\n",
            "Epoch [15], Batch [32/157], test Loss: 0.5011\n",
            "Epoch [15], Batch [33/157], test Loss: 0.5667\n",
            "Epoch [15], Batch [34/157], test Loss: 0.3699\n",
            "Epoch [15], Batch [35/157], test Loss: 0.4091\n",
            "Epoch [15], Batch [36/157], test Loss: 0.5138\n",
            "Epoch [15], Batch [37/157], test Loss: 0.4195\n",
            "Epoch [15], Batch [38/157], test Loss: 0.5936\n",
            "Epoch [15], Batch [39/157], test Loss: 0.6451\n",
            "Epoch [15], Batch [40/157], test Loss: 0.4487\n",
            "Epoch [15], Batch [41/157], test Loss: 0.3765\n",
            "Epoch [15], Batch [42/157], test Loss: 0.3833\n",
            "Epoch [15], Batch [43/157], test Loss: 0.4357\n",
            "Epoch [15], Batch [44/157], test Loss: 0.4282\n",
            "Epoch [15], Batch [45/157], test Loss: 0.4263\n",
            "Epoch [15], Batch [46/157], test Loss: 0.4360\n",
            "Epoch [15], Batch [47/157], test Loss: 0.5478\n",
            "Epoch [15], Batch [48/157], test Loss: 0.6372\n",
            "Epoch [15], Batch [49/157], test Loss: 0.3068\n",
            "Epoch [15], Batch [50/157], test Loss: 0.5969\n",
            "Epoch [15], Batch [51/157], test Loss: 0.4822\n",
            "Epoch [15], Batch [52/157], test Loss: 0.4758\n",
            "Epoch [15], Batch [53/157], test Loss: 0.2860\n",
            "Epoch [15], Batch [54/157], test Loss: 0.5032\n",
            "Epoch [15], Batch [55/157], test Loss: 0.3458\n",
            "Epoch [15], Batch [56/157], test Loss: 0.5702\n",
            "Epoch [15], Batch [57/157], test Loss: 0.5226\n",
            "Epoch [15], Batch [58/157], test Loss: 0.4983\n",
            "Epoch [15], Batch [59/157], test Loss: 0.5126\n",
            "Epoch [15], Batch [60/157], test Loss: 0.4674\n",
            "Epoch [15], Batch [61/157], test Loss: 0.6021\n",
            "Epoch [15], Batch [62/157], test Loss: 0.5181\n",
            "Epoch [15], Batch [63/157], test Loss: 0.5653\n",
            "Epoch [15], Batch [64/157], test Loss: 0.7257\n",
            "Epoch [15], Batch [65/157], test Loss: 0.4209\n",
            "Epoch [15], Batch [66/157], test Loss: 0.5448\n",
            "Epoch [15], Batch [67/157], test Loss: 0.4862\n",
            "Epoch [15], Batch [68/157], test Loss: 0.7797\n",
            "Epoch [15], Batch [69/157], test Loss: 0.8293\n",
            "Epoch [15], Batch [70/157], test Loss: 0.4526\n",
            "Epoch [15], Batch [71/157], test Loss: 0.5468\n",
            "Epoch [15], Batch [72/157], test Loss: 0.4501\n",
            "Epoch [15], Batch [73/157], test Loss: 0.6488\n",
            "Epoch [15], Batch [74/157], test Loss: 0.3043\n",
            "Epoch [15], Batch [75/157], test Loss: 0.5129\n",
            "Epoch [15], Batch [76/157], test Loss: 0.6412\n",
            "Epoch [15], Batch [77/157], test Loss: 0.3381\n",
            "Epoch [15], Batch [78/157], test Loss: 0.6584\n",
            "Epoch [15], Batch [79/157], test Loss: 0.7328\n",
            "Epoch [15], Batch [80/157], test Loss: 0.6118\n",
            "Epoch [15], Batch [81/157], test Loss: 0.3681\n",
            "Epoch [15], Batch [82/157], test Loss: 0.3668\n",
            "Epoch [15], Batch [83/157], test Loss: 0.5412\n",
            "Epoch [15], Batch [84/157], test Loss: 0.4662\n",
            "Epoch [15], Batch [85/157], test Loss: 0.4198\n",
            "Epoch [15], Batch [86/157], test Loss: 0.4991\n",
            "Epoch [15], Batch [87/157], test Loss: 0.5182\n",
            "Epoch [15], Batch [88/157], test Loss: 0.4638\n",
            "Epoch [15], Batch [89/157], test Loss: 0.3261\n",
            "Epoch [15], Batch [90/157], test Loss: 0.5652\n",
            "Epoch [15], Batch [91/157], test Loss: 0.5464\n",
            "Epoch [15], Batch [92/157], test Loss: 0.3742\n",
            "Epoch [15], Batch [93/157], test Loss: 0.4271\n",
            "Epoch [15], Batch [94/157], test Loss: 0.4822\n",
            "Epoch [15], Batch [95/157], test Loss: 0.4748\n",
            "Epoch [15], Batch [96/157], test Loss: 0.6314\n",
            "Epoch [15], Batch [97/157], test Loss: 0.4281\n",
            "Epoch [15], Batch [98/157], test Loss: 0.4966\n",
            "Epoch [15], Batch [99/157], test Loss: 0.4935\n",
            "Epoch [15], Batch [100/157], test Loss: 0.2746\n",
            "Epoch [15], Batch [101/157], test Loss: 0.3694\n",
            "Epoch [15], Batch [102/157], test Loss: 0.9576\n",
            "Epoch [15], Batch [103/157], test Loss: 0.3768\n",
            "Epoch [15], Batch [104/157], test Loss: 0.3920\n",
            "Epoch [15], Batch [105/157], test Loss: 0.3268\n",
            "Epoch [15], Batch [106/157], test Loss: 0.4479\n",
            "Epoch [15], Batch [107/157], test Loss: 0.4047\n",
            "Epoch [15], Batch [108/157], test Loss: 0.5334\n",
            "Epoch [15], Batch [109/157], test Loss: 0.3730\n",
            "Epoch [15], Batch [110/157], test Loss: 0.4534\n",
            "Epoch [15], Batch [111/157], test Loss: 0.4103\n",
            "Epoch [15], Batch [112/157], test Loss: 0.5035\n",
            "Epoch [15], Batch [113/157], test Loss: 0.7255\n",
            "Epoch [15], Batch [114/157], test Loss: 0.4301\n",
            "Epoch [15], Batch [115/157], test Loss: 0.7894\n",
            "Epoch [15], Batch [116/157], test Loss: 0.3750\n",
            "Epoch [15], Batch [117/157], test Loss: 0.6285\n",
            "Epoch [15], Batch [118/157], test Loss: 0.6030\n",
            "Epoch [15], Batch [119/157], test Loss: 0.5141\n",
            "Epoch [15], Batch [120/157], test Loss: 0.4356\n",
            "Epoch [15], Batch [121/157], test Loss: 0.5305\n",
            "Epoch [15], Batch [122/157], test Loss: 0.6198\n",
            "Epoch [15], Batch [123/157], test Loss: 0.6447\n",
            "Epoch [15], Batch [124/157], test Loss: 0.7088\n",
            "Epoch [15], Batch [125/157], test Loss: 0.5041\n",
            "Epoch [15], Batch [126/157], test Loss: 0.3669\n",
            "Epoch [15], Batch [127/157], test Loss: 0.3676\n",
            "Epoch [15], Batch [128/157], test Loss: 0.5757\n",
            "Epoch [15], Batch [129/157], test Loss: 0.5524\n",
            "Epoch [15], Batch [130/157], test Loss: 0.3876\n",
            "Epoch [15], Batch [131/157], test Loss: 0.4276\n",
            "Epoch [15], Batch [132/157], test Loss: 0.3679\n",
            "Epoch [15], Batch [133/157], test Loss: 0.5055\n",
            "Epoch [15], Batch [134/157], test Loss: 0.6245\n",
            "Epoch [15], Batch [135/157], test Loss: 0.6898\n",
            "Epoch [15], Batch [136/157], test Loss: 0.2850\n",
            "Epoch [15], Batch [137/157], test Loss: 0.4094\n",
            "Epoch [15], Batch [138/157], test Loss: 0.4761\n",
            "Epoch [15], Batch [139/157], test Loss: 0.3540\n",
            "Epoch [15], Batch [140/157], test Loss: 0.4064\n",
            "Epoch [15], Batch [141/157], test Loss: 0.3292\n",
            "Epoch [15], Batch [142/157], test Loss: 0.3938\n",
            "Epoch [15], Batch [143/157], test Loss: 0.3189\n",
            "Epoch [15], Batch [144/157], test Loss: 0.6296\n",
            "Epoch [15], Batch [145/157], test Loss: 0.3255\n",
            "Epoch [15], Batch [146/157], test Loss: 0.5137\n",
            "Epoch [15], Batch [147/157], test Loss: 0.2924\n",
            "Epoch [15], Batch [148/157], test Loss: 0.2472\n",
            "Epoch [15], Batch [149/157], test Loss: 0.4859\n",
            "Epoch [15], Batch [150/157], test Loss: 0.4853\n",
            "Epoch [15], Batch [151/157], test Loss: 0.5848\n",
            "Epoch [15], Batch [152/157], test Loss: 0.5298\n",
            "Epoch [15], Batch [153/157], test Loss: 0.4149\n",
            "Epoch [15], Batch [154/157], test Loss: 0.4170\n",
            "Epoch [15], Batch [155/157], test Loss: 0.4893\n",
            "Epoch [15], Batch [156/157], test Loss: 0.4074\n",
            "Epoch [15], Batch [157/157], test Loss: 0.6399\n",
            "Accuracy of test set: 82.96%\n",
            "Epoch [16/25] - Train Loss: 0.4497, Train Accuracy: 84.12% - Test Loss: 0.4801, Test Accuracy: 82.96%\n",
            "Epoch [16], Batch [10/938], Loss: 0.5222\n",
            "Epoch [16], Batch [20/938], Loss: 0.4059\n",
            "Epoch [16], Batch [30/938], Loss: 0.5207\n",
            "Epoch [16], Batch [40/938], Loss: 0.4001\n",
            "Epoch [16], Batch [50/938], Loss: 0.3920\n",
            "Epoch [16], Batch [60/938], Loss: 0.4929\n",
            "Epoch [16], Batch [70/938], Loss: 0.3819\n",
            "Epoch [16], Batch [80/938], Loss: 0.4050\n",
            "Epoch [16], Batch [90/938], Loss: 0.5367\n",
            "Epoch [16], Batch [100/938], Loss: 0.4339\n",
            "Epoch [16], Batch [110/938], Loss: 0.5192\n",
            "Epoch [16], Batch [120/938], Loss: 0.5406\n",
            "Epoch [16], Batch [130/938], Loss: 0.4793\n",
            "Epoch [16], Batch [140/938], Loss: 0.4682\n",
            "Epoch [16], Batch [150/938], Loss: 0.2998\n",
            "Epoch [16], Batch [160/938], Loss: 0.3472\n",
            "Epoch [16], Batch [170/938], Loss: 0.4311\n",
            "Epoch [16], Batch [180/938], Loss: 0.4365\n",
            "Epoch [16], Batch [190/938], Loss: 0.4700\n",
            "Epoch [16], Batch [200/938], Loss: 0.6555\n",
            "Epoch [16], Batch [210/938], Loss: 0.4115\n",
            "Epoch [16], Batch [220/938], Loss: 0.3922\n",
            "Epoch [16], Batch [230/938], Loss: 0.3922\n",
            "Epoch [16], Batch [240/938], Loss: 0.4986\n",
            "Epoch [16], Batch [250/938], Loss: 0.2764\n",
            "Epoch [16], Batch [260/938], Loss: 0.4063\n",
            "Epoch [16], Batch [270/938], Loss: 0.5998\n",
            "Epoch [16], Batch [280/938], Loss: 0.3303\n",
            "Epoch [16], Batch [290/938], Loss: 0.1860\n",
            "Epoch [16], Batch [300/938], Loss: 0.3063\n",
            "Epoch [16], Batch [310/938], Loss: 0.5722\n",
            "Epoch [16], Batch [320/938], Loss: 0.3211\n",
            "Epoch [16], Batch [330/938], Loss: 0.3389\n",
            "Epoch [16], Batch [340/938], Loss: 0.4108\n",
            "Epoch [16], Batch [350/938], Loss: 0.6173\n",
            "Epoch [16], Batch [360/938], Loss: 0.4365\n",
            "Epoch [16], Batch [370/938], Loss: 0.3266\n",
            "Epoch [16], Batch [380/938], Loss: 0.2949\n",
            "Epoch [16], Batch [390/938], Loss: 0.4430\n",
            "Epoch [16], Batch [400/938], Loss: 0.3979\n",
            "Epoch [16], Batch [410/938], Loss: 0.6157\n",
            "Epoch [16], Batch [420/938], Loss: 0.4429\n",
            "Epoch [16], Batch [430/938], Loss: 0.4154\n",
            "Epoch [16], Batch [440/938], Loss: 0.4603\n",
            "Epoch [16], Batch [450/938], Loss: 0.4335\n",
            "Epoch [16], Batch [460/938], Loss: 0.6122\n",
            "Epoch [16], Batch [470/938], Loss: 0.5270\n",
            "Epoch [16], Batch [480/938], Loss: 0.3404\n",
            "Epoch [16], Batch [490/938], Loss: 0.5019\n",
            "Epoch [16], Batch [500/938], Loss: 0.3509\n",
            "Epoch [16], Batch [510/938], Loss: 0.3653\n",
            "Epoch [16], Batch [520/938], Loss: 0.3353\n",
            "Epoch [16], Batch [530/938], Loss: 0.4899\n",
            "Epoch [16], Batch [540/938], Loss: 0.5691\n",
            "Epoch [16], Batch [550/938], Loss: 0.4437\n",
            "Epoch [16], Batch [560/938], Loss: 0.3660\n",
            "Epoch [16], Batch [570/938], Loss: 0.4617\n",
            "Epoch [16], Batch [580/938], Loss: 0.5651\n",
            "Epoch [16], Batch [590/938], Loss: 0.4597\n",
            "Epoch [16], Batch [600/938], Loss: 0.4204\n",
            "Epoch [16], Batch [610/938], Loss: 0.3749\n",
            "Epoch [16], Batch [620/938], Loss: 0.3697\n",
            "Epoch [16], Batch [630/938], Loss: 0.4478\n",
            "Epoch [16], Batch [640/938], Loss: 0.3765\n",
            "Epoch [16], Batch [650/938], Loss: 0.4501\n",
            "Epoch [16], Batch [660/938], Loss: 0.3225\n",
            "Epoch [16], Batch [670/938], Loss: 0.4346\n",
            "Epoch [16], Batch [680/938], Loss: 0.4497\n",
            "Epoch [16], Batch [690/938], Loss: 0.3336\n",
            "Epoch [16], Batch [700/938], Loss: 0.3169\n",
            "Epoch [16], Batch [710/938], Loss: 0.3562\n",
            "Epoch [16], Batch [720/938], Loss: 0.3060\n",
            "Epoch [16], Batch [730/938], Loss: 0.4122\n",
            "Epoch [16], Batch [740/938], Loss: 0.3306\n",
            "Epoch [16], Batch [750/938], Loss: 0.3298\n",
            "Epoch [16], Batch [760/938], Loss: 0.5377\n",
            "Epoch [16], Batch [770/938], Loss: 0.5608\n",
            "Epoch [16], Batch [780/938], Loss: 0.4358\n",
            "Epoch [16], Batch [790/938], Loss: 0.5256\n",
            "Epoch [16], Batch [800/938], Loss: 0.3677\n",
            "Epoch [16], Batch [810/938], Loss: 0.5116\n",
            "Epoch [16], Batch [820/938], Loss: 0.6031\n",
            "Epoch [16], Batch [830/938], Loss: 0.4904\n",
            "Epoch [16], Batch [840/938], Loss: 0.3787\n",
            "Epoch [16], Batch [850/938], Loss: 0.3991\n",
            "Epoch [16], Batch [860/938], Loss: 0.3832\n",
            "Epoch [16], Batch [870/938], Loss: 0.3913\n",
            "Epoch [16], Batch [880/938], Loss: 0.3978\n",
            "Epoch [16], Batch [890/938], Loss: 0.3208\n",
            "Epoch [16], Batch [900/938], Loss: 0.4135\n",
            "Epoch [16], Batch [910/938], Loss: 0.3160\n",
            "Epoch [16], Batch [920/938], Loss: 0.3682\n",
            "Epoch [16], Batch [930/938], Loss: 0.3140\n",
            "Epoch [16], Batch [938/938], Loss: 0.6313\n",
            "Accuracy of train set: 84.42%\n",
            "Epoch [16], Batch [1/157], test Loss: 0.6094\n",
            "Epoch [16], Batch [2/157], test Loss: 0.5543\n",
            "Epoch [16], Batch [3/157], test Loss: 0.4432\n",
            "Epoch [16], Batch [4/157], test Loss: 0.4495\n",
            "Epoch [16], Batch [5/157], test Loss: 0.5080\n",
            "Epoch [16], Batch [6/157], test Loss: 0.4796\n",
            "Epoch [16], Batch [7/157], test Loss: 0.5042\n",
            "Epoch [16], Batch [8/157], test Loss: 0.4444\n",
            "Epoch [16], Batch [9/157], test Loss: 0.3677\n",
            "Epoch [16], Batch [10/157], test Loss: 0.5603\n",
            "Epoch [16], Batch [11/157], test Loss: 0.4431\n",
            "Epoch [16], Batch [12/157], test Loss: 0.3150\n",
            "Epoch [16], Batch [13/157], test Loss: 0.4924\n",
            "Epoch [16], Batch [14/157], test Loss: 0.3887\n",
            "Epoch [16], Batch [15/157], test Loss: 0.3703\n",
            "Epoch [16], Batch [16/157], test Loss: 0.2992\n",
            "Epoch [16], Batch [17/157], test Loss: 0.6505\n",
            "Epoch [16], Batch [18/157], test Loss: 0.3371\n",
            "Epoch [16], Batch [19/157], test Loss: 0.3975\n",
            "Epoch [16], Batch [20/157], test Loss: 0.4006\n",
            "Epoch [16], Batch [21/157], test Loss: 0.4970\n",
            "Epoch [16], Batch [22/157], test Loss: 0.5229\n",
            "Epoch [16], Batch [23/157], test Loss: 0.4816\n",
            "Epoch [16], Batch [24/157], test Loss: 0.3180\n",
            "Epoch [16], Batch [25/157], test Loss: 0.3678\n",
            "Epoch [16], Batch [26/157], test Loss: 0.4736\n",
            "Epoch [16], Batch [27/157], test Loss: 0.3466\n",
            "Epoch [16], Batch [28/157], test Loss: 0.6337\n",
            "Epoch [16], Batch [29/157], test Loss: 0.4612\n",
            "Epoch [16], Batch [30/157], test Loss: 0.6225\n",
            "Epoch [16], Batch [31/157], test Loss: 0.5365\n",
            "Epoch [16], Batch [32/157], test Loss: 0.4219\n",
            "Epoch [16], Batch [33/157], test Loss: 0.5102\n",
            "Epoch [16], Batch [34/157], test Loss: 0.4297\n",
            "Epoch [16], Batch [35/157], test Loss: 0.4454\n",
            "Epoch [16], Batch [36/157], test Loss: 0.3539\n",
            "Epoch [16], Batch [37/157], test Loss: 0.3533\n",
            "Epoch [16], Batch [38/157], test Loss: 0.4396\n",
            "Epoch [16], Batch [39/157], test Loss: 0.4740\n",
            "Epoch [16], Batch [40/157], test Loss: 0.4685\n",
            "Epoch [16], Batch [41/157], test Loss: 0.4895\n",
            "Epoch [16], Batch [42/157], test Loss: 0.5835\n",
            "Epoch [16], Batch [43/157], test Loss: 0.3704\n",
            "Epoch [16], Batch [44/157], test Loss: 0.4333\n",
            "Epoch [16], Batch [45/157], test Loss: 0.3568\n",
            "Epoch [16], Batch [46/157], test Loss: 0.3365\n",
            "Epoch [16], Batch [47/157], test Loss: 0.3237\n",
            "Epoch [16], Batch [48/157], test Loss: 0.3001\n",
            "Epoch [16], Batch [49/157], test Loss: 0.4471\n",
            "Epoch [16], Batch [50/157], test Loss: 0.4500\n",
            "Epoch [16], Batch [51/157], test Loss: 0.5472\n",
            "Epoch [16], Batch [52/157], test Loss: 0.5009\n",
            "Epoch [16], Batch [53/157], test Loss: 0.5079\n",
            "Epoch [16], Batch [54/157], test Loss: 0.3640\n",
            "Epoch [16], Batch [55/157], test Loss: 0.3756\n",
            "Epoch [16], Batch [56/157], test Loss: 0.7242\n",
            "Epoch [16], Batch [57/157], test Loss: 0.6464\n",
            "Epoch [16], Batch [58/157], test Loss: 0.6069\n",
            "Epoch [16], Batch [59/157], test Loss: 0.5781\n",
            "Epoch [16], Batch [60/157], test Loss: 0.4623\n",
            "Epoch [16], Batch [61/157], test Loss: 0.6060\n",
            "Epoch [16], Batch [62/157], test Loss: 0.5965\n",
            "Epoch [16], Batch [63/157], test Loss: 0.3847\n",
            "Epoch [16], Batch [64/157], test Loss: 0.4600\n",
            "Epoch [16], Batch [65/157], test Loss: 0.6081\n",
            "Epoch [16], Batch [66/157], test Loss: 0.3617\n",
            "Epoch [16], Batch [67/157], test Loss: 0.2966\n",
            "Epoch [16], Batch [68/157], test Loss: 0.2930\n",
            "Epoch [16], Batch [69/157], test Loss: 0.4940\n",
            "Epoch [16], Batch [70/157], test Loss: 0.4001\n",
            "Epoch [16], Batch [71/157], test Loss: 0.4803\n",
            "Epoch [16], Batch [72/157], test Loss: 0.4155\n",
            "Epoch [16], Batch [73/157], test Loss: 0.4139\n",
            "Epoch [16], Batch [74/157], test Loss: 0.4403\n",
            "Epoch [16], Batch [75/157], test Loss: 0.4787\n",
            "Epoch [16], Batch [76/157], test Loss: 0.5757\n",
            "Epoch [16], Batch [77/157], test Loss: 0.2922\n",
            "Epoch [16], Batch [78/157], test Loss: 0.5876\n",
            "Epoch [16], Batch [79/157], test Loss: 0.4081\n",
            "Epoch [16], Batch [80/157], test Loss: 0.6171\n",
            "Epoch [16], Batch [81/157], test Loss: 0.4185\n",
            "Epoch [16], Batch [82/157], test Loss: 0.4621\n",
            "Epoch [16], Batch [83/157], test Loss: 0.6359\n",
            "Epoch [16], Batch [84/157], test Loss: 0.4076\n",
            "Epoch [16], Batch [85/157], test Loss: 0.4326\n",
            "Epoch [16], Batch [86/157], test Loss: 0.5048\n",
            "Epoch [16], Batch [87/157], test Loss: 0.5248\n",
            "Epoch [16], Batch [88/157], test Loss: 0.2642\n",
            "Epoch [16], Batch [89/157], test Loss: 0.4516\n",
            "Epoch [16], Batch [90/157], test Loss: 0.3500\n",
            "Epoch [16], Batch [91/157], test Loss: 0.4712\n",
            "Epoch [16], Batch [92/157], test Loss: 0.3547\n",
            "Epoch [16], Batch [93/157], test Loss: 0.5831\n",
            "Epoch [16], Batch [94/157], test Loss: 0.4828\n",
            "Epoch [16], Batch [95/157], test Loss: 0.6129\n",
            "Epoch [16], Batch [96/157], test Loss: 0.5904\n",
            "Epoch [16], Batch [97/157], test Loss: 0.3983\n",
            "Epoch [16], Batch [98/157], test Loss: 0.3354\n",
            "Epoch [16], Batch [99/157], test Loss: 0.3805\n",
            "Epoch [16], Batch [100/157], test Loss: 0.4537\n",
            "Epoch [16], Batch [101/157], test Loss: 0.4351\n",
            "Epoch [16], Batch [102/157], test Loss: 0.4060\n",
            "Epoch [16], Batch [103/157], test Loss: 0.2967\n",
            "Epoch [16], Batch [104/157], test Loss: 0.3916\n",
            "Epoch [16], Batch [105/157], test Loss: 0.7366\n",
            "Epoch [16], Batch [106/157], test Loss: 0.3770\n",
            "Epoch [16], Batch [107/157], test Loss: 0.5483\n",
            "Epoch [16], Batch [108/157], test Loss: 0.4414\n",
            "Epoch [16], Batch [109/157], test Loss: 0.6524\n",
            "Epoch [16], Batch [110/157], test Loss: 0.6015\n",
            "Epoch [16], Batch [111/157], test Loss: 0.4398\n",
            "Epoch [16], Batch [112/157], test Loss: 0.3673\n",
            "Epoch [16], Batch [113/157], test Loss: 0.5114\n",
            "Epoch [16], Batch [114/157], test Loss: 0.3698\n",
            "Epoch [16], Batch [115/157], test Loss: 0.3638\n",
            "Epoch [16], Batch [116/157], test Loss: 0.3797\n",
            "Epoch [16], Batch [117/157], test Loss: 0.4652\n",
            "Epoch [16], Batch [118/157], test Loss: 0.4413\n",
            "Epoch [16], Batch [119/157], test Loss: 0.6909\n",
            "Epoch [16], Batch [120/157], test Loss: 0.4523\n",
            "Epoch [16], Batch [121/157], test Loss: 0.3334\n",
            "Epoch [16], Batch [122/157], test Loss: 0.4562\n",
            "Epoch [16], Batch [123/157], test Loss: 0.5400\n",
            "Epoch [16], Batch [124/157], test Loss: 0.4311\n",
            "Epoch [16], Batch [125/157], test Loss: 0.3422\n",
            "Epoch [16], Batch [126/157], test Loss: 0.3102\n",
            "Epoch [16], Batch [127/157], test Loss: 0.4515\n",
            "Epoch [16], Batch [128/157], test Loss: 0.3994\n",
            "Epoch [16], Batch [129/157], test Loss: 0.5559\n",
            "Epoch [16], Batch [130/157], test Loss: 0.3377\n",
            "Epoch [16], Batch [131/157], test Loss: 0.6949\n",
            "Epoch [16], Batch [132/157], test Loss: 0.6201\n",
            "Epoch [16], Batch [133/157], test Loss: 0.4670\n",
            "Epoch [16], Batch [134/157], test Loss: 0.4146\n",
            "Epoch [16], Batch [135/157], test Loss: 0.3552\n",
            "Epoch [16], Batch [136/157], test Loss: 0.5260\n",
            "Epoch [16], Batch [137/157], test Loss: 0.4464\n",
            "Epoch [16], Batch [138/157], test Loss: 0.4869\n",
            "Epoch [16], Batch [139/157], test Loss: 0.5364\n",
            "Epoch [16], Batch [140/157], test Loss: 0.3828\n",
            "Epoch [16], Batch [141/157], test Loss: 0.3337\n",
            "Epoch [16], Batch [142/157], test Loss: 0.3395\n",
            "Epoch [16], Batch [143/157], test Loss: 0.4360\n",
            "Epoch [16], Batch [144/157], test Loss: 0.3670\n",
            "Epoch [16], Batch [145/157], test Loss: 0.4397\n",
            "Epoch [16], Batch [146/157], test Loss: 0.5285\n",
            "Epoch [16], Batch [147/157], test Loss: 0.5511\n",
            "Epoch [16], Batch [148/157], test Loss: 0.3882\n",
            "Epoch [16], Batch [149/157], test Loss: 0.4532\n",
            "Epoch [16], Batch [150/157], test Loss: 0.5472\n",
            "Epoch [16], Batch [151/157], test Loss: 0.7405\n",
            "Epoch [16], Batch [152/157], test Loss: 0.5425\n",
            "Epoch [16], Batch [153/157], test Loss: 0.3897\n",
            "Epoch [16], Batch [154/157], test Loss: 0.4199\n",
            "Epoch [16], Batch [155/157], test Loss: 0.4121\n",
            "Epoch [16], Batch [156/157], test Loss: 0.4756\n",
            "Epoch [16], Batch [157/157], test Loss: 0.3107\n",
            "Accuracy of test set: 83.85%\n",
            "Epoch [17/25] - Train Loss: 0.4384, Train Accuracy: 84.42% - Test Loss: 0.4588, Test Accuracy: 83.85%\n",
            "Epoch [17], Batch [10/938], Loss: 0.2550\n",
            "Epoch [17], Batch [20/938], Loss: 0.6005\n",
            "Epoch [17], Batch [30/938], Loss: 0.3355\n",
            "Epoch [17], Batch [40/938], Loss: 0.5333\n",
            "Epoch [17], Batch [50/938], Loss: 0.3725\n",
            "Epoch [17], Batch [60/938], Loss: 0.4863\n",
            "Epoch [17], Batch [70/938], Loss: 0.5465\n",
            "Epoch [17], Batch [80/938], Loss: 0.2526\n",
            "Epoch [17], Batch [90/938], Loss: 0.3580\n",
            "Epoch [17], Batch [100/938], Loss: 0.3337\n",
            "Epoch [17], Batch [110/938], Loss: 0.3459\n",
            "Epoch [17], Batch [120/938], Loss: 0.3199\n",
            "Epoch [17], Batch [130/938], Loss: 0.4396\n",
            "Epoch [17], Batch [140/938], Loss: 0.4659\n",
            "Epoch [17], Batch [150/938], Loss: 0.2333\n",
            "Epoch [17], Batch [160/938], Loss: 0.4070\n",
            "Epoch [17], Batch [170/938], Loss: 0.4425\n",
            "Epoch [17], Batch [180/938], Loss: 0.3899\n",
            "Epoch [17], Batch [190/938], Loss: 0.4373\n",
            "Epoch [17], Batch [200/938], Loss: 0.4610\n",
            "Epoch [17], Batch [210/938], Loss: 0.4901\n",
            "Epoch [17], Batch [220/938], Loss: 0.2889\n",
            "Epoch [17], Batch [230/938], Loss: 0.4359\n",
            "Epoch [17], Batch [240/938], Loss: 0.3603\n",
            "Epoch [17], Batch [250/938], Loss: 0.4173\n",
            "Epoch [17], Batch [260/938], Loss: 0.5454\n",
            "Epoch [17], Batch [270/938], Loss: 0.3855\n",
            "Epoch [17], Batch [280/938], Loss: 0.5788\n",
            "Epoch [17], Batch [290/938], Loss: 0.4078\n",
            "Epoch [17], Batch [300/938], Loss: 0.5499\n",
            "Epoch [17], Batch [310/938], Loss: 0.3452\n",
            "Epoch [17], Batch [320/938], Loss: 0.5383\n",
            "Epoch [17], Batch [330/938], Loss: 0.5485\n",
            "Epoch [17], Batch [340/938], Loss: 0.5225\n",
            "Epoch [17], Batch [350/938], Loss: 0.6594\n",
            "Epoch [17], Batch [360/938], Loss: 0.4082\n",
            "Epoch [17], Batch [370/938], Loss: 0.3786\n",
            "Epoch [17], Batch [380/938], Loss: 0.5756\n",
            "Epoch [17], Batch [390/938], Loss: 0.5022\n",
            "Epoch [17], Batch [400/938], Loss: 0.4351\n",
            "Epoch [17], Batch [410/938], Loss: 0.4639\n",
            "Epoch [17], Batch [420/938], Loss: 0.5266\n",
            "Epoch [17], Batch [430/938], Loss: 0.4820\n",
            "Epoch [17], Batch [440/938], Loss: 0.5084\n",
            "Epoch [17], Batch [450/938], Loss: 0.3721\n",
            "Epoch [17], Batch [460/938], Loss: 0.3392\n",
            "Epoch [17], Batch [470/938], Loss: 0.3357\n",
            "Epoch [17], Batch [480/938], Loss: 0.6530\n",
            "Epoch [17], Batch [490/938], Loss: 0.5002\n",
            "Epoch [17], Batch [500/938], Loss: 0.5317\n",
            "Epoch [17], Batch [510/938], Loss: 0.4508\n",
            "Epoch [17], Batch [520/938], Loss: 0.4666\n",
            "Epoch [17], Batch [530/938], Loss: 0.3312\n",
            "Epoch [17], Batch [540/938], Loss: 0.6471\n",
            "Epoch [17], Batch [550/938], Loss: 0.4267\n",
            "Epoch [17], Batch [560/938], Loss: 0.4641\n",
            "Epoch [17], Batch [570/938], Loss: 0.5927\n",
            "Epoch [17], Batch [580/938], Loss: 0.5542\n",
            "Epoch [17], Batch [590/938], Loss: 0.4251\n",
            "Epoch [17], Batch [600/938], Loss: 0.4094\n",
            "Epoch [17], Batch [610/938], Loss: 0.5221\n",
            "Epoch [17], Batch [620/938], Loss: 0.4778\n",
            "Epoch [17], Batch [630/938], Loss: 0.3663\n",
            "Epoch [17], Batch [640/938], Loss: 0.3931\n",
            "Epoch [17], Batch [650/938], Loss: 0.5016\n",
            "Epoch [17], Batch [660/938], Loss: 0.1827\n",
            "Epoch [17], Batch [670/938], Loss: 0.6530\n",
            "Epoch [17], Batch [680/938], Loss: 0.4957\n",
            "Epoch [17], Batch [690/938], Loss: 0.4355\n",
            "Epoch [17], Batch [700/938], Loss: 0.5244\n",
            "Epoch [17], Batch [710/938], Loss: 0.2958\n",
            "Epoch [17], Batch [720/938], Loss: 0.5268\n",
            "Epoch [17], Batch [730/938], Loss: 0.4839\n",
            "Epoch [17], Batch [740/938], Loss: 0.3785\n",
            "Epoch [17], Batch [750/938], Loss: 0.3478\n",
            "Epoch [17], Batch [760/938], Loss: 0.2788\n",
            "Epoch [17], Batch [770/938], Loss: 0.3691\n",
            "Epoch [17], Batch [780/938], Loss: 0.3189\n",
            "Epoch [17], Batch [790/938], Loss: 0.4188\n",
            "Epoch [17], Batch [800/938], Loss: 0.4160\n",
            "Epoch [17], Batch [810/938], Loss: 0.3564\n",
            "Epoch [17], Batch [820/938], Loss: 0.2918\n",
            "Epoch [17], Batch [830/938], Loss: 0.4751\n",
            "Epoch [17], Batch [840/938], Loss: 0.3275\n",
            "Epoch [17], Batch [850/938], Loss: 0.4656\n",
            "Epoch [17], Batch [860/938], Loss: 0.3710\n",
            "Epoch [17], Batch [870/938], Loss: 0.4988\n",
            "Epoch [17], Batch [880/938], Loss: 0.4392\n",
            "Epoch [17], Batch [890/938], Loss: 0.4137\n",
            "Epoch [17], Batch [900/938], Loss: 0.2984\n",
            "Epoch [17], Batch [910/938], Loss: 0.3800\n",
            "Epoch [17], Batch [920/938], Loss: 0.5697\n",
            "Epoch [17], Batch [930/938], Loss: 0.4637\n",
            "Epoch [17], Batch [938/938], Loss: 0.6248\n",
            "Accuracy of train set: 84.75%\n",
            "Epoch [17], Batch [1/157], test Loss: 0.4014\n",
            "Epoch [17], Batch [2/157], test Loss: 0.4305\n",
            "Epoch [17], Batch [3/157], test Loss: 0.3599\n",
            "Epoch [17], Batch [4/157], test Loss: 0.5122\n",
            "Epoch [17], Batch [5/157], test Loss: 0.5097\n",
            "Epoch [17], Batch [6/157], test Loss: 0.3926\n",
            "Epoch [17], Batch [7/157], test Loss: 0.4729\n",
            "Epoch [17], Batch [8/157], test Loss: 0.8059\n",
            "Epoch [17], Batch [9/157], test Loss: 0.3358\n",
            "Epoch [17], Batch [10/157], test Loss: 0.6504\n",
            "Epoch [17], Batch [11/157], test Loss: 0.6020\n",
            "Epoch [17], Batch [12/157], test Loss: 0.4328\n",
            "Epoch [17], Batch [13/157], test Loss: 0.2753\n",
            "Epoch [17], Batch [14/157], test Loss: 0.4434\n",
            "Epoch [17], Batch [15/157], test Loss: 0.4828\n",
            "Epoch [17], Batch [16/157], test Loss: 0.3383\n",
            "Epoch [17], Batch [17/157], test Loss: 0.2187\n",
            "Epoch [17], Batch [18/157], test Loss: 0.4036\n",
            "Epoch [17], Batch [19/157], test Loss: 0.6929\n",
            "Epoch [17], Batch [20/157], test Loss: 0.5839\n",
            "Epoch [17], Batch [21/157], test Loss: 0.4782\n",
            "Epoch [17], Batch [22/157], test Loss: 0.5260\n",
            "Epoch [17], Batch [23/157], test Loss: 0.5198\n",
            "Epoch [17], Batch [24/157], test Loss: 0.4924\n",
            "Epoch [17], Batch [25/157], test Loss: 0.4770\n",
            "Epoch [17], Batch [26/157], test Loss: 0.3831\n",
            "Epoch [17], Batch [27/157], test Loss: 0.4592\n",
            "Epoch [17], Batch [28/157], test Loss: 0.3772\n",
            "Epoch [17], Batch [29/157], test Loss: 0.2780\n",
            "Epoch [17], Batch [30/157], test Loss: 0.4603\n",
            "Epoch [17], Batch [31/157], test Loss: 0.2855\n",
            "Epoch [17], Batch [32/157], test Loss: 0.4218\n",
            "Epoch [17], Batch [33/157], test Loss: 0.3625\n",
            "Epoch [17], Batch [34/157], test Loss: 0.5566\n",
            "Epoch [17], Batch [35/157], test Loss: 0.3203\n",
            "Epoch [17], Batch [36/157], test Loss: 0.4848\n",
            "Epoch [17], Batch [37/157], test Loss: 0.5982\n",
            "Epoch [17], Batch [38/157], test Loss: 0.4520\n",
            "Epoch [17], Batch [39/157], test Loss: 0.4272\n",
            "Epoch [17], Batch [40/157], test Loss: 0.4893\n",
            "Epoch [17], Batch [41/157], test Loss: 0.4455\n",
            "Epoch [17], Batch [42/157], test Loss: 0.4234\n",
            "Epoch [17], Batch [43/157], test Loss: 0.5473\n",
            "Epoch [17], Batch [44/157], test Loss: 0.4509\n",
            "Epoch [17], Batch [45/157], test Loss: 0.6130\n",
            "Epoch [17], Batch [46/157], test Loss: 0.4588\n",
            "Epoch [17], Batch [47/157], test Loss: 0.6980\n",
            "Epoch [17], Batch [48/157], test Loss: 0.3836\n",
            "Epoch [17], Batch [49/157], test Loss: 0.4767\n",
            "Epoch [17], Batch [50/157], test Loss: 0.5349\n",
            "Epoch [17], Batch [51/157], test Loss: 0.5347\n",
            "Epoch [17], Batch [52/157], test Loss: 0.3331\n",
            "Epoch [17], Batch [53/157], test Loss: 0.3523\n",
            "Epoch [17], Batch [54/157], test Loss: 0.4702\n",
            "Epoch [17], Batch [55/157], test Loss: 0.3597\n",
            "Epoch [17], Batch [56/157], test Loss: 0.5312\n",
            "Epoch [17], Batch [57/157], test Loss: 0.6393\n",
            "Epoch [17], Batch [58/157], test Loss: 0.4948\n",
            "Epoch [17], Batch [59/157], test Loss: 0.5992\n",
            "Epoch [17], Batch [60/157], test Loss: 0.4589\n",
            "Epoch [17], Batch [61/157], test Loss: 0.3491\n",
            "Epoch [17], Batch [62/157], test Loss: 0.5438\n",
            "Epoch [17], Batch [63/157], test Loss: 0.5988\n",
            "Epoch [17], Batch [64/157], test Loss: 0.2832\n",
            "Epoch [17], Batch [65/157], test Loss: 0.3876\n",
            "Epoch [17], Batch [66/157], test Loss: 0.3397\n",
            "Epoch [17], Batch [67/157], test Loss: 0.5526\n",
            "Epoch [17], Batch [68/157], test Loss: 0.4437\n",
            "Epoch [17], Batch [69/157], test Loss: 0.6424\n",
            "Epoch [17], Batch [70/157], test Loss: 0.4385\n",
            "Epoch [17], Batch [71/157], test Loss: 0.3164\n",
            "Epoch [17], Batch [72/157], test Loss: 0.4970\n",
            "Epoch [17], Batch [73/157], test Loss: 0.5814\n",
            "Epoch [17], Batch [74/157], test Loss: 0.4221\n",
            "Epoch [17], Batch [75/157], test Loss: 0.6145\n",
            "Epoch [17], Batch [76/157], test Loss: 0.4814\n",
            "Epoch [17], Batch [77/157], test Loss: 0.6044\n",
            "Epoch [17], Batch [78/157], test Loss: 0.3325\n",
            "Epoch [17], Batch [79/157], test Loss: 0.5937\n",
            "Epoch [17], Batch [80/157], test Loss: 0.4843\n",
            "Epoch [17], Batch [81/157], test Loss: 0.4383\n",
            "Epoch [17], Batch [82/157], test Loss: 0.3336\n",
            "Epoch [17], Batch [83/157], test Loss: 0.4161\n",
            "Epoch [17], Batch [84/157], test Loss: 0.6549\n",
            "Epoch [17], Batch [85/157], test Loss: 0.4275\n",
            "Epoch [17], Batch [86/157], test Loss: 0.5850\n",
            "Epoch [17], Batch [87/157], test Loss: 0.3061\n",
            "Epoch [17], Batch [88/157], test Loss: 0.4795\n",
            "Epoch [17], Batch [89/157], test Loss: 0.8456\n",
            "Epoch [17], Batch [90/157], test Loss: 0.3653\n",
            "Epoch [17], Batch [91/157], test Loss: 0.5683\n",
            "Epoch [17], Batch [92/157], test Loss: 0.5702\n",
            "Epoch [17], Batch [93/157], test Loss: 0.4324\n",
            "Epoch [17], Batch [94/157], test Loss: 0.4140\n",
            "Epoch [17], Batch [95/157], test Loss: 0.4657\n",
            "Epoch [17], Batch [96/157], test Loss: 0.5298\n",
            "Epoch [17], Batch [97/157], test Loss: 0.4028\n",
            "Epoch [17], Batch [98/157], test Loss: 0.6376\n",
            "Epoch [17], Batch [99/157], test Loss: 0.3968\n",
            "Epoch [17], Batch [100/157], test Loss: 0.4400\n",
            "Epoch [17], Batch [101/157], test Loss: 0.4312\n",
            "Epoch [17], Batch [102/157], test Loss: 0.5632\n",
            "Epoch [17], Batch [103/157], test Loss: 0.4198\n",
            "Epoch [17], Batch [104/157], test Loss: 0.4347\n",
            "Epoch [17], Batch [105/157], test Loss: 0.5182\n",
            "Epoch [17], Batch [106/157], test Loss: 0.4729\n",
            "Epoch [17], Batch [107/157], test Loss: 0.5385\n",
            "Epoch [17], Batch [108/157], test Loss: 0.3085\n",
            "Epoch [17], Batch [109/157], test Loss: 0.6210\n",
            "Epoch [17], Batch [110/157], test Loss: 0.5275\n",
            "Epoch [17], Batch [111/157], test Loss: 0.3857\n",
            "Epoch [17], Batch [112/157], test Loss: 0.5307\n",
            "Epoch [17], Batch [113/157], test Loss: 0.3013\n",
            "Epoch [17], Batch [114/157], test Loss: 0.3672\n",
            "Epoch [17], Batch [115/157], test Loss: 0.3945\n",
            "Epoch [17], Batch [116/157], test Loss: 0.3171\n",
            "Epoch [17], Batch [117/157], test Loss: 0.3779\n",
            "Epoch [17], Batch [118/157], test Loss: 0.3526\n",
            "Epoch [17], Batch [119/157], test Loss: 0.4905\n",
            "Epoch [17], Batch [120/157], test Loss: 0.5374\n",
            "Epoch [17], Batch [121/157], test Loss: 0.5393\n",
            "Epoch [17], Batch [122/157], test Loss: 0.3323\n",
            "Epoch [17], Batch [123/157], test Loss: 0.4603\n",
            "Epoch [17], Batch [124/157], test Loss: 0.3156\n",
            "Epoch [17], Batch [125/157], test Loss: 0.5564\n",
            "Epoch [17], Batch [126/157], test Loss: 0.6910\n",
            "Epoch [17], Batch [127/157], test Loss: 0.4486\n",
            "Epoch [17], Batch [128/157], test Loss: 0.4980\n",
            "Epoch [17], Batch [129/157], test Loss: 0.4512\n",
            "Epoch [17], Batch [130/157], test Loss: 0.3874\n",
            "Epoch [17], Batch [131/157], test Loss: 0.5295\n",
            "Epoch [17], Batch [132/157], test Loss: 0.3747\n",
            "Epoch [17], Batch [133/157], test Loss: 0.4767\n",
            "Epoch [17], Batch [134/157], test Loss: 0.3638\n",
            "Epoch [17], Batch [135/157], test Loss: 0.3809\n",
            "Epoch [17], Batch [136/157], test Loss: 0.4273\n",
            "Epoch [17], Batch [137/157], test Loss: 0.4407\n",
            "Epoch [17], Batch [138/157], test Loss: 0.4567\n",
            "Epoch [17], Batch [139/157], test Loss: 0.5193\n",
            "Epoch [17], Batch [140/157], test Loss: 0.5215\n",
            "Epoch [17], Batch [141/157], test Loss: 0.3774\n",
            "Epoch [17], Batch [142/157], test Loss: 0.4221\n",
            "Epoch [17], Batch [143/157], test Loss: 0.4979\n",
            "Epoch [17], Batch [144/157], test Loss: 0.5435\n",
            "Epoch [17], Batch [145/157], test Loss: 0.6254\n",
            "Epoch [17], Batch [146/157], test Loss: 0.5277\n",
            "Epoch [17], Batch [147/157], test Loss: 0.5221\n",
            "Epoch [17], Batch [148/157], test Loss: 0.3564\n",
            "Epoch [17], Batch [149/157], test Loss: 0.6288\n",
            "Epoch [17], Batch [150/157], test Loss: 0.4698\n",
            "Epoch [17], Batch [151/157], test Loss: 0.4578\n",
            "Epoch [17], Batch [152/157], test Loss: 0.3904\n",
            "Epoch [17], Batch [153/157], test Loss: 0.3887\n",
            "Epoch [17], Batch [154/157], test Loss: 0.6453\n",
            "Epoch [17], Batch [155/157], test Loss: 0.6540\n",
            "Epoch [17], Batch [156/157], test Loss: 0.3705\n",
            "Epoch [17], Batch [157/157], test Loss: 0.4387\n",
            "Accuracy of test set: 83.55%\n",
            "Epoch [18/25] - Train Loss: 0.4287, Train Accuracy: 84.75% - Test Loss: 0.4688, Test Accuracy: 83.55%\n",
            "Epoch [18], Batch [10/938], Loss: 0.2851\n",
            "Epoch [18], Batch [20/938], Loss: 0.3216\n",
            "Epoch [18], Batch [30/938], Loss: 0.3448\n",
            "Epoch [18], Batch [40/938], Loss: 0.4092\n",
            "Epoch [18], Batch [50/938], Loss: 0.3313\n",
            "Epoch [18], Batch [60/938], Loss: 0.3969\n",
            "Epoch [18], Batch [70/938], Loss: 0.2932\n",
            "Epoch [18], Batch [80/938], Loss: 0.6557\n",
            "Epoch [18], Batch [90/938], Loss: 0.4474\n",
            "Epoch [18], Batch [100/938], Loss: 0.4338\n",
            "Epoch [18], Batch [110/938], Loss: 0.3210\n",
            "Epoch [18], Batch [120/938], Loss: 0.3860\n",
            "Epoch [18], Batch [130/938], Loss: 0.4452\n",
            "Epoch [18], Batch [140/938], Loss: 0.6144\n",
            "Epoch [18], Batch [150/938], Loss: 0.3657\n",
            "Epoch [18], Batch [160/938], Loss: 0.3562\n",
            "Epoch [18], Batch [170/938], Loss: 0.3266\n",
            "Epoch [18], Batch [180/938], Loss: 0.4181\n",
            "Epoch [18], Batch [190/938], Loss: 0.4568\n",
            "Epoch [18], Batch [200/938], Loss: 0.3480\n",
            "Epoch [18], Batch [210/938], Loss: 0.5135\n",
            "Epoch [18], Batch [220/938], Loss: 0.4229\n",
            "Epoch [18], Batch [230/938], Loss: 0.5195\n",
            "Epoch [18], Batch [240/938], Loss: 0.4666\n",
            "Epoch [18], Batch [250/938], Loss: 0.3118\n",
            "Epoch [18], Batch [260/938], Loss: 0.3393\n",
            "Epoch [18], Batch [270/938], Loss: 0.4081\n",
            "Epoch [18], Batch [280/938], Loss: 0.4351\n",
            "Epoch [18], Batch [290/938], Loss: 0.5816\n",
            "Epoch [18], Batch [300/938], Loss: 0.3470\n",
            "Epoch [18], Batch [310/938], Loss: 0.4846\n",
            "Epoch [18], Batch [320/938], Loss: 0.4964\n",
            "Epoch [18], Batch [330/938], Loss: 0.3410\n",
            "Epoch [18], Batch [340/938], Loss: 0.4569\n",
            "Epoch [18], Batch [350/938], Loss: 0.4638\n",
            "Epoch [18], Batch [360/938], Loss: 0.4800\n",
            "Epoch [18], Batch [370/938], Loss: 0.6734\n",
            "Epoch [18], Batch [380/938], Loss: 0.4002\n",
            "Epoch [18], Batch [390/938], Loss: 0.3801\n",
            "Epoch [18], Batch [400/938], Loss: 0.5581\n",
            "Epoch [18], Batch [410/938], Loss: 0.4339\n",
            "Epoch [18], Batch [420/938], Loss: 0.3257\n",
            "Epoch [18], Batch [430/938], Loss: 0.3328\n",
            "Epoch [18], Batch [440/938], Loss: 0.5393\n",
            "Epoch [18], Batch [450/938], Loss: 0.6131\n",
            "Epoch [18], Batch [460/938], Loss: 0.5599\n",
            "Epoch [18], Batch [470/938], Loss: 0.3346\n",
            "Epoch [18], Batch [480/938], Loss: 0.2883\n",
            "Epoch [18], Batch [490/938], Loss: 0.5777\n",
            "Epoch [18], Batch [500/938], Loss: 0.4501\n",
            "Epoch [18], Batch [510/938], Loss: 0.5203\n",
            "Epoch [18], Batch [520/938], Loss: 0.5441\n",
            "Epoch [18], Batch [530/938], Loss: 0.3905\n",
            "Epoch [18], Batch [540/938], Loss: 0.5096\n",
            "Epoch [18], Batch [550/938], Loss: 0.3846\n",
            "Epoch [18], Batch [560/938], Loss: 0.4216\n",
            "Epoch [18], Batch [570/938], Loss: 0.3534\n",
            "Epoch [18], Batch [580/938], Loss: 0.3655\n",
            "Epoch [18], Batch [590/938], Loss: 0.3670\n",
            "Epoch [18], Batch [600/938], Loss: 0.2044\n",
            "Epoch [18], Batch [610/938], Loss: 0.5564\n",
            "Epoch [18], Batch [620/938], Loss: 0.1816\n",
            "Epoch [18], Batch [630/938], Loss: 0.4539\n",
            "Epoch [18], Batch [640/938], Loss: 0.5131\n",
            "Epoch [18], Batch [650/938], Loss: 0.3871\n",
            "Epoch [18], Batch [660/938], Loss: 0.3938\n",
            "Epoch [18], Batch [670/938], Loss: 0.3069\n",
            "Epoch [18], Batch [680/938], Loss: 0.3289\n",
            "Epoch [18], Batch [690/938], Loss: 0.6622\n",
            "Epoch [18], Batch [700/938], Loss: 0.6111\n",
            "Epoch [18], Batch [710/938], Loss: 0.3093\n",
            "Epoch [18], Batch [720/938], Loss: 0.5084\n",
            "Epoch [18], Batch [730/938], Loss: 0.3924\n",
            "Epoch [18], Batch [740/938], Loss: 0.4647\n",
            "Epoch [18], Batch [750/938], Loss: 0.4939\n",
            "Epoch [18], Batch [760/938], Loss: 0.4557\n",
            "Epoch [18], Batch [770/938], Loss: 0.5016\n",
            "Epoch [18], Batch [780/938], Loss: 0.6291\n",
            "Epoch [18], Batch [790/938], Loss: 0.4128\n",
            "Epoch [18], Batch [800/938], Loss: 0.4716\n",
            "Epoch [18], Batch [810/938], Loss: 0.4802\n",
            "Epoch [18], Batch [820/938], Loss: 0.3147\n",
            "Epoch [18], Batch [830/938], Loss: 0.3083\n",
            "Epoch [18], Batch [840/938], Loss: 0.3718\n",
            "Epoch [18], Batch [850/938], Loss: 0.3484\n",
            "Epoch [18], Batch [860/938], Loss: 0.3308\n",
            "Epoch [18], Batch [870/938], Loss: 0.3666\n",
            "Epoch [18], Batch [880/938], Loss: 0.3987\n",
            "Epoch [18], Batch [890/938], Loss: 0.3907\n",
            "Epoch [18], Batch [900/938], Loss: 0.3420\n",
            "Epoch [18], Batch [910/938], Loss: 0.3167\n",
            "Epoch [18], Batch [920/938], Loss: 0.4299\n",
            "Epoch [18], Batch [930/938], Loss: 0.4915\n",
            "Epoch [18], Batch [938/938], Loss: 0.3382\n",
            "Accuracy of train set: 85.19%\n",
            "Epoch [18], Batch [1/157], test Loss: 0.3738\n",
            "Epoch [18], Batch [2/157], test Loss: 0.4378\n",
            "Epoch [18], Batch [3/157], test Loss: 0.6046\n",
            "Epoch [18], Batch [4/157], test Loss: 0.4700\n",
            "Epoch [18], Batch [5/157], test Loss: 0.5323\n",
            "Epoch [18], Batch [6/157], test Loss: 0.2477\n",
            "Epoch [18], Batch [7/157], test Loss: 0.4838\n",
            "Epoch [18], Batch [8/157], test Loss: 0.2627\n",
            "Epoch [18], Batch [9/157], test Loss: 0.2762\n",
            "Epoch [18], Batch [10/157], test Loss: 0.5138\n",
            "Epoch [18], Batch [11/157], test Loss: 0.5592\n",
            "Epoch [18], Batch [12/157], test Loss: 0.4753\n",
            "Epoch [18], Batch [13/157], test Loss: 0.3597\n",
            "Epoch [18], Batch [14/157], test Loss: 0.3542\n",
            "Epoch [18], Batch [15/157], test Loss: 0.3838\n",
            "Epoch [18], Batch [16/157], test Loss: 0.3808\n",
            "Epoch [18], Batch [17/157], test Loss: 0.5345\n",
            "Epoch [18], Batch [18/157], test Loss: 0.6813\n",
            "Epoch [18], Batch [19/157], test Loss: 0.4920\n",
            "Epoch [18], Batch [20/157], test Loss: 0.3887\n",
            "Epoch [18], Batch [21/157], test Loss: 0.6148\n",
            "Epoch [18], Batch [22/157], test Loss: 0.3264\n",
            "Epoch [18], Batch [23/157], test Loss: 0.2640\n",
            "Epoch [18], Batch [24/157], test Loss: 0.4424\n",
            "Epoch [18], Batch [25/157], test Loss: 0.3548\n",
            "Epoch [18], Batch [26/157], test Loss: 0.4367\n",
            "Epoch [18], Batch [27/157], test Loss: 0.5531\n",
            "Epoch [18], Batch [28/157], test Loss: 0.6207\n",
            "Epoch [18], Batch [29/157], test Loss: 0.4427\n",
            "Epoch [18], Batch [30/157], test Loss: 0.2359\n",
            "Epoch [18], Batch [31/157], test Loss: 0.5736\n",
            "Epoch [18], Batch [32/157], test Loss: 0.3184\n",
            "Epoch [18], Batch [33/157], test Loss: 0.3304\n",
            "Epoch [18], Batch [34/157], test Loss: 0.3910\n",
            "Epoch [18], Batch [35/157], test Loss: 0.3813\n",
            "Epoch [18], Batch [36/157], test Loss: 0.4680\n",
            "Epoch [18], Batch [37/157], test Loss: 0.4356\n",
            "Epoch [18], Batch [38/157], test Loss: 0.5875\n",
            "Epoch [18], Batch [39/157], test Loss: 0.6591\n",
            "Epoch [18], Batch [40/157], test Loss: 0.5394\n",
            "Epoch [18], Batch [41/157], test Loss: 0.3834\n",
            "Epoch [18], Batch [42/157], test Loss: 0.5655\n",
            "Epoch [18], Batch [43/157], test Loss: 0.2775\n",
            "Epoch [18], Batch [44/157], test Loss: 0.4082\n",
            "Epoch [18], Batch [45/157], test Loss: 0.4569\n",
            "Epoch [18], Batch [46/157], test Loss: 0.4041\n",
            "Epoch [18], Batch [47/157], test Loss: 0.3701\n",
            "Epoch [18], Batch [48/157], test Loss: 0.4346\n",
            "Epoch [18], Batch [49/157], test Loss: 0.5388\n",
            "Epoch [18], Batch [50/157], test Loss: 0.4740\n",
            "Epoch [18], Batch [51/157], test Loss: 0.3178\n",
            "Epoch [18], Batch [52/157], test Loss: 0.5508\n",
            "Epoch [18], Batch [53/157], test Loss: 0.4864\n",
            "Epoch [18], Batch [54/157], test Loss: 0.4531\n",
            "Epoch [18], Batch [55/157], test Loss: 0.5675\n",
            "Epoch [18], Batch [56/157], test Loss: 0.3186\n",
            "Epoch [18], Batch [57/157], test Loss: 0.6251\n",
            "Epoch [18], Batch [58/157], test Loss: 0.3885\n",
            "Epoch [18], Batch [59/157], test Loss: 0.3844\n",
            "Epoch [18], Batch [60/157], test Loss: 0.3953\n",
            "Epoch [18], Batch [61/157], test Loss: 0.3172\n",
            "Epoch [18], Batch [62/157], test Loss: 0.6575\n",
            "Epoch [18], Batch [63/157], test Loss: 0.6081\n",
            "Epoch [18], Batch [64/157], test Loss: 0.4010\n",
            "Epoch [18], Batch [65/157], test Loss: 0.7354\n",
            "Epoch [18], Batch [66/157], test Loss: 0.5636\n",
            "Epoch [18], Batch [67/157], test Loss: 0.3246\n",
            "Epoch [18], Batch [68/157], test Loss: 0.4719\n",
            "Epoch [18], Batch [69/157], test Loss: 0.4375\n",
            "Epoch [18], Batch [70/157], test Loss: 0.5412\n",
            "Epoch [18], Batch [71/157], test Loss: 0.2839\n",
            "Epoch [18], Batch [72/157], test Loss: 0.4353\n",
            "Epoch [18], Batch [73/157], test Loss: 0.3729\n",
            "Epoch [18], Batch [74/157], test Loss: 0.3064\n",
            "Epoch [18], Batch [75/157], test Loss: 0.5117\n",
            "Epoch [18], Batch [76/157], test Loss: 0.4049\n",
            "Epoch [18], Batch [77/157], test Loss: 0.7505\n",
            "Epoch [18], Batch [78/157], test Loss: 0.3505\n",
            "Epoch [18], Batch [79/157], test Loss: 0.5820\n",
            "Epoch [18], Batch [80/157], test Loss: 0.3212\n",
            "Epoch [18], Batch [81/157], test Loss: 0.3737\n",
            "Epoch [18], Batch [82/157], test Loss: 0.2612\n",
            "Epoch [18], Batch [83/157], test Loss: 0.5450\n",
            "Epoch [18], Batch [84/157], test Loss: 0.4662\n",
            "Epoch [18], Batch [85/157], test Loss: 0.4988\n",
            "Epoch [18], Batch [86/157], test Loss: 0.4460\n",
            "Epoch [18], Batch [87/157], test Loss: 0.4262\n",
            "Epoch [18], Batch [88/157], test Loss: 0.4850\n",
            "Epoch [18], Batch [89/157], test Loss: 0.5510\n",
            "Epoch [18], Batch [90/157], test Loss: 0.5127\n",
            "Epoch [18], Batch [91/157], test Loss: 0.4219\n",
            "Epoch [18], Batch [92/157], test Loss: 0.2379\n",
            "Epoch [18], Batch [93/157], test Loss: 0.2667\n",
            "Epoch [18], Batch [94/157], test Loss: 0.4044\n",
            "Epoch [18], Batch [95/157], test Loss: 0.3232\n",
            "Epoch [18], Batch [96/157], test Loss: 0.4493\n",
            "Epoch [18], Batch [97/157], test Loss: 0.5591\n",
            "Epoch [18], Batch [98/157], test Loss: 0.6206\n",
            "Epoch [18], Batch [99/157], test Loss: 0.5118\n",
            "Epoch [18], Batch [100/157], test Loss: 0.3939\n",
            "Epoch [18], Batch [101/157], test Loss: 0.5194\n",
            "Epoch [18], Batch [102/157], test Loss: 0.4165\n",
            "Epoch [18], Batch [103/157], test Loss: 0.4592\n",
            "Epoch [18], Batch [104/157], test Loss: 0.3978\n",
            "Epoch [18], Batch [105/157], test Loss: 0.7253\n",
            "Epoch [18], Batch [106/157], test Loss: 0.5660\n",
            "Epoch [18], Batch [107/157], test Loss: 0.4973\n",
            "Epoch [18], Batch [108/157], test Loss: 0.4440\n",
            "Epoch [18], Batch [109/157], test Loss: 0.4612\n",
            "Epoch [18], Batch [110/157], test Loss: 0.3017\n",
            "Epoch [18], Batch [111/157], test Loss: 0.4281\n",
            "Epoch [18], Batch [112/157], test Loss: 0.2982\n",
            "Epoch [18], Batch [113/157], test Loss: 0.4123\n",
            "Epoch [18], Batch [114/157], test Loss: 0.5211\n",
            "Epoch [18], Batch [115/157], test Loss: 0.5555\n",
            "Epoch [18], Batch [116/157], test Loss: 0.5166\n",
            "Epoch [18], Batch [117/157], test Loss: 0.3157\n",
            "Epoch [18], Batch [118/157], test Loss: 0.2868\n",
            "Epoch [18], Batch [119/157], test Loss: 0.5051\n",
            "Epoch [18], Batch [120/157], test Loss: 0.4760\n",
            "Epoch [18], Batch [121/157], test Loss: 0.4424\n",
            "Epoch [18], Batch [122/157], test Loss: 0.5101\n",
            "Epoch [18], Batch [123/157], test Loss: 0.6942\n",
            "Epoch [18], Batch [124/157], test Loss: 0.3754\n",
            "Epoch [18], Batch [125/157], test Loss: 0.3380\n",
            "Epoch [18], Batch [126/157], test Loss: 0.3748\n",
            "Epoch [18], Batch [127/157], test Loss: 0.3715\n",
            "Epoch [18], Batch [128/157], test Loss: 0.4328\n",
            "Epoch [18], Batch [129/157], test Loss: 0.3744\n",
            "Epoch [18], Batch [130/157], test Loss: 0.3275\n",
            "Epoch [18], Batch [131/157], test Loss: 0.5278\n",
            "Epoch [18], Batch [132/157], test Loss: 0.4177\n",
            "Epoch [18], Batch [133/157], test Loss: 0.4349\n",
            "Epoch [18], Batch [134/157], test Loss: 0.5004\n",
            "Epoch [18], Batch [135/157], test Loss: 0.4635\n",
            "Epoch [18], Batch [136/157], test Loss: 0.3842\n",
            "Epoch [18], Batch [137/157], test Loss: 0.2448\n",
            "Epoch [18], Batch [138/157], test Loss: 0.4694\n",
            "Epoch [18], Batch [139/157], test Loss: 0.2285\n",
            "Epoch [18], Batch [140/157], test Loss: 0.4089\n",
            "Epoch [18], Batch [141/157], test Loss: 0.4163\n",
            "Epoch [18], Batch [142/157], test Loss: 0.5635\n",
            "Epoch [18], Batch [143/157], test Loss: 0.4353\n",
            "Epoch [18], Batch [144/157], test Loss: 0.4263\n",
            "Epoch [18], Batch [145/157], test Loss: 0.4270\n",
            "Epoch [18], Batch [146/157], test Loss: 0.3915\n",
            "Epoch [18], Batch [147/157], test Loss: 0.4128\n",
            "Epoch [18], Batch [148/157], test Loss: 0.3765\n",
            "Epoch [18], Batch [149/157], test Loss: 0.4315\n",
            "Epoch [18], Batch [150/157], test Loss: 0.5255\n",
            "Epoch [18], Batch [151/157], test Loss: 0.3605\n",
            "Epoch [18], Batch [152/157], test Loss: 0.2384\n",
            "Epoch [18], Batch [153/157], test Loss: 0.6738\n",
            "Epoch [18], Batch [154/157], test Loss: 0.5253\n",
            "Epoch [18], Batch [155/157], test Loss: 0.4232\n",
            "Epoch [18], Batch [156/157], test Loss: 0.3623\n",
            "Epoch [18], Batch [157/157], test Loss: 0.5932\n",
            "Accuracy of test set: 84.49%\n",
            "Epoch [19/25] - Train Loss: 0.4191, Train Accuracy: 85.19% - Test Loss: 0.4441, Test Accuracy: 84.49%\n",
            "Epoch [19], Batch [10/938], Loss: 0.4341\n",
            "Epoch [19], Batch [20/938], Loss: 0.4190\n",
            "Epoch [19], Batch [30/938], Loss: 0.3373\n",
            "Epoch [19], Batch [40/938], Loss: 0.4744\n",
            "Epoch [19], Batch [50/938], Loss: 0.5313\n",
            "Epoch [19], Batch [60/938], Loss: 0.3937\n",
            "Epoch [19], Batch [70/938], Loss: 0.3643\n",
            "Epoch [19], Batch [80/938], Loss: 0.4477\n",
            "Epoch [19], Batch [90/938], Loss: 0.5332\n",
            "Epoch [19], Batch [100/938], Loss: 0.6716\n",
            "Epoch [19], Batch [110/938], Loss: 0.3508\n",
            "Epoch [19], Batch [120/938], Loss: 0.3288\n",
            "Epoch [19], Batch [130/938], Loss: 0.4485\n",
            "Epoch [19], Batch [140/938], Loss: 0.5735\n",
            "Epoch [19], Batch [150/938], Loss: 0.3346\n",
            "Epoch [19], Batch [160/938], Loss: 0.3443\n",
            "Epoch [19], Batch [170/938], Loss: 0.4071\n",
            "Epoch [19], Batch [180/938], Loss: 0.3149\n",
            "Epoch [19], Batch [190/938], Loss: 0.3151\n",
            "Epoch [19], Batch [200/938], Loss: 0.3859\n",
            "Epoch [19], Batch [210/938], Loss: 0.5536\n",
            "Epoch [19], Batch [220/938], Loss: 0.3508\n",
            "Epoch [19], Batch [230/938], Loss: 0.3794\n",
            "Epoch [19], Batch [240/938], Loss: 0.6168\n",
            "Epoch [19], Batch [250/938], Loss: 0.3698\n",
            "Epoch [19], Batch [260/938], Loss: 0.4889\n",
            "Epoch [19], Batch [270/938], Loss: 0.3904\n",
            "Epoch [19], Batch [280/938], Loss: 0.3346\n",
            "Epoch [19], Batch [290/938], Loss: 0.3972\n",
            "Epoch [19], Batch [300/938], Loss: 0.4024\n",
            "Epoch [19], Batch [310/938], Loss: 0.3392\n",
            "Epoch [19], Batch [320/938], Loss: 0.4383\n",
            "Epoch [19], Batch [330/938], Loss: 0.4639\n",
            "Epoch [19], Batch [340/938], Loss: 0.5325\n",
            "Epoch [19], Batch [350/938], Loss: 0.2979\n",
            "Epoch [19], Batch [360/938], Loss: 0.4271\n",
            "Epoch [19], Batch [370/938], Loss: 0.4074\n",
            "Epoch [19], Batch [380/938], Loss: 0.4036\n",
            "Epoch [19], Batch [390/938], Loss: 0.3979\n",
            "Epoch [19], Batch [400/938], Loss: 0.3512\n",
            "Epoch [19], Batch [410/938], Loss: 0.2327\n",
            "Epoch [19], Batch [420/938], Loss: 0.3423\n",
            "Epoch [19], Batch [430/938], Loss: 0.7549\n",
            "Epoch [19], Batch [440/938], Loss: 0.3804\n",
            "Epoch [19], Batch [450/938], Loss: 0.4406\n",
            "Epoch [19], Batch [460/938], Loss: 0.6373\n",
            "Epoch [19], Batch [470/938], Loss: 0.5081\n",
            "Epoch [19], Batch [480/938], Loss: 0.3300\n",
            "Epoch [19], Batch [490/938], Loss: 0.3039\n",
            "Epoch [19], Batch [500/938], Loss: 0.3158\n",
            "Epoch [19], Batch [510/938], Loss: 0.5155\n",
            "Epoch [19], Batch [520/938], Loss: 0.4651\n",
            "Epoch [19], Batch [530/938], Loss: 0.2691\n",
            "Epoch [19], Batch [540/938], Loss: 0.3790\n",
            "Epoch [19], Batch [550/938], Loss: 0.3380\n",
            "Epoch [19], Batch [560/938], Loss: 0.6373\n",
            "Epoch [19], Batch [570/938], Loss: 0.6051\n",
            "Epoch [19], Batch [580/938], Loss: 0.5069\n",
            "Epoch [19], Batch [590/938], Loss: 0.4215\n",
            "Epoch [19], Batch [600/938], Loss: 0.5187\n",
            "Epoch [19], Batch [610/938], Loss: 0.4550\n",
            "Epoch [19], Batch [620/938], Loss: 0.4687\n",
            "Epoch [19], Batch [630/938], Loss: 0.5358\n",
            "Epoch [19], Batch [640/938], Loss: 0.4528\n",
            "Epoch [19], Batch [650/938], Loss: 0.2415\n",
            "Epoch [19], Batch [660/938], Loss: 0.4007\n",
            "Epoch [19], Batch [670/938], Loss: 0.4765\n",
            "Epoch [19], Batch [680/938], Loss: 0.4153\n",
            "Epoch [19], Batch [690/938], Loss: 0.3476\n",
            "Epoch [19], Batch [700/938], Loss: 0.4909\n",
            "Epoch [19], Batch [710/938], Loss: 0.3494\n",
            "Epoch [19], Batch [720/938], Loss: 0.3356\n",
            "Epoch [19], Batch [730/938], Loss: 0.3936\n",
            "Epoch [19], Batch [740/938], Loss: 0.4870\n",
            "Epoch [19], Batch [750/938], Loss: 0.3972\n",
            "Epoch [19], Batch [760/938], Loss: 0.2794\n",
            "Epoch [19], Batch [770/938], Loss: 0.3382\n",
            "Epoch [19], Batch [780/938], Loss: 0.2716\n",
            "Epoch [19], Batch [790/938], Loss: 0.5802\n",
            "Epoch [19], Batch [800/938], Loss: 0.2848\n",
            "Epoch [19], Batch [810/938], Loss: 0.4817\n",
            "Epoch [19], Batch [820/938], Loss: 0.4441\n",
            "Epoch [19], Batch [830/938], Loss: 0.4391\n",
            "Epoch [19], Batch [840/938], Loss: 0.3732\n",
            "Epoch [19], Batch [850/938], Loss: 0.3376\n",
            "Epoch [19], Batch [860/938], Loss: 0.3685\n",
            "Epoch [19], Batch [870/938], Loss: 0.4971\n",
            "Epoch [19], Batch [880/938], Loss: 0.3733\n",
            "Epoch [19], Batch [890/938], Loss: 0.4311\n",
            "Epoch [19], Batch [900/938], Loss: 0.3502\n",
            "Epoch [19], Batch [910/938], Loss: 0.3797\n",
            "Epoch [19], Batch [920/938], Loss: 0.4110\n",
            "Epoch [19], Batch [930/938], Loss: 0.3500\n",
            "Epoch [19], Batch [938/938], Loss: 0.3494\n",
            "Accuracy of train set: 85.39%\n",
            "Epoch [19], Batch [1/157], test Loss: 0.3377\n",
            "Epoch [19], Batch [2/157], test Loss: 0.2927\n",
            "Epoch [19], Batch [3/157], test Loss: 0.4372\n",
            "Epoch [19], Batch [4/157], test Loss: 0.4989\n",
            "Epoch [19], Batch [5/157], test Loss: 0.3914\n",
            "Epoch [19], Batch [6/157], test Loss: 0.4520\n",
            "Epoch [19], Batch [7/157], test Loss: 0.3768\n",
            "Epoch [19], Batch [8/157], test Loss: 0.6246\n",
            "Epoch [19], Batch [9/157], test Loss: 0.4518\n",
            "Epoch [19], Batch [10/157], test Loss: 0.3842\n",
            "Epoch [19], Batch [11/157], test Loss: 0.4386\n",
            "Epoch [19], Batch [12/157], test Loss: 0.5974\n",
            "Epoch [19], Batch [13/157], test Loss: 0.3069\n",
            "Epoch [19], Batch [14/157], test Loss: 0.4590\n",
            "Epoch [19], Batch [15/157], test Loss: 0.3726\n",
            "Epoch [19], Batch [16/157], test Loss: 0.3741\n",
            "Epoch [19], Batch [17/157], test Loss: 0.5747\n",
            "Epoch [19], Batch [18/157], test Loss: 0.5301\n",
            "Epoch [19], Batch [19/157], test Loss: 0.5026\n",
            "Epoch [19], Batch [20/157], test Loss: 0.3582\n",
            "Epoch [19], Batch [21/157], test Loss: 0.4211\n",
            "Epoch [19], Batch [22/157], test Loss: 0.3884\n",
            "Epoch [19], Batch [23/157], test Loss: 0.4727\n",
            "Epoch [19], Batch [24/157], test Loss: 0.3116\n",
            "Epoch [19], Batch [25/157], test Loss: 0.4163\n",
            "Epoch [19], Batch [26/157], test Loss: 0.5612\n",
            "Epoch [19], Batch [27/157], test Loss: 0.4109\n",
            "Epoch [19], Batch [28/157], test Loss: 0.5238\n",
            "Epoch [19], Batch [29/157], test Loss: 0.4767\n",
            "Epoch [19], Batch [30/157], test Loss: 0.4233\n",
            "Epoch [19], Batch [31/157], test Loss: 0.3345\n",
            "Epoch [19], Batch [32/157], test Loss: 0.5477\n",
            "Epoch [19], Batch [33/157], test Loss: 0.2624\n",
            "Epoch [19], Batch [34/157], test Loss: 0.7215\n",
            "Epoch [19], Batch [35/157], test Loss: 0.3699\n",
            "Epoch [19], Batch [36/157], test Loss: 0.3817\n",
            "Epoch [19], Batch [37/157], test Loss: 0.3025\n",
            "Epoch [19], Batch [38/157], test Loss: 0.4625\n",
            "Epoch [19], Batch [39/157], test Loss: 0.4412\n",
            "Epoch [19], Batch [40/157], test Loss: 0.5328\n",
            "Epoch [19], Batch [41/157], test Loss: 0.5961\n",
            "Epoch [19], Batch [42/157], test Loss: 0.3020\n",
            "Epoch [19], Batch [43/157], test Loss: 0.4800\n",
            "Epoch [19], Batch [44/157], test Loss: 0.3803\n",
            "Epoch [19], Batch [45/157], test Loss: 0.5548\n",
            "Epoch [19], Batch [46/157], test Loss: 0.3288\n",
            "Epoch [19], Batch [47/157], test Loss: 0.6201\n",
            "Epoch [19], Batch [48/157], test Loss: 0.1921\n",
            "Epoch [19], Batch [49/157], test Loss: 0.3247\n",
            "Epoch [19], Batch [50/157], test Loss: 0.3571\n",
            "Epoch [19], Batch [51/157], test Loss: 0.4835\n",
            "Epoch [19], Batch [52/157], test Loss: 0.4101\n",
            "Epoch [19], Batch [53/157], test Loss: 0.4765\n",
            "Epoch [19], Batch [54/157], test Loss: 0.5527\n",
            "Epoch [19], Batch [55/157], test Loss: 0.2268\n",
            "Epoch [19], Batch [56/157], test Loss: 0.4731\n",
            "Epoch [19], Batch [57/157], test Loss: 0.3086\n",
            "Epoch [19], Batch [58/157], test Loss: 0.3523\n",
            "Epoch [19], Batch [59/157], test Loss: 0.4991\n",
            "Epoch [19], Batch [60/157], test Loss: 0.2568\n",
            "Epoch [19], Batch [61/157], test Loss: 0.4025\n",
            "Epoch [19], Batch [62/157], test Loss: 0.4317\n",
            "Epoch [19], Batch [63/157], test Loss: 0.5079\n",
            "Epoch [19], Batch [64/157], test Loss: 0.4076\n",
            "Epoch [19], Batch [65/157], test Loss: 0.2880\n",
            "Epoch [19], Batch [66/157], test Loss: 0.6003\n",
            "Epoch [19], Batch [67/157], test Loss: 0.3873\n",
            "Epoch [19], Batch [68/157], test Loss: 0.4345\n",
            "Epoch [19], Batch [69/157], test Loss: 0.4306\n",
            "Epoch [19], Batch [70/157], test Loss: 0.4945\n",
            "Epoch [19], Batch [71/157], test Loss: 0.2645\n",
            "Epoch [19], Batch [72/157], test Loss: 0.5678\n",
            "Epoch [19], Batch [73/157], test Loss: 0.3639\n",
            "Epoch [19], Batch [74/157], test Loss: 0.2773\n",
            "Epoch [19], Batch [75/157], test Loss: 0.6748\n",
            "Epoch [19], Batch [76/157], test Loss: 0.5683\n",
            "Epoch [19], Batch [77/157], test Loss: 0.3957\n",
            "Epoch [19], Batch [78/157], test Loss: 0.5438\n",
            "Epoch [19], Batch [79/157], test Loss: 0.5935\n",
            "Epoch [19], Batch [80/157], test Loss: 0.4731\n",
            "Epoch [19], Batch [81/157], test Loss: 0.3787\n",
            "Epoch [19], Batch [82/157], test Loss: 0.5244\n",
            "Epoch [19], Batch [83/157], test Loss: 0.3489\n",
            "Epoch [19], Batch [84/157], test Loss: 0.3685\n",
            "Epoch [19], Batch [85/157], test Loss: 0.4380\n",
            "Epoch [19], Batch [86/157], test Loss: 0.3442\n",
            "Epoch [19], Batch [87/157], test Loss: 0.3929\n",
            "Epoch [19], Batch [88/157], test Loss: 0.3258\n",
            "Epoch [19], Batch [89/157], test Loss: 0.4179\n",
            "Epoch [19], Batch [90/157], test Loss: 0.2422\n",
            "Epoch [19], Batch [91/157], test Loss: 0.3035\n",
            "Epoch [19], Batch [92/157], test Loss: 0.3596\n",
            "Epoch [19], Batch [93/157], test Loss: 0.2594\n",
            "Epoch [19], Batch [94/157], test Loss: 0.4096\n",
            "Epoch [19], Batch [95/157], test Loss: 0.5470\n",
            "Epoch [19], Batch [96/157], test Loss: 0.3561\n",
            "Epoch [19], Batch [97/157], test Loss: 0.3645\n",
            "Epoch [19], Batch [98/157], test Loss: 0.5637\n",
            "Epoch [19], Batch [99/157], test Loss: 0.4644\n",
            "Epoch [19], Batch [100/157], test Loss: 0.5290\n",
            "Epoch [19], Batch [101/157], test Loss: 0.6597\n",
            "Epoch [19], Batch [102/157], test Loss: 0.4444\n",
            "Epoch [19], Batch [103/157], test Loss: 0.5788\n",
            "Epoch [19], Batch [104/157], test Loss: 0.5113\n",
            "Epoch [19], Batch [105/157], test Loss: 0.4796\n",
            "Epoch [19], Batch [106/157], test Loss: 0.4478\n",
            "Epoch [19], Batch [107/157], test Loss: 0.5604\n",
            "Epoch [19], Batch [108/157], test Loss: 0.5844\n",
            "Epoch [19], Batch [109/157], test Loss: 0.4387\n",
            "Epoch [19], Batch [110/157], test Loss: 0.7045\n",
            "Epoch [19], Batch [111/157], test Loss: 0.3695\n",
            "Epoch [19], Batch [112/157], test Loss: 0.2941\n",
            "Epoch [19], Batch [113/157], test Loss: 0.5367\n",
            "Epoch [19], Batch [114/157], test Loss: 0.3996\n",
            "Epoch [19], Batch [115/157], test Loss: 0.4065\n",
            "Epoch [19], Batch [116/157], test Loss: 0.2409\n",
            "Epoch [19], Batch [117/157], test Loss: 0.4630\n",
            "Epoch [19], Batch [118/157], test Loss: 0.5362\n",
            "Epoch [19], Batch [119/157], test Loss: 0.5675\n",
            "Epoch [19], Batch [120/157], test Loss: 0.4895\n",
            "Epoch [19], Batch [121/157], test Loss: 0.2676\n",
            "Epoch [19], Batch [122/157], test Loss: 0.4982\n",
            "Epoch [19], Batch [123/157], test Loss: 0.4963\n",
            "Epoch [19], Batch [124/157], test Loss: 0.5400\n",
            "Epoch [19], Batch [125/157], test Loss: 0.4456\n",
            "Epoch [19], Batch [126/157], test Loss: 0.6289\n",
            "Epoch [19], Batch [127/157], test Loss: 0.3601\n",
            "Epoch [19], Batch [128/157], test Loss: 0.5372\n",
            "Epoch [19], Batch [129/157], test Loss: 0.4067\n",
            "Epoch [19], Batch [130/157], test Loss: 0.5311\n",
            "Epoch [19], Batch [131/157], test Loss: 0.3202\n",
            "Epoch [19], Batch [132/157], test Loss: 0.5290\n",
            "Epoch [19], Batch [133/157], test Loss: 0.4444\n",
            "Epoch [19], Batch [134/157], test Loss: 0.5393\n",
            "Epoch [19], Batch [135/157], test Loss: 0.4358\n",
            "Epoch [19], Batch [136/157], test Loss: 0.4925\n",
            "Epoch [19], Batch [137/157], test Loss: 0.4710\n",
            "Epoch [19], Batch [138/157], test Loss: 0.3860\n",
            "Epoch [19], Batch [139/157], test Loss: 0.3891\n",
            "Epoch [19], Batch [140/157], test Loss: 0.3789\n",
            "Epoch [19], Batch [141/157], test Loss: 0.5841\n",
            "Epoch [19], Batch [142/157], test Loss: 0.3865\n",
            "Epoch [19], Batch [143/157], test Loss: 0.4653\n",
            "Epoch [19], Batch [144/157], test Loss: 0.3712\n",
            "Epoch [19], Batch [145/157], test Loss: 0.6272\n",
            "Epoch [19], Batch [146/157], test Loss: 0.4145\n",
            "Epoch [19], Batch [147/157], test Loss: 0.4764\n",
            "Epoch [19], Batch [148/157], test Loss: 0.4900\n",
            "Epoch [19], Batch [149/157], test Loss: 0.4189\n",
            "Epoch [19], Batch [150/157], test Loss: 0.2885\n",
            "Epoch [19], Batch [151/157], test Loss: 0.5755\n",
            "Epoch [19], Batch [152/157], test Loss: 0.6030\n",
            "Epoch [19], Batch [153/157], test Loss: 0.6368\n",
            "Epoch [19], Batch [154/157], test Loss: 0.4477\n",
            "Epoch [19], Batch [155/157], test Loss: 0.3201\n",
            "Epoch [19], Batch [156/157], test Loss: 0.6724\n",
            "Epoch [19], Batch [157/157], test Loss: 0.2403\n",
            "Accuracy of test set: 84.08%\n",
            "Epoch [20/25] - Train Loss: 0.4099, Train Accuracy: 85.39% - Test Loss: 0.4432, Test Accuracy: 84.08%\n",
            "Epoch [20], Batch [10/938], Loss: 0.3833\n",
            "Epoch [20], Batch [20/938], Loss: 0.4205\n",
            "Epoch [20], Batch [30/938], Loss: 0.3898\n",
            "Epoch [20], Batch [40/938], Loss: 0.5131\n",
            "Epoch [20], Batch [50/938], Loss: 0.2028\n",
            "Epoch [20], Batch [60/938], Loss: 0.4156\n",
            "Epoch [20], Batch [70/938], Loss: 0.4025\n",
            "Epoch [20], Batch [80/938], Loss: 0.5468\n",
            "Epoch [20], Batch [90/938], Loss: 0.4736\n",
            "Epoch [20], Batch [100/938], Loss: 0.4122\n",
            "Epoch [20], Batch [110/938], Loss: 0.3041\n",
            "Epoch [20], Batch [120/938], Loss: 0.4395\n",
            "Epoch [20], Batch [130/938], Loss: 0.3431\n",
            "Epoch [20], Batch [140/938], Loss: 0.2598\n",
            "Epoch [20], Batch [150/938], Loss: 0.4508\n",
            "Epoch [20], Batch [160/938], Loss: 0.4453\n",
            "Epoch [20], Batch [170/938], Loss: 0.5061\n",
            "Epoch [20], Batch [180/938], Loss: 0.3206\n",
            "Epoch [20], Batch [190/938], Loss: 0.5739\n",
            "Epoch [20], Batch [200/938], Loss: 0.3763\n",
            "Epoch [20], Batch [210/938], Loss: 0.2610\n",
            "Epoch [20], Batch [220/938], Loss: 0.3806\n",
            "Epoch [20], Batch [230/938], Loss: 0.5022\n",
            "Epoch [20], Batch [240/938], Loss: 0.5054\n",
            "Epoch [20], Batch [250/938], Loss: 0.4591\n",
            "Epoch [20], Batch [260/938], Loss: 0.3126\n",
            "Epoch [20], Batch [270/938], Loss: 0.5042\n",
            "Epoch [20], Batch [280/938], Loss: 0.5083\n",
            "Epoch [20], Batch [290/938], Loss: 0.4598\n",
            "Epoch [20], Batch [300/938], Loss: 0.3989\n",
            "Epoch [20], Batch [310/938], Loss: 0.3795\n",
            "Epoch [20], Batch [320/938], Loss: 0.4057\n",
            "Epoch [20], Batch [330/938], Loss: 0.4097\n",
            "Epoch [20], Batch [340/938], Loss: 0.4102\n",
            "Epoch [20], Batch [350/938], Loss: 0.3109\n",
            "Epoch [20], Batch [360/938], Loss: 0.5371\n",
            "Epoch [20], Batch [370/938], Loss: 0.3803\n",
            "Epoch [20], Batch [380/938], Loss: 0.5257\n",
            "Epoch [20], Batch [390/938], Loss: 0.3937\n",
            "Epoch [20], Batch [400/938], Loss: 0.2584\n",
            "Epoch [20], Batch [410/938], Loss: 0.4492\n",
            "Epoch [20], Batch [420/938], Loss: 0.3717\n",
            "Epoch [20], Batch [430/938], Loss: 0.2880\n",
            "Epoch [20], Batch [440/938], Loss: 0.4862\n",
            "Epoch [20], Batch [450/938], Loss: 0.4385\n",
            "Epoch [20], Batch [460/938], Loss: 0.2799\n",
            "Epoch [20], Batch [470/938], Loss: 0.4872\n",
            "Epoch [20], Batch [480/938], Loss: 0.4843\n",
            "Epoch [20], Batch [490/938], Loss: 0.4157\n",
            "Epoch [20], Batch [500/938], Loss: 0.3465\n",
            "Epoch [20], Batch [510/938], Loss: 0.4784\n",
            "Epoch [20], Batch [520/938], Loss: 0.4653\n",
            "Epoch [20], Batch [530/938], Loss: 0.5398\n",
            "Epoch [20], Batch [540/938], Loss: 0.3418\n",
            "Epoch [20], Batch [550/938], Loss: 0.4067\n",
            "Epoch [20], Batch [560/938], Loss: 0.5247\n",
            "Epoch [20], Batch [570/938], Loss: 0.2160\n",
            "Epoch [20], Batch [580/938], Loss: 0.3598\n",
            "Epoch [20], Batch [590/938], Loss: 0.4090\n",
            "Epoch [20], Batch [600/938], Loss: 0.3888\n",
            "Epoch [20], Batch [610/938], Loss: 0.4682\n",
            "Epoch [20], Batch [620/938], Loss: 0.4147\n",
            "Epoch [20], Batch [630/938], Loss: 0.5351\n",
            "Epoch [20], Batch [640/938], Loss: 0.2847\n",
            "Epoch [20], Batch [650/938], Loss: 0.5402\n",
            "Epoch [20], Batch [660/938], Loss: 0.3583\n",
            "Epoch [20], Batch [670/938], Loss: 0.4240\n",
            "Epoch [20], Batch [680/938], Loss: 0.4369\n",
            "Epoch [20], Batch [690/938], Loss: 0.5066\n",
            "Epoch [20], Batch [700/938], Loss: 0.3945\n",
            "Epoch [20], Batch [710/938], Loss: 0.5491\n",
            "Epoch [20], Batch [720/938], Loss: 0.3774\n",
            "Epoch [20], Batch [730/938], Loss: 0.2758\n",
            "Epoch [20], Batch [740/938], Loss: 0.3709\n",
            "Epoch [20], Batch [750/938], Loss: 0.3728\n",
            "Epoch [20], Batch [760/938], Loss: 0.3785\n",
            "Epoch [20], Batch [770/938], Loss: 0.3382\n",
            "Epoch [20], Batch [780/938], Loss: 0.5603\n",
            "Epoch [20], Batch [790/938], Loss: 0.3671\n",
            "Epoch [20], Batch [800/938], Loss: 0.4335\n",
            "Epoch [20], Batch [810/938], Loss: 0.5012\n",
            "Epoch [20], Batch [820/938], Loss: 0.4729\n",
            "Epoch [20], Batch [830/938], Loss: 0.3717\n",
            "Epoch [20], Batch [840/938], Loss: 0.3378\n",
            "Epoch [20], Batch [850/938], Loss: 0.3051\n",
            "Epoch [20], Batch [860/938], Loss: 0.3507\n",
            "Epoch [20], Batch [870/938], Loss: 0.4713\n",
            "Epoch [20], Batch [880/938], Loss: 0.3341\n",
            "Epoch [20], Batch [890/938], Loss: 0.2842\n",
            "Epoch [20], Batch [900/938], Loss: 0.3988\n",
            "Epoch [20], Batch [910/938], Loss: 0.2681\n",
            "Epoch [20], Batch [920/938], Loss: 0.2908\n",
            "Epoch [20], Batch [930/938], Loss: 0.4368\n",
            "Epoch [20], Batch [938/938], Loss: 0.2522\n",
            "Accuracy of train set: 85.75%\n",
            "Epoch [20], Batch [1/157], test Loss: 0.3772\n",
            "Epoch [20], Batch [2/157], test Loss: 0.3941\n",
            "Epoch [20], Batch [3/157], test Loss: 0.5194\n",
            "Epoch [20], Batch [4/157], test Loss: 0.4147\n",
            "Epoch [20], Batch [5/157], test Loss: 0.5126\n",
            "Epoch [20], Batch [6/157], test Loss: 0.3436\n",
            "Epoch [20], Batch [7/157], test Loss: 0.4285\n",
            "Epoch [20], Batch [8/157], test Loss: 0.4193\n",
            "Epoch [20], Batch [9/157], test Loss: 0.4030\n",
            "Epoch [20], Batch [10/157], test Loss: 0.3942\n",
            "Epoch [20], Batch [11/157], test Loss: 0.5253\n",
            "Epoch [20], Batch [12/157], test Loss: 0.4988\n",
            "Epoch [20], Batch [13/157], test Loss: 0.5325\n",
            "Epoch [20], Batch [14/157], test Loss: 0.5989\n",
            "Epoch [20], Batch [15/157], test Loss: 0.6964\n",
            "Epoch [20], Batch [16/157], test Loss: 0.5372\n",
            "Epoch [20], Batch [17/157], test Loss: 0.5830\n",
            "Epoch [20], Batch [18/157], test Loss: 0.3945\n",
            "Epoch [20], Batch [19/157], test Loss: 0.4789\n",
            "Epoch [20], Batch [20/157], test Loss: 0.4350\n",
            "Epoch [20], Batch [21/157], test Loss: 0.4768\n",
            "Epoch [20], Batch [22/157], test Loss: 0.4721\n",
            "Epoch [20], Batch [23/157], test Loss: 0.3081\n",
            "Epoch [20], Batch [24/157], test Loss: 0.2789\n",
            "Epoch [20], Batch [25/157], test Loss: 0.6184\n",
            "Epoch [20], Batch [26/157], test Loss: 0.5234\n",
            "Epoch [20], Batch [27/157], test Loss: 0.4186\n",
            "Epoch [20], Batch [28/157], test Loss: 0.3979\n",
            "Epoch [20], Batch [29/157], test Loss: 0.5163\n",
            "Epoch [20], Batch [30/157], test Loss: 0.5779\n",
            "Epoch [20], Batch [31/157], test Loss: 0.3527\n",
            "Epoch [20], Batch [32/157], test Loss: 0.5220\n",
            "Epoch [20], Batch [33/157], test Loss: 0.6215\n",
            "Epoch [20], Batch [34/157], test Loss: 0.4427\n",
            "Epoch [20], Batch [35/157], test Loss: 0.5495\n",
            "Epoch [20], Batch [36/157], test Loss: 0.5359\n",
            "Epoch [20], Batch [37/157], test Loss: 0.3826\n",
            "Epoch [20], Batch [38/157], test Loss: 0.3912\n",
            "Epoch [20], Batch [39/157], test Loss: 0.5019\n",
            "Epoch [20], Batch [40/157], test Loss: 0.3316\n",
            "Epoch [20], Batch [41/157], test Loss: 0.5190\n",
            "Epoch [20], Batch [42/157], test Loss: 0.3740\n",
            "Epoch [20], Batch [43/157], test Loss: 0.4652\n",
            "Epoch [20], Batch [44/157], test Loss: 0.4818\n",
            "Epoch [20], Batch [45/157], test Loss: 0.3170\n",
            "Epoch [20], Batch [46/157], test Loss: 0.3819\n",
            "Epoch [20], Batch [47/157], test Loss: 0.4712\n",
            "Epoch [20], Batch [48/157], test Loss: 0.5642\n",
            "Epoch [20], Batch [49/157], test Loss: 0.4387\n",
            "Epoch [20], Batch [50/157], test Loss: 0.5460\n",
            "Epoch [20], Batch [51/157], test Loss: 0.4397\n",
            "Epoch [20], Batch [52/157], test Loss: 0.5888\n",
            "Epoch [20], Batch [53/157], test Loss: 0.3598\n",
            "Epoch [20], Batch [54/157], test Loss: 0.4085\n",
            "Epoch [20], Batch [55/157], test Loss: 0.3621\n",
            "Epoch [20], Batch [56/157], test Loss: 0.5768\n",
            "Epoch [20], Batch [57/157], test Loss: 0.4431\n",
            "Epoch [20], Batch [58/157], test Loss: 0.4923\n",
            "Epoch [20], Batch [59/157], test Loss: 0.4035\n",
            "Epoch [20], Batch [60/157], test Loss: 0.6128\n",
            "Epoch [20], Batch [61/157], test Loss: 0.5572\n",
            "Epoch [20], Batch [62/157], test Loss: 0.5316\n",
            "Epoch [20], Batch [63/157], test Loss: 0.6015\n",
            "Epoch [20], Batch [64/157], test Loss: 0.4731\n",
            "Epoch [20], Batch [65/157], test Loss: 0.3684\n",
            "Epoch [20], Batch [66/157], test Loss: 0.3854\n",
            "Epoch [20], Batch [67/157], test Loss: 0.3584\n",
            "Epoch [20], Batch [68/157], test Loss: 0.2704\n",
            "Epoch [20], Batch [69/157], test Loss: 0.4386\n",
            "Epoch [20], Batch [70/157], test Loss: 0.4786\n",
            "Epoch [20], Batch [71/157], test Loss: 0.6970\n",
            "Epoch [20], Batch [72/157], test Loss: 0.4170\n",
            "Epoch [20], Batch [73/157], test Loss: 0.5554\n",
            "Epoch [20], Batch [74/157], test Loss: 0.4017\n",
            "Epoch [20], Batch [75/157], test Loss: 0.3688\n",
            "Epoch [20], Batch [76/157], test Loss: 0.4517\n",
            "Epoch [20], Batch [77/157], test Loss: 0.5201\n",
            "Epoch [20], Batch [78/157], test Loss: 0.6317\n",
            "Epoch [20], Batch [79/157], test Loss: 0.2596\n",
            "Epoch [20], Batch [80/157], test Loss: 0.3718\n",
            "Epoch [20], Batch [81/157], test Loss: 0.3937\n",
            "Epoch [20], Batch [82/157], test Loss: 0.3230\n",
            "Epoch [20], Batch [83/157], test Loss: 0.2745\n",
            "Epoch [20], Batch [84/157], test Loss: 0.6888\n",
            "Epoch [20], Batch [85/157], test Loss: 0.4730\n",
            "Epoch [20], Batch [86/157], test Loss: 0.5341\n",
            "Epoch [20], Batch [87/157], test Loss: 0.5035\n",
            "Epoch [20], Batch [88/157], test Loss: 0.4616\n",
            "Epoch [20], Batch [89/157], test Loss: 0.3157\n",
            "Epoch [20], Batch [90/157], test Loss: 0.6627\n",
            "Epoch [20], Batch [91/157], test Loss: 0.4868\n",
            "Epoch [20], Batch [92/157], test Loss: 0.7487\n",
            "Epoch [20], Batch [93/157], test Loss: 0.3976\n",
            "Epoch [20], Batch [94/157], test Loss: 0.5792\n",
            "Epoch [20], Batch [95/157], test Loss: 0.2940\n",
            "Epoch [20], Batch [96/157], test Loss: 0.3136\n",
            "Epoch [20], Batch [97/157], test Loss: 0.3440\n",
            "Epoch [20], Batch [98/157], test Loss: 0.4792\n",
            "Epoch [20], Batch [99/157], test Loss: 0.5544\n",
            "Epoch [20], Batch [100/157], test Loss: 0.6544\n",
            "Epoch [20], Batch [101/157], test Loss: 0.8506\n",
            "Epoch [20], Batch [102/157], test Loss: 0.3634\n",
            "Epoch [20], Batch [103/157], test Loss: 0.5569\n",
            "Epoch [20], Batch [104/157], test Loss: 0.3295\n",
            "Epoch [20], Batch [105/157], test Loss: 0.3173\n",
            "Epoch [20], Batch [106/157], test Loss: 0.3312\n",
            "Epoch [20], Batch [107/157], test Loss: 0.6199\n",
            "Epoch [20], Batch [108/157], test Loss: 0.2937\n",
            "Epoch [20], Batch [109/157], test Loss: 0.4352\n",
            "Epoch [20], Batch [110/157], test Loss: 0.3266\n",
            "Epoch [20], Batch [111/157], test Loss: 0.4913\n",
            "Epoch [20], Batch [112/157], test Loss: 0.4652\n",
            "Epoch [20], Batch [113/157], test Loss: 0.5154\n",
            "Epoch [20], Batch [114/157], test Loss: 0.2460\n",
            "Epoch [20], Batch [115/157], test Loss: 0.4630\n",
            "Epoch [20], Batch [116/157], test Loss: 0.5838\n",
            "Epoch [20], Batch [117/157], test Loss: 0.3888\n",
            "Epoch [20], Batch [118/157], test Loss: 0.4923\n",
            "Epoch [20], Batch [119/157], test Loss: 0.4911\n",
            "Epoch [20], Batch [120/157], test Loss: 0.4914\n",
            "Epoch [20], Batch [121/157], test Loss: 0.4094\n",
            "Epoch [20], Batch [122/157], test Loss: 0.4739\n",
            "Epoch [20], Batch [123/157], test Loss: 0.3754\n",
            "Epoch [20], Batch [124/157], test Loss: 0.5280\n",
            "Epoch [20], Batch [125/157], test Loss: 0.4684\n",
            "Epoch [20], Batch [126/157], test Loss: 0.7962\n",
            "Epoch [20], Batch [127/157], test Loss: 0.2192\n",
            "Epoch [20], Batch [128/157], test Loss: 0.3913\n",
            "Epoch [20], Batch [129/157], test Loss: 0.3804\n",
            "Epoch [20], Batch [130/157], test Loss: 0.4453\n",
            "Epoch [20], Batch [131/157], test Loss: 0.4456\n",
            "Epoch [20], Batch [132/157], test Loss: 0.4746\n",
            "Epoch [20], Batch [133/157], test Loss: 0.4009\n",
            "Epoch [20], Batch [134/157], test Loss: 0.3543\n",
            "Epoch [20], Batch [135/157], test Loss: 0.2451\n",
            "Epoch [20], Batch [136/157], test Loss: 0.4262\n",
            "Epoch [20], Batch [137/157], test Loss: 0.4405\n",
            "Epoch [20], Batch [138/157], test Loss: 0.5266\n",
            "Epoch [20], Batch [139/157], test Loss: 0.4285\n",
            "Epoch [20], Batch [140/157], test Loss: 0.3793\n",
            "Epoch [20], Batch [141/157], test Loss: 0.5532\n",
            "Epoch [20], Batch [142/157], test Loss: 0.4124\n",
            "Epoch [20], Batch [143/157], test Loss: 0.4264\n",
            "Epoch [20], Batch [144/157], test Loss: 0.3850\n",
            "Epoch [20], Batch [145/157], test Loss: 0.3695\n",
            "Epoch [20], Batch [146/157], test Loss: 0.5959\n",
            "Epoch [20], Batch [147/157], test Loss: 0.5674\n",
            "Epoch [20], Batch [148/157], test Loss: 0.2605\n",
            "Epoch [20], Batch [149/157], test Loss: 0.6095\n",
            "Epoch [20], Batch [150/157], test Loss: 0.4174\n",
            "Epoch [20], Batch [151/157], test Loss: 0.4099\n",
            "Epoch [20], Batch [152/157], test Loss: 0.4815\n",
            "Epoch [20], Batch [153/157], test Loss: 0.4481\n",
            "Epoch [20], Batch [154/157], test Loss: 0.5081\n",
            "Epoch [20], Batch [155/157], test Loss: 0.6038\n",
            "Epoch [20], Batch [156/157], test Loss: 0.4362\n",
            "Epoch [20], Batch [157/157], test Loss: 0.5345\n",
            "Accuracy of test set: 83.80%\n",
            "Epoch [21/25] - Train Loss: 0.4016, Train Accuracy: 85.75% - Test Loss: 0.4602, Test Accuracy: 83.80%\n",
            "Epoch [21], Batch [10/938], Loss: 0.4176\n",
            "Epoch [21], Batch [20/938], Loss: 0.3738\n",
            "Epoch [21], Batch [30/938], Loss: 0.3671\n",
            "Epoch [21], Batch [40/938], Loss: 0.2868\n",
            "Epoch [21], Batch [50/938], Loss: 0.4715\n",
            "Epoch [21], Batch [60/938], Loss: 0.4186\n",
            "Epoch [21], Batch [70/938], Loss: 0.2820\n",
            "Epoch [21], Batch [80/938], Loss: 0.2746\n",
            "Epoch [21], Batch [90/938], Loss: 0.3651\n",
            "Epoch [21], Batch [100/938], Loss: 0.8238\n",
            "Epoch [21], Batch [110/938], Loss: 0.4225\n",
            "Epoch [21], Batch [120/938], Loss: 0.3022\n",
            "Epoch [21], Batch [130/938], Loss: 0.4584\n",
            "Epoch [21], Batch [140/938], Loss: 0.3246\n",
            "Epoch [21], Batch [150/938], Loss: 0.3284\n",
            "Epoch [21], Batch [160/938], Loss: 0.2498\n",
            "Epoch [21], Batch [170/938], Loss: 0.4105\n",
            "Epoch [21], Batch [180/938], Loss: 0.5467\n",
            "Epoch [21], Batch [190/938], Loss: 0.4107\n",
            "Epoch [21], Batch [200/938], Loss: 0.4363\n",
            "Epoch [21], Batch [210/938], Loss: 0.2721\n",
            "Epoch [21], Batch [220/938], Loss: 0.5591\n",
            "Epoch [21], Batch [230/938], Loss: 0.3024\n",
            "Epoch [21], Batch [240/938], Loss: 0.5014\n",
            "Epoch [21], Batch [250/938], Loss: 0.3635\n",
            "Epoch [21], Batch [260/938], Loss: 0.6636\n",
            "Epoch [21], Batch [270/938], Loss: 0.3087\n",
            "Epoch [21], Batch [280/938], Loss: 0.2965\n",
            "Epoch [21], Batch [290/938], Loss: 0.3853\n",
            "Epoch [21], Batch [300/938], Loss: 0.2891\n",
            "Epoch [21], Batch [310/938], Loss: 0.3300\n",
            "Epoch [21], Batch [320/938], Loss: 0.4110\n",
            "Epoch [21], Batch [330/938], Loss: 0.2665\n",
            "Epoch [21], Batch [340/938], Loss: 0.2988\n",
            "Epoch [21], Batch [350/938], Loss: 0.3492\n",
            "Epoch [21], Batch [360/938], Loss: 0.5204\n",
            "Epoch [21], Batch [370/938], Loss: 0.5953\n",
            "Epoch [21], Batch [380/938], Loss: 0.3457\n",
            "Epoch [21], Batch [390/938], Loss: 0.4676\n",
            "Epoch [21], Batch [400/938], Loss: 0.4072\n",
            "Epoch [21], Batch [410/938], Loss: 0.2860\n",
            "Epoch [21], Batch [420/938], Loss: 0.4083\n",
            "Epoch [21], Batch [430/938], Loss: 0.3505\n",
            "Epoch [21], Batch [440/938], Loss: 0.3983\n",
            "Epoch [21], Batch [450/938], Loss: 0.3457\n",
            "Epoch [21], Batch [460/938], Loss: 0.1969\n",
            "Epoch [21], Batch [470/938], Loss: 0.2296\n",
            "Epoch [21], Batch [480/938], Loss: 0.5533\n",
            "Epoch [21], Batch [490/938], Loss: 0.2979\n",
            "Epoch [21], Batch [500/938], Loss: 0.2618\n",
            "Epoch [21], Batch [510/938], Loss: 0.3636\n",
            "Epoch [21], Batch [520/938], Loss: 0.3984\n",
            "Epoch [21], Batch [530/938], Loss: 0.3979\n",
            "Epoch [21], Batch [540/938], Loss: 0.3779\n",
            "Epoch [21], Batch [550/938], Loss: 0.2797\n",
            "Epoch [21], Batch [560/938], Loss: 0.1883\n",
            "Epoch [21], Batch [570/938], Loss: 0.4021\n",
            "Epoch [21], Batch [580/938], Loss: 0.2605\n",
            "Epoch [21], Batch [590/938], Loss: 0.4869\n",
            "Epoch [21], Batch [600/938], Loss: 0.2630\n",
            "Epoch [21], Batch [610/938], Loss: 0.3299\n",
            "Epoch [21], Batch [620/938], Loss: 0.5395\n",
            "Epoch [21], Batch [630/938], Loss: 0.3093\n",
            "Epoch [21], Batch [640/938], Loss: 0.3556\n",
            "Epoch [21], Batch [650/938], Loss: 0.2123\n",
            "Epoch [21], Batch [660/938], Loss: 0.3971\n",
            "Epoch [21], Batch [670/938], Loss: 0.2781\n",
            "Epoch [21], Batch [680/938], Loss: 0.2525\n",
            "Epoch [21], Batch [690/938], Loss: 0.3094\n",
            "Epoch [21], Batch [700/938], Loss: 0.2732\n",
            "Epoch [21], Batch [710/938], Loss: 0.3273\n",
            "Epoch [21], Batch [720/938], Loss: 0.2475\n",
            "Epoch [21], Batch [730/938], Loss: 0.3903\n",
            "Epoch [21], Batch [740/938], Loss: 0.5323\n",
            "Epoch [21], Batch [750/938], Loss: 0.4020\n",
            "Epoch [21], Batch [760/938], Loss: 0.4269\n",
            "Epoch [21], Batch [770/938], Loss: 0.3479\n",
            "Epoch [21], Batch [780/938], Loss: 0.4661\n",
            "Epoch [21], Batch [790/938], Loss: 0.3048\n",
            "Epoch [21], Batch [800/938], Loss: 0.4160\n",
            "Epoch [21], Batch [810/938], Loss: 0.4216\n",
            "Epoch [21], Batch [820/938], Loss: 0.2266\n",
            "Epoch [21], Batch [830/938], Loss: 0.4882\n",
            "Epoch [21], Batch [840/938], Loss: 0.4565\n",
            "Epoch [21], Batch [850/938], Loss: 0.4549\n",
            "Epoch [21], Batch [860/938], Loss: 0.2786\n",
            "Epoch [21], Batch [870/938], Loss: 0.4347\n",
            "Epoch [21], Batch [880/938], Loss: 0.3099\n",
            "Epoch [21], Batch [890/938], Loss: 0.4022\n",
            "Epoch [21], Batch [900/938], Loss: 0.3313\n",
            "Epoch [21], Batch [910/938], Loss: 0.3625\n",
            "Epoch [21], Batch [920/938], Loss: 0.2871\n",
            "Epoch [21], Batch [930/938], Loss: 0.3534\n",
            "Epoch [21], Batch [938/938], Loss: 0.3566\n",
            "Accuracy of train set: 86.06%\n",
            "Epoch [21], Batch [1/157], test Loss: 0.5131\n",
            "Epoch [21], Batch [2/157], test Loss: 0.4322\n",
            "Epoch [21], Batch [3/157], test Loss: 0.5176\n",
            "Epoch [21], Batch [4/157], test Loss: 0.7744\n",
            "Epoch [21], Batch [5/157], test Loss: 0.3981\n",
            "Epoch [21], Batch [6/157], test Loss: 0.8928\n",
            "Epoch [21], Batch [7/157], test Loss: 0.7009\n",
            "Epoch [21], Batch [8/157], test Loss: 0.5221\n",
            "Epoch [21], Batch [9/157], test Loss: 0.6445\n",
            "Epoch [21], Batch [10/157], test Loss: 0.3092\n",
            "Epoch [21], Batch [11/157], test Loss: 0.6087\n",
            "Epoch [21], Batch [12/157], test Loss: 0.3180\n",
            "Epoch [21], Batch [13/157], test Loss: 0.5994\n",
            "Epoch [21], Batch [14/157], test Loss: 0.7097\n",
            "Epoch [21], Batch [15/157], test Loss: 0.3326\n",
            "Epoch [21], Batch [16/157], test Loss: 0.2925\n",
            "Epoch [21], Batch [17/157], test Loss: 0.2639\n",
            "Epoch [21], Batch [18/157], test Loss: 0.2800\n",
            "Epoch [21], Batch [19/157], test Loss: 0.5462\n",
            "Epoch [21], Batch [20/157], test Loss: 0.4066\n",
            "Epoch [21], Batch [21/157], test Loss: 0.3877\n",
            "Epoch [21], Batch [22/157], test Loss: 0.3924\n",
            "Epoch [21], Batch [23/157], test Loss: 0.3458\n",
            "Epoch [21], Batch [24/157], test Loss: 0.4423\n",
            "Epoch [21], Batch [25/157], test Loss: 0.3113\n",
            "Epoch [21], Batch [26/157], test Loss: 0.3578\n",
            "Epoch [21], Batch [27/157], test Loss: 0.4010\n",
            "Epoch [21], Batch [28/157], test Loss: 0.3269\n",
            "Epoch [21], Batch [29/157], test Loss: 0.3794\n",
            "Epoch [21], Batch [30/157], test Loss: 0.4110\n",
            "Epoch [21], Batch [31/157], test Loss: 0.3160\n",
            "Epoch [21], Batch [32/157], test Loss: 0.2836\n",
            "Epoch [21], Batch [33/157], test Loss: 0.4483\n",
            "Epoch [21], Batch [34/157], test Loss: 0.3220\n",
            "Epoch [21], Batch [35/157], test Loss: 0.5152\n",
            "Epoch [21], Batch [36/157], test Loss: 0.5683\n",
            "Epoch [21], Batch [37/157], test Loss: 0.4766\n",
            "Epoch [21], Batch [38/157], test Loss: 0.3742\n",
            "Epoch [21], Batch [39/157], test Loss: 0.4604\n",
            "Epoch [21], Batch [40/157], test Loss: 0.3849\n",
            "Epoch [21], Batch [41/157], test Loss: 0.3607\n",
            "Epoch [21], Batch [42/157], test Loss: 0.5841\n",
            "Epoch [21], Batch [43/157], test Loss: 0.4758\n",
            "Epoch [21], Batch [44/157], test Loss: 0.5887\n",
            "Epoch [21], Batch [45/157], test Loss: 0.4374\n",
            "Epoch [21], Batch [46/157], test Loss: 0.4598\n",
            "Epoch [21], Batch [47/157], test Loss: 0.3446\n",
            "Epoch [21], Batch [48/157], test Loss: 0.2247\n",
            "Epoch [21], Batch [49/157], test Loss: 0.5647\n",
            "Epoch [21], Batch [50/157], test Loss: 0.2600\n",
            "Epoch [21], Batch [51/157], test Loss: 0.4358\n",
            "Epoch [21], Batch [52/157], test Loss: 0.3403\n",
            "Epoch [21], Batch [53/157], test Loss: 0.6310\n",
            "Epoch [21], Batch [54/157], test Loss: 0.5953\n",
            "Epoch [21], Batch [55/157], test Loss: 0.3578\n",
            "Epoch [21], Batch [56/157], test Loss: 0.4544\n",
            "Epoch [21], Batch [57/157], test Loss: 0.2935\n",
            "Epoch [21], Batch [58/157], test Loss: 0.5241\n",
            "Epoch [21], Batch [59/157], test Loss: 0.3920\n",
            "Epoch [21], Batch [60/157], test Loss: 0.5131\n",
            "Epoch [21], Batch [61/157], test Loss: 0.6222\n",
            "Epoch [21], Batch [62/157], test Loss: 0.5171\n",
            "Epoch [21], Batch [63/157], test Loss: 0.3492\n",
            "Epoch [21], Batch [64/157], test Loss: 0.4104\n",
            "Epoch [21], Batch [65/157], test Loss: 0.4915\n",
            "Epoch [21], Batch [66/157], test Loss: 0.3166\n",
            "Epoch [21], Batch [67/157], test Loss: 0.2484\n",
            "Epoch [21], Batch [68/157], test Loss: 0.3450\n",
            "Epoch [21], Batch [69/157], test Loss: 0.4473\n",
            "Epoch [21], Batch [70/157], test Loss: 0.7873\n",
            "Epoch [21], Batch [71/157], test Loss: 0.3842\n",
            "Epoch [21], Batch [72/157], test Loss: 0.3605\n",
            "Epoch [21], Batch [73/157], test Loss: 0.5009\n",
            "Epoch [21], Batch [74/157], test Loss: 0.3332\n",
            "Epoch [21], Batch [75/157], test Loss: 0.4519\n",
            "Epoch [21], Batch [76/157], test Loss: 0.4323\n",
            "Epoch [21], Batch [77/157], test Loss: 0.4749\n",
            "Epoch [21], Batch [78/157], test Loss: 0.2411\n",
            "Epoch [21], Batch [79/157], test Loss: 0.4645\n",
            "Epoch [21], Batch [80/157], test Loss: 0.4715\n",
            "Epoch [21], Batch [81/157], test Loss: 0.4712\n",
            "Epoch [21], Batch [82/157], test Loss: 0.5410\n",
            "Epoch [21], Batch [83/157], test Loss: 0.4070\n",
            "Epoch [21], Batch [84/157], test Loss: 0.4857\n",
            "Epoch [21], Batch [85/157], test Loss: 0.3418\n",
            "Epoch [21], Batch [86/157], test Loss: 0.4491\n",
            "Epoch [21], Batch [87/157], test Loss: 0.6044\n",
            "Epoch [21], Batch [88/157], test Loss: 0.4717\n",
            "Epoch [21], Batch [89/157], test Loss: 0.5657\n",
            "Epoch [21], Batch [90/157], test Loss: 0.5339\n",
            "Epoch [21], Batch [91/157], test Loss: 0.4231\n",
            "Epoch [21], Batch [92/157], test Loss: 0.2821\n",
            "Epoch [21], Batch [93/157], test Loss: 0.4086\n",
            "Epoch [21], Batch [94/157], test Loss: 0.3734\n",
            "Epoch [21], Batch [95/157], test Loss: 0.3817\n",
            "Epoch [21], Batch [96/157], test Loss: 0.4437\n",
            "Epoch [21], Batch [97/157], test Loss: 0.6033\n",
            "Epoch [21], Batch [98/157], test Loss: 0.7147\n",
            "Epoch [21], Batch [99/157], test Loss: 0.4157\n",
            "Epoch [21], Batch [100/157], test Loss: 0.5250\n",
            "Epoch [21], Batch [101/157], test Loss: 0.3403\n",
            "Epoch [21], Batch [102/157], test Loss: 0.3737\n",
            "Epoch [21], Batch [103/157], test Loss: 0.3860\n",
            "Epoch [21], Batch [104/157], test Loss: 0.2859\n",
            "Epoch [21], Batch [105/157], test Loss: 0.3605\n",
            "Epoch [21], Batch [106/157], test Loss: 0.3256\n",
            "Epoch [21], Batch [107/157], test Loss: 0.3243\n",
            "Epoch [21], Batch [108/157], test Loss: 0.3272\n",
            "Epoch [21], Batch [109/157], test Loss: 0.5019\n",
            "Epoch [21], Batch [110/157], test Loss: 0.6096\n",
            "Epoch [21], Batch [111/157], test Loss: 0.4348\n",
            "Epoch [21], Batch [112/157], test Loss: 0.3202\n",
            "Epoch [21], Batch [113/157], test Loss: 0.4751\n",
            "Epoch [21], Batch [114/157], test Loss: 0.3557\n",
            "Epoch [21], Batch [115/157], test Loss: 0.3695\n",
            "Epoch [21], Batch [116/157], test Loss: 0.4259\n",
            "Epoch [21], Batch [117/157], test Loss: 0.4257\n",
            "Epoch [21], Batch [118/157], test Loss: 0.4117\n",
            "Epoch [21], Batch [119/157], test Loss: 0.3984\n",
            "Epoch [21], Batch [120/157], test Loss: 0.3500\n",
            "Epoch [21], Batch [121/157], test Loss: 0.2929\n",
            "Epoch [21], Batch [122/157], test Loss: 0.3075\n",
            "Epoch [21], Batch [123/157], test Loss: 0.3170\n",
            "Epoch [21], Batch [124/157], test Loss: 0.5747\n",
            "Epoch [21], Batch [125/157], test Loss: 0.4753\n",
            "Epoch [21], Batch [126/157], test Loss: 0.6774\n",
            "Epoch [21], Batch [127/157], test Loss: 0.4324\n",
            "Epoch [21], Batch [128/157], test Loss: 0.4475\n",
            "Epoch [21], Batch [129/157], test Loss: 0.5759\n",
            "Epoch [21], Batch [130/157], test Loss: 0.3606\n",
            "Epoch [21], Batch [131/157], test Loss: 0.5967\n",
            "Epoch [21], Batch [132/157], test Loss: 0.4158\n",
            "Epoch [21], Batch [133/157], test Loss: 0.5174\n",
            "Epoch [21], Batch [134/157], test Loss: 0.4358\n",
            "Epoch [21], Batch [135/157], test Loss: 0.5268\n",
            "Epoch [21], Batch [136/157], test Loss: 0.2728\n",
            "Epoch [21], Batch [137/157], test Loss: 0.4360\n",
            "Epoch [21], Batch [138/157], test Loss: 0.4348\n",
            "Epoch [21], Batch [139/157], test Loss: 0.3729\n",
            "Epoch [21], Batch [140/157], test Loss: 0.4147\n",
            "Epoch [21], Batch [141/157], test Loss: 0.4483\n",
            "Epoch [21], Batch [142/157], test Loss: 0.5074\n",
            "Epoch [21], Batch [143/157], test Loss: 0.3172\n",
            "Epoch [21], Batch [144/157], test Loss: 0.6388\n",
            "Epoch [21], Batch [145/157], test Loss: 0.3328\n",
            "Epoch [21], Batch [146/157], test Loss: 0.4276\n",
            "Epoch [21], Batch [147/157], test Loss: 0.3930\n",
            "Epoch [21], Batch [148/157], test Loss: 0.5157\n",
            "Epoch [21], Batch [149/157], test Loss: 0.4520\n",
            "Epoch [21], Batch [150/157], test Loss: 0.5649\n",
            "Epoch [21], Batch [151/157], test Loss: 0.2878\n",
            "Epoch [21], Batch [152/157], test Loss: 0.3786\n",
            "Epoch [21], Batch [153/157], test Loss: 0.3127\n",
            "Epoch [21], Batch [154/157], test Loss: 0.4184\n",
            "Epoch [21], Batch [155/157], test Loss: 0.2902\n",
            "Epoch [21], Batch [156/157], test Loss: 0.4827\n",
            "Epoch [21], Batch [157/157], test Loss: 0.5244\n",
            "Accuracy of test set: 84.28%\n",
            "Epoch [22/25] - Train Loss: 0.3939, Train Accuracy: 86.06% - Test Loss: 0.4387, Test Accuracy: 84.28%\n",
            "Epoch [22], Batch [10/938], Loss: 0.5499\n",
            "Epoch [22], Batch [20/938], Loss: 0.4703\n",
            "Epoch [22], Batch [30/938], Loss: 0.3566\n",
            "Epoch [22], Batch [40/938], Loss: 0.3157\n",
            "Epoch [22], Batch [50/938], Loss: 0.4215\n",
            "Epoch [22], Batch [60/938], Loss: 0.3179\n",
            "Epoch [22], Batch [70/938], Loss: 0.4043\n",
            "Epoch [22], Batch [80/938], Loss: 0.5600\n",
            "Epoch [22], Batch [90/938], Loss: 0.2846\n",
            "Epoch [22], Batch [100/938], Loss: 0.2053\n",
            "Epoch [22], Batch [110/938], Loss: 0.4250\n",
            "Epoch [22], Batch [120/938], Loss: 0.3998\n",
            "Epoch [22], Batch [130/938], Loss: 0.3750\n",
            "Epoch [22], Batch [140/938], Loss: 0.3677\n",
            "Epoch [22], Batch [150/938], Loss: 0.3504\n",
            "Epoch [22], Batch [160/938], Loss: 0.5071\n",
            "Epoch [22], Batch [170/938], Loss: 0.4140\n",
            "Epoch [22], Batch [180/938], Loss: 0.4209\n",
            "Epoch [22], Batch [190/938], Loss: 0.5723\n",
            "Epoch [22], Batch [200/938], Loss: 0.4282\n",
            "Epoch [22], Batch [210/938], Loss: 0.4079\n",
            "Epoch [22], Batch [220/938], Loss: 0.4433\n",
            "Epoch [22], Batch [230/938], Loss: 0.3538\n",
            "Epoch [22], Batch [240/938], Loss: 0.4866\n",
            "Epoch [22], Batch [250/938], Loss: 0.4773\n",
            "Epoch [22], Batch [260/938], Loss: 0.4272\n",
            "Epoch [22], Batch [270/938], Loss: 0.4884\n",
            "Epoch [22], Batch [280/938], Loss: 0.2748\n",
            "Epoch [22], Batch [290/938], Loss: 0.2384\n",
            "Epoch [22], Batch [300/938], Loss: 0.3075\n",
            "Epoch [22], Batch [310/938], Loss: 0.5244\n",
            "Epoch [22], Batch [320/938], Loss: 0.3605\n",
            "Epoch [22], Batch [330/938], Loss: 0.4676\n",
            "Epoch [22], Batch [340/938], Loss: 0.4574\n",
            "Epoch [22], Batch [350/938], Loss: 0.2618\n",
            "Epoch [22], Batch [360/938], Loss: 0.2751\n",
            "Epoch [22], Batch [370/938], Loss: 0.2631\n",
            "Epoch [22], Batch [380/938], Loss: 0.4774\n",
            "Epoch [22], Batch [390/938], Loss: 0.2224\n",
            "Epoch [22], Batch [400/938], Loss: 0.3812\n",
            "Epoch [22], Batch [410/938], Loss: 0.4419\n",
            "Epoch [22], Batch [420/938], Loss: 0.4687\n",
            "Epoch [22], Batch [430/938], Loss: 0.4110\n",
            "Epoch [22], Batch [440/938], Loss: 0.1299\n",
            "Epoch [22], Batch [450/938], Loss: 0.2767\n",
            "Epoch [22], Batch [460/938], Loss: 0.5100\n",
            "Epoch [22], Batch [470/938], Loss: 0.4121\n",
            "Epoch [22], Batch [480/938], Loss: 0.3937\n",
            "Epoch [22], Batch [490/938], Loss: 0.2611\n",
            "Epoch [22], Batch [500/938], Loss: 0.4155\n",
            "Epoch [22], Batch [510/938], Loss: 0.3398\n",
            "Epoch [22], Batch [520/938], Loss: 0.4408\n",
            "Epoch [22], Batch [530/938], Loss: 0.2735\n",
            "Epoch [22], Batch [540/938], Loss: 0.2442\n",
            "Epoch [22], Batch [550/938], Loss: 0.3941\n",
            "Epoch [22], Batch [560/938], Loss: 0.6798\n",
            "Epoch [22], Batch [570/938], Loss: 0.3834\n",
            "Epoch [22], Batch [580/938], Loss: 0.3209\n",
            "Epoch [22], Batch [590/938], Loss: 0.4246\n",
            "Epoch [22], Batch [600/938], Loss: 0.2819\n",
            "Epoch [22], Batch [610/938], Loss: 0.5688\n",
            "Epoch [22], Batch [620/938], Loss: 0.4567\n",
            "Epoch [22], Batch [630/938], Loss: 0.2960\n",
            "Epoch [22], Batch [640/938], Loss: 0.3324\n",
            "Epoch [22], Batch [650/938], Loss: 0.5501\n",
            "Epoch [22], Batch [660/938], Loss: 0.4024\n",
            "Epoch [22], Batch [670/938], Loss: 0.4755\n",
            "Epoch [22], Batch [680/938], Loss: 0.4419\n",
            "Epoch [22], Batch [690/938], Loss: 0.5023\n",
            "Epoch [22], Batch [700/938], Loss: 0.3196\n",
            "Epoch [22], Batch [710/938], Loss: 0.2572\n",
            "Epoch [22], Batch [720/938], Loss: 0.2753\n",
            "Epoch [22], Batch [730/938], Loss: 0.3088\n",
            "Epoch [22], Batch [740/938], Loss: 0.5117\n",
            "Epoch [22], Batch [750/938], Loss: 0.4615\n",
            "Epoch [22], Batch [760/938], Loss: 0.2463\n",
            "Epoch [22], Batch [770/938], Loss: 0.2125\n",
            "Epoch [22], Batch [780/938], Loss: 0.5316\n",
            "Epoch [22], Batch [790/938], Loss: 0.3681\n",
            "Epoch [22], Batch [800/938], Loss: 0.4904\n",
            "Epoch [22], Batch [810/938], Loss: 0.4221\n",
            "Epoch [22], Batch [820/938], Loss: 0.4037\n",
            "Epoch [22], Batch [830/938], Loss: 0.3334\n",
            "Epoch [22], Batch [840/938], Loss: 0.4944\n",
            "Epoch [22], Batch [850/938], Loss: 0.3305\n",
            "Epoch [22], Batch [860/938], Loss: 0.3410\n",
            "Epoch [22], Batch [870/938], Loss: 0.5291\n",
            "Epoch [22], Batch [880/938], Loss: 0.3627\n",
            "Epoch [22], Batch [890/938], Loss: 0.3640\n",
            "Epoch [22], Batch [900/938], Loss: 0.2644\n",
            "Epoch [22], Batch [910/938], Loss: 0.2623\n",
            "Epoch [22], Batch [920/938], Loss: 0.3678\n",
            "Epoch [22], Batch [930/938], Loss: 0.3413\n",
            "Epoch [22], Batch [938/938], Loss: 0.2420\n",
            "Accuracy of train set: 86.25%\n",
            "Epoch [22], Batch [1/157], test Loss: 0.2670\n",
            "Epoch [22], Batch [2/157], test Loss: 0.3469\n",
            "Epoch [22], Batch [3/157], test Loss: 0.3477\n",
            "Epoch [22], Batch [4/157], test Loss: 0.3272\n",
            "Epoch [22], Batch [5/157], test Loss: 0.4865\n",
            "Epoch [22], Batch [6/157], test Loss: 0.5841\n",
            "Epoch [22], Batch [7/157], test Loss: 0.4355\n",
            "Epoch [22], Batch [8/157], test Loss: 0.4692\n",
            "Epoch [22], Batch [9/157], test Loss: 0.4554\n",
            "Epoch [22], Batch [10/157], test Loss: 0.3631\n",
            "Epoch [22], Batch [11/157], test Loss: 0.4977\n",
            "Epoch [22], Batch [12/157], test Loss: 0.3579\n",
            "Epoch [22], Batch [13/157], test Loss: 0.4072\n",
            "Epoch [22], Batch [14/157], test Loss: 0.4371\n",
            "Epoch [22], Batch [15/157], test Loss: 0.3157\n",
            "Epoch [22], Batch [16/157], test Loss: 0.4278\n",
            "Epoch [22], Batch [17/157], test Loss: 0.3420\n",
            "Epoch [22], Batch [18/157], test Loss: 0.3050\n",
            "Epoch [22], Batch [19/157], test Loss: 0.4776\n",
            "Epoch [22], Batch [20/157], test Loss: 0.4392\n",
            "Epoch [22], Batch [21/157], test Loss: 0.3705\n",
            "Epoch [22], Batch [22/157], test Loss: 0.3403\n",
            "Epoch [22], Batch [23/157], test Loss: 0.3052\n",
            "Epoch [22], Batch [24/157], test Loss: 0.5178\n",
            "Epoch [22], Batch [25/157], test Loss: 0.3267\n",
            "Epoch [22], Batch [26/157], test Loss: 0.2003\n",
            "Epoch [22], Batch [27/157], test Loss: 0.3786\n",
            "Epoch [22], Batch [28/157], test Loss: 0.4216\n",
            "Epoch [22], Batch [29/157], test Loss: 0.4395\n",
            "Epoch [22], Batch [30/157], test Loss: 0.5753\n",
            "Epoch [22], Batch [31/157], test Loss: 0.5089\n",
            "Epoch [22], Batch [32/157], test Loss: 0.4430\n",
            "Epoch [22], Batch [33/157], test Loss: 0.2541\n",
            "Epoch [22], Batch [34/157], test Loss: 0.4861\n",
            "Epoch [22], Batch [35/157], test Loss: 0.3826\n",
            "Epoch [22], Batch [36/157], test Loss: 0.3753\n",
            "Epoch [22], Batch [37/157], test Loss: 0.4092\n",
            "Epoch [22], Batch [38/157], test Loss: 0.4317\n",
            "Epoch [22], Batch [39/157], test Loss: 0.2790\n",
            "Epoch [22], Batch [40/157], test Loss: 0.4597\n",
            "Epoch [22], Batch [41/157], test Loss: 0.2670\n",
            "Epoch [22], Batch [42/157], test Loss: 0.5500\n",
            "Epoch [22], Batch [43/157], test Loss: 0.2318\n",
            "Epoch [22], Batch [44/157], test Loss: 0.5197\n",
            "Epoch [22], Batch [45/157], test Loss: 0.3324\n",
            "Epoch [22], Batch [46/157], test Loss: 0.5248\n",
            "Epoch [22], Batch [47/157], test Loss: 0.2328\n",
            "Epoch [22], Batch [48/157], test Loss: 0.3825\n",
            "Epoch [22], Batch [49/157], test Loss: 0.3284\n",
            "Epoch [22], Batch [50/157], test Loss: 0.4617\n",
            "Epoch [22], Batch [51/157], test Loss: 0.3171\n",
            "Epoch [22], Batch [52/157], test Loss: 0.2924\n",
            "Epoch [22], Batch [53/157], test Loss: 0.4109\n",
            "Epoch [22], Batch [54/157], test Loss: 0.4397\n",
            "Epoch [22], Batch [55/157], test Loss: 0.4676\n",
            "Epoch [22], Batch [56/157], test Loss: 0.4112\n",
            "Epoch [22], Batch [57/157], test Loss: 0.5522\n",
            "Epoch [22], Batch [58/157], test Loss: 0.4139\n",
            "Epoch [22], Batch [59/157], test Loss: 0.7513\n",
            "Epoch [22], Batch [60/157], test Loss: 0.5238\n",
            "Epoch [22], Batch [61/157], test Loss: 0.4059\n",
            "Epoch [22], Batch [62/157], test Loss: 0.3025\n",
            "Epoch [22], Batch [63/157], test Loss: 0.4683\n",
            "Epoch [22], Batch [64/157], test Loss: 0.4681\n",
            "Epoch [22], Batch [65/157], test Loss: 0.6909\n",
            "Epoch [22], Batch [66/157], test Loss: 0.1886\n",
            "Epoch [22], Batch [67/157], test Loss: 0.4065\n",
            "Epoch [22], Batch [68/157], test Loss: 0.5232\n",
            "Epoch [22], Batch [69/157], test Loss: 0.4289\n",
            "Epoch [22], Batch [70/157], test Loss: 0.4505\n",
            "Epoch [22], Batch [71/157], test Loss: 0.9106\n",
            "Epoch [22], Batch [72/157], test Loss: 0.4105\n",
            "Epoch [22], Batch [73/157], test Loss: 0.2394\n",
            "Epoch [22], Batch [74/157], test Loss: 0.4896\n",
            "Epoch [22], Batch [75/157], test Loss: 0.6633\n",
            "Epoch [22], Batch [76/157], test Loss: 0.6676\n",
            "Epoch [22], Batch [77/157], test Loss: 0.5356\n",
            "Epoch [22], Batch [78/157], test Loss: 0.5808\n",
            "Epoch [22], Batch [79/157], test Loss: 0.5910\n",
            "Epoch [22], Batch [80/157], test Loss: 0.3948\n",
            "Epoch [22], Batch [81/157], test Loss: 0.4116\n",
            "Epoch [22], Batch [82/157], test Loss: 0.4271\n",
            "Epoch [22], Batch [83/157], test Loss: 0.2814\n",
            "Epoch [22], Batch [84/157], test Loss: 0.3508\n",
            "Epoch [22], Batch [85/157], test Loss: 0.3638\n",
            "Epoch [22], Batch [86/157], test Loss: 0.2674\n",
            "Epoch [22], Batch [87/157], test Loss: 0.2818\n",
            "Epoch [22], Batch [88/157], test Loss: 0.3435\n",
            "Epoch [22], Batch [89/157], test Loss: 0.4899\n",
            "Epoch [22], Batch [90/157], test Loss: 0.4805\n",
            "Epoch [22], Batch [91/157], test Loss: 0.7681\n",
            "Epoch [22], Batch [92/157], test Loss: 0.3395\n",
            "Epoch [22], Batch [93/157], test Loss: 0.3550\n",
            "Epoch [22], Batch [94/157], test Loss: 0.2913\n",
            "Epoch [22], Batch [95/157], test Loss: 0.4255\n",
            "Epoch [22], Batch [96/157], test Loss: 0.5124\n",
            "Epoch [22], Batch [97/157], test Loss: 0.2345\n",
            "Epoch [22], Batch [98/157], test Loss: 0.5079\n",
            "Epoch [22], Batch [99/157], test Loss: 0.2594\n",
            "Epoch [22], Batch [100/157], test Loss: 0.4074\n",
            "Epoch [22], Batch [101/157], test Loss: 0.4207\n",
            "Epoch [22], Batch [102/157], test Loss: 0.4375\n",
            "Epoch [22], Batch [103/157], test Loss: 0.2205\n",
            "Epoch [22], Batch [104/157], test Loss: 0.5476\n",
            "Epoch [22], Batch [105/157], test Loss: 0.5440\n",
            "Epoch [22], Batch [106/157], test Loss: 0.4117\n",
            "Epoch [22], Batch [107/157], test Loss: 0.5593\n",
            "Epoch [22], Batch [108/157], test Loss: 0.4753\n",
            "Epoch [22], Batch [109/157], test Loss: 0.5170\n",
            "Epoch [22], Batch [110/157], test Loss: 0.3582\n",
            "Epoch [22], Batch [111/157], test Loss: 0.3456\n",
            "Epoch [22], Batch [112/157], test Loss: 0.3641\n",
            "Epoch [22], Batch [113/157], test Loss: 0.5037\n",
            "Epoch [22], Batch [114/157], test Loss: 0.2918\n",
            "Epoch [22], Batch [115/157], test Loss: 0.4480\n",
            "Epoch [22], Batch [116/157], test Loss: 0.3220\n",
            "Epoch [22], Batch [117/157], test Loss: 0.3281\n",
            "Epoch [22], Batch [118/157], test Loss: 0.3906\n",
            "Epoch [22], Batch [119/157], test Loss: 0.3885\n",
            "Epoch [22], Batch [120/157], test Loss: 0.3621\n",
            "Epoch [22], Batch [121/157], test Loss: 0.4847\n",
            "Epoch [22], Batch [122/157], test Loss: 0.4264\n",
            "Epoch [22], Batch [123/157], test Loss: 0.3417\n",
            "Epoch [22], Batch [124/157], test Loss: 0.3895\n",
            "Epoch [22], Batch [125/157], test Loss: 0.4990\n",
            "Epoch [22], Batch [126/157], test Loss: 0.5117\n",
            "Epoch [22], Batch [127/157], test Loss: 0.5048\n",
            "Epoch [22], Batch [128/157], test Loss: 0.4049\n",
            "Epoch [22], Batch [129/157], test Loss: 0.4583\n",
            "Epoch [22], Batch [130/157], test Loss: 0.4301\n",
            "Epoch [22], Batch [131/157], test Loss: 0.5573\n",
            "Epoch [22], Batch [132/157], test Loss: 0.6121\n",
            "Epoch [22], Batch [133/157], test Loss: 0.6163\n",
            "Epoch [22], Batch [134/157], test Loss: 0.4167\n",
            "Epoch [22], Batch [135/157], test Loss: 0.4590\n",
            "Epoch [22], Batch [136/157], test Loss: 0.4148\n",
            "Epoch [22], Batch [137/157], test Loss: 0.3518\n",
            "Epoch [22], Batch [138/157], test Loss: 0.5221\n",
            "Epoch [22], Batch [139/157], test Loss: 0.4820\n",
            "Epoch [22], Batch [140/157], test Loss: 0.2876\n",
            "Epoch [22], Batch [141/157], test Loss: 0.3401\n",
            "Epoch [22], Batch [142/157], test Loss: 0.1982\n",
            "Epoch [22], Batch [143/157], test Loss: 0.4486\n",
            "Epoch [22], Batch [144/157], test Loss: 0.4418\n",
            "Epoch [22], Batch [145/157], test Loss: 0.3801\n",
            "Epoch [22], Batch [146/157], test Loss: 0.2248\n",
            "Epoch [22], Batch [147/157], test Loss: 0.5407\n",
            "Epoch [22], Batch [148/157], test Loss: 0.5110\n",
            "Epoch [22], Batch [149/157], test Loss: 0.3930\n",
            "Epoch [22], Batch [150/157], test Loss: 0.2675\n",
            "Epoch [22], Batch [151/157], test Loss: 0.4646\n",
            "Epoch [22], Batch [152/157], test Loss: 0.4942\n",
            "Epoch [22], Batch [153/157], test Loss: 0.5510\n",
            "Epoch [22], Batch [154/157], test Loss: 0.3733\n",
            "Epoch [22], Batch [155/157], test Loss: 0.4941\n",
            "Epoch [22], Batch [156/157], test Loss: 0.4971\n",
            "Epoch [22], Batch [157/157], test Loss: 0.2401\n",
            "Accuracy of test set: 84.86%\n",
            "Epoch [23/25] - Train Loss: 0.3874, Train Accuracy: 86.25% - Test Loss: 0.4222, Test Accuracy: 84.86%\n",
            "Epoch [23], Batch [10/938], Loss: 0.3250\n",
            "Epoch [23], Batch [20/938], Loss: 0.3770\n",
            "Epoch [23], Batch [30/938], Loss: 0.2948\n",
            "Epoch [23], Batch [40/938], Loss: 0.6700\n",
            "Epoch [23], Batch [50/938], Loss: 0.3408\n",
            "Epoch [23], Batch [60/938], Loss: 0.4182\n",
            "Epoch [23], Batch [70/938], Loss: 0.4291\n",
            "Epoch [23], Batch [80/938], Loss: 0.5326\n",
            "Epoch [23], Batch [90/938], Loss: 0.3758\n",
            "Epoch [23], Batch [100/938], Loss: 0.5189\n",
            "Epoch [23], Batch [110/938], Loss: 0.3972\n",
            "Epoch [23], Batch [120/938], Loss: 0.2213\n",
            "Epoch [23], Batch [130/938], Loss: 0.5083\n",
            "Epoch [23], Batch [140/938], Loss: 0.2316\n",
            "Epoch [23], Batch [150/938], Loss: 0.3812\n",
            "Epoch [23], Batch [160/938], Loss: 0.3898\n",
            "Epoch [23], Batch [170/938], Loss: 0.3550\n",
            "Epoch [23], Batch [180/938], Loss: 0.3044\n",
            "Epoch [23], Batch [190/938], Loss: 0.4231\n",
            "Epoch [23], Batch [200/938], Loss: 0.2945\n",
            "Epoch [23], Batch [210/938], Loss: 0.3256\n",
            "Epoch [23], Batch [220/938], Loss: 0.4580\n",
            "Epoch [23], Batch [230/938], Loss: 0.3507\n",
            "Epoch [23], Batch [240/938], Loss: 0.4605\n",
            "Epoch [23], Batch [250/938], Loss: 0.4448\n",
            "Epoch [23], Batch [260/938], Loss: 0.3153\n",
            "Epoch [23], Batch [270/938], Loss: 0.5970\n",
            "Epoch [23], Batch [280/938], Loss: 0.5350\n",
            "Epoch [23], Batch [290/938], Loss: 0.4641\n",
            "Epoch [23], Batch [300/938], Loss: 0.3171\n",
            "Epoch [23], Batch [310/938], Loss: 0.3692\n",
            "Epoch [23], Batch [320/938], Loss: 0.4337\n",
            "Epoch [23], Batch [330/938], Loss: 0.4733\n",
            "Epoch [23], Batch [340/938], Loss: 0.4380\n",
            "Epoch [23], Batch [350/938], Loss: 0.3529\n",
            "Epoch [23], Batch [360/938], Loss: 0.4586\n",
            "Epoch [23], Batch [370/938], Loss: 0.2773\n",
            "Epoch [23], Batch [380/938], Loss: 0.3848\n",
            "Epoch [23], Batch [390/938], Loss: 0.5782\n",
            "Epoch [23], Batch [400/938], Loss: 0.2798\n",
            "Epoch [23], Batch [410/938], Loss: 0.3744\n",
            "Epoch [23], Batch [420/938], Loss: 0.4646\n",
            "Epoch [23], Batch [430/938], Loss: 0.4047\n",
            "Epoch [23], Batch [440/938], Loss: 0.3215\n",
            "Epoch [23], Batch [450/938], Loss: 0.3599\n",
            "Epoch [23], Batch [460/938], Loss: 0.3669\n",
            "Epoch [23], Batch [470/938], Loss: 0.4164\n",
            "Epoch [23], Batch [480/938], Loss: 0.4980\n",
            "Epoch [23], Batch [490/938], Loss: 0.2297\n",
            "Epoch [23], Batch [500/938], Loss: 0.2895\n",
            "Epoch [23], Batch [510/938], Loss: 0.4210\n",
            "Epoch [23], Batch [520/938], Loss: 0.4280\n",
            "Epoch [23], Batch [530/938], Loss: 0.2543\n",
            "Epoch [23], Batch [540/938], Loss: 0.3056\n",
            "Epoch [23], Batch [550/938], Loss: 0.5029\n",
            "Epoch [23], Batch [560/938], Loss: 0.3148\n",
            "Epoch [23], Batch [570/938], Loss: 0.3722\n",
            "Epoch [23], Batch [580/938], Loss: 0.4092\n",
            "Epoch [23], Batch [590/938], Loss: 0.3421\n",
            "Epoch [23], Batch [600/938], Loss: 0.4008\n",
            "Epoch [23], Batch [610/938], Loss: 0.4133\n",
            "Epoch [23], Batch [620/938], Loss: 0.3997\n",
            "Epoch [23], Batch [630/938], Loss: 0.3055\n",
            "Epoch [23], Batch [640/938], Loss: 0.4067\n",
            "Epoch [23], Batch [650/938], Loss: 0.5084\n",
            "Epoch [23], Batch [660/938], Loss: 0.4806\n",
            "Epoch [23], Batch [670/938], Loss: 0.4768\n",
            "Epoch [23], Batch [680/938], Loss: 0.4022\n",
            "Epoch [23], Batch [690/938], Loss: 0.3516\n",
            "Epoch [23], Batch [700/938], Loss: 0.2740\n",
            "Epoch [23], Batch [710/938], Loss: 0.3012\n",
            "Epoch [23], Batch [720/938], Loss: 0.2896\n",
            "Epoch [23], Batch [730/938], Loss: 0.3938\n",
            "Epoch [23], Batch [740/938], Loss: 0.5243\n",
            "Epoch [23], Batch [750/938], Loss: 0.3530\n",
            "Epoch [23], Batch [760/938], Loss: 0.4901\n",
            "Epoch [23], Batch [770/938], Loss: 0.5124\n",
            "Epoch [23], Batch [780/938], Loss: 0.2795\n",
            "Epoch [23], Batch [790/938], Loss: 0.3546\n",
            "Epoch [23], Batch [800/938], Loss: 0.4582\n",
            "Epoch [23], Batch [810/938], Loss: 0.1383\n",
            "Epoch [23], Batch [820/938], Loss: 0.5235\n",
            "Epoch [23], Batch [830/938], Loss: 0.2711\n",
            "Epoch [23], Batch [840/938], Loss: 0.5621\n",
            "Epoch [23], Batch [850/938], Loss: 0.3400\n",
            "Epoch [23], Batch [860/938], Loss: 0.3882\n",
            "Epoch [23], Batch [870/938], Loss: 0.3285\n",
            "Epoch [23], Batch [880/938], Loss: 0.4420\n",
            "Epoch [23], Batch [890/938], Loss: 0.5078\n",
            "Epoch [23], Batch [900/938], Loss: 0.3179\n",
            "Epoch [23], Batch [910/938], Loss: 0.5196\n",
            "Epoch [23], Batch [920/938], Loss: 0.3385\n",
            "Epoch [23], Batch [930/938], Loss: 0.4913\n",
            "Epoch [23], Batch [938/938], Loss: 0.3017\n",
            "Accuracy of train set: 86.55%\n",
            "Epoch [23], Batch [1/157], test Loss: 0.3533\n",
            "Epoch [23], Batch [2/157], test Loss: 0.6023\n",
            "Epoch [23], Batch [3/157], test Loss: 0.4776\n",
            "Epoch [23], Batch [4/157], test Loss: 0.4625\n",
            "Epoch [23], Batch [5/157], test Loss: 0.3785\n",
            "Epoch [23], Batch [6/157], test Loss: 0.4610\n",
            "Epoch [23], Batch [7/157], test Loss: 0.4453\n",
            "Epoch [23], Batch [8/157], test Loss: 0.3460\n",
            "Epoch [23], Batch [9/157], test Loss: 0.5352\n",
            "Epoch [23], Batch [10/157], test Loss: 0.4926\n",
            "Epoch [23], Batch [11/157], test Loss: 0.5978\n",
            "Epoch [23], Batch [12/157], test Loss: 0.4883\n",
            "Epoch [23], Batch [13/157], test Loss: 0.5954\n",
            "Epoch [23], Batch [14/157], test Loss: 0.4978\n",
            "Epoch [23], Batch [15/157], test Loss: 0.4581\n",
            "Epoch [23], Batch [16/157], test Loss: 0.3028\n",
            "Epoch [23], Batch [17/157], test Loss: 0.3322\n",
            "Epoch [23], Batch [18/157], test Loss: 0.3298\n",
            "Epoch [23], Batch [19/157], test Loss: 0.3283\n",
            "Epoch [23], Batch [20/157], test Loss: 0.3805\n",
            "Epoch [23], Batch [21/157], test Loss: 0.3157\n",
            "Epoch [23], Batch [22/157], test Loss: 0.2799\n",
            "Epoch [23], Batch [23/157], test Loss: 0.3185\n",
            "Epoch [23], Batch [24/157], test Loss: 0.4272\n",
            "Epoch [23], Batch [25/157], test Loss: 0.4073\n",
            "Epoch [23], Batch [26/157], test Loss: 0.4680\n",
            "Epoch [23], Batch [27/157], test Loss: 0.3943\n",
            "Epoch [23], Batch [28/157], test Loss: 0.5292\n",
            "Epoch [23], Batch [29/157], test Loss: 0.5519\n",
            "Epoch [23], Batch [30/157], test Loss: 0.1879\n",
            "Epoch [23], Batch [31/157], test Loss: 0.5231\n",
            "Epoch [23], Batch [32/157], test Loss: 0.5056\n",
            "Epoch [23], Batch [33/157], test Loss: 0.3585\n",
            "Epoch [23], Batch [34/157], test Loss: 0.3200\n",
            "Epoch [23], Batch [35/157], test Loss: 0.3569\n",
            "Epoch [23], Batch [36/157], test Loss: 0.4739\n",
            "Epoch [23], Batch [37/157], test Loss: 0.4692\n",
            "Epoch [23], Batch [38/157], test Loss: 0.4093\n",
            "Epoch [23], Batch [39/157], test Loss: 0.4863\n",
            "Epoch [23], Batch [40/157], test Loss: 0.6366\n",
            "Epoch [23], Batch [41/157], test Loss: 0.5910\n",
            "Epoch [23], Batch [42/157], test Loss: 0.5526\n",
            "Epoch [23], Batch [43/157], test Loss: 0.5724\n",
            "Epoch [23], Batch [44/157], test Loss: 0.4244\n",
            "Epoch [23], Batch [45/157], test Loss: 0.7732\n",
            "Epoch [23], Batch [46/157], test Loss: 0.5995\n",
            "Epoch [23], Batch [47/157], test Loss: 0.4594\n",
            "Epoch [23], Batch [48/157], test Loss: 0.3739\n",
            "Epoch [23], Batch [49/157], test Loss: 0.5417\n",
            "Epoch [23], Batch [50/157], test Loss: 0.5029\n",
            "Epoch [23], Batch [51/157], test Loss: 0.4006\n",
            "Epoch [23], Batch [52/157], test Loss: 0.5603\n",
            "Epoch [23], Batch [53/157], test Loss: 0.4313\n",
            "Epoch [23], Batch [54/157], test Loss: 0.4584\n",
            "Epoch [23], Batch [55/157], test Loss: 0.2924\n",
            "Epoch [23], Batch [56/157], test Loss: 0.2942\n",
            "Epoch [23], Batch [57/157], test Loss: 0.5810\n",
            "Epoch [23], Batch [58/157], test Loss: 0.4177\n",
            "Epoch [23], Batch [59/157], test Loss: 0.6087\n",
            "Epoch [23], Batch [60/157], test Loss: 0.5920\n",
            "Epoch [23], Batch [61/157], test Loss: 0.3881\n",
            "Epoch [23], Batch [62/157], test Loss: 0.3633\n",
            "Epoch [23], Batch [63/157], test Loss: 0.4489\n",
            "Epoch [23], Batch [64/157], test Loss: 0.3710\n",
            "Epoch [23], Batch [65/157], test Loss: 0.2587\n",
            "Epoch [23], Batch [66/157], test Loss: 0.4865\n",
            "Epoch [23], Batch [67/157], test Loss: 0.6850\n",
            "Epoch [23], Batch [68/157], test Loss: 0.4230\n",
            "Epoch [23], Batch [69/157], test Loss: 0.7327\n",
            "Epoch [23], Batch [70/157], test Loss: 0.3759\n",
            "Epoch [23], Batch [71/157], test Loss: 0.4850\n",
            "Epoch [23], Batch [72/157], test Loss: 0.3573\n",
            "Epoch [23], Batch [73/157], test Loss: 0.3256\n",
            "Epoch [23], Batch [74/157], test Loss: 0.3235\n",
            "Epoch [23], Batch [75/157], test Loss: 0.5661\n",
            "Epoch [23], Batch [76/157], test Loss: 0.3040\n",
            "Epoch [23], Batch [77/157], test Loss: 0.3477\n",
            "Epoch [23], Batch [78/157], test Loss: 0.2831\n",
            "Epoch [23], Batch [79/157], test Loss: 0.5533\n",
            "Epoch [23], Batch [80/157], test Loss: 0.3175\n",
            "Epoch [23], Batch [81/157], test Loss: 0.2318\n",
            "Epoch [23], Batch [82/157], test Loss: 0.5142\n",
            "Epoch [23], Batch [83/157], test Loss: 0.4785\n",
            "Epoch [23], Batch [84/157], test Loss: 0.3626\n",
            "Epoch [23], Batch [85/157], test Loss: 0.2726\n",
            "Epoch [23], Batch [86/157], test Loss: 0.3117\n",
            "Epoch [23], Batch [87/157], test Loss: 0.3421\n",
            "Epoch [23], Batch [88/157], test Loss: 0.3878\n",
            "Epoch [23], Batch [89/157], test Loss: 0.2766\n",
            "Epoch [23], Batch [90/157], test Loss: 0.3109\n",
            "Epoch [23], Batch [91/157], test Loss: 0.4941\n",
            "Epoch [23], Batch [92/157], test Loss: 0.3266\n",
            "Epoch [23], Batch [93/157], test Loss: 0.2688\n",
            "Epoch [23], Batch [94/157], test Loss: 0.3510\n",
            "Epoch [23], Batch [95/157], test Loss: 0.3061\n",
            "Epoch [23], Batch [96/157], test Loss: 0.4095\n",
            "Epoch [23], Batch [97/157], test Loss: 0.4346\n",
            "Epoch [23], Batch [98/157], test Loss: 0.2581\n",
            "Epoch [23], Batch [99/157], test Loss: 0.3250\n",
            "Epoch [23], Batch [100/157], test Loss: 0.3756\n",
            "Epoch [23], Batch [101/157], test Loss: 0.6339\n",
            "Epoch [23], Batch [102/157], test Loss: 0.2937\n",
            "Epoch [23], Batch [103/157], test Loss: 0.4754\n",
            "Epoch [23], Batch [104/157], test Loss: 0.6738\n",
            "Epoch [23], Batch [105/157], test Loss: 0.4350\n",
            "Epoch [23], Batch [106/157], test Loss: 0.3206\n",
            "Epoch [23], Batch [107/157], test Loss: 0.6299\n",
            "Epoch [23], Batch [108/157], test Loss: 0.4239\n",
            "Epoch [23], Batch [109/157], test Loss: 0.4788\n",
            "Epoch [23], Batch [110/157], test Loss: 0.2525\n",
            "Epoch [23], Batch [111/157], test Loss: 0.3234\n",
            "Epoch [23], Batch [112/157], test Loss: 0.3549\n",
            "Epoch [23], Batch [113/157], test Loss: 0.4553\n",
            "Epoch [23], Batch [114/157], test Loss: 0.3574\n",
            "Epoch [23], Batch [115/157], test Loss: 0.5393\n",
            "Epoch [23], Batch [116/157], test Loss: 0.3534\n",
            "Epoch [23], Batch [117/157], test Loss: 0.6261\n",
            "Epoch [23], Batch [118/157], test Loss: 0.4044\n",
            "Epoch [23], Batch [119/157], test Loss: 0.3108\n",
            "Epoch [23], Batch [120/157], test Loss: 0.3411\n",
            "Epoch [23], Batch [121/157], test Loss: 0.2829\n",
            "Epoch [23], Batch [122/157], test Loss: 0.2420\n",
            "Epoch [23], Batch [123/157], test Loss: 0.3666\n",
            "Epoch [23], Batch [124/157], test Loss: 0.4488\n",
            "Epoch [23], Batch [125/157], test Loss: 0.4103\n",
            "Epoch [23], Batch [126/157], test Loss: 0.4279\n",
            "Epoch [23], Batch [127/157], test Loss: 0.3942\n",
            "Epoch [23], Batch [128/157], test Loss: 0.4484\n",
            "Epoch [23], Batch [129/157], test Loss: 0.2481\n",
            "Epoch [23], Batch [130/157], test Loss: 0.4044\n",
            "Epoch [23], Batch [131/157], test Loss: 0.4555\n",
            "Epoch [23], Batch [132/157], test Loss: 0.2746\n",
            "Epoch [23], Batch [133/157], test Loss: 0.5502\n",
            "Epoch [23], Batch [134/157], test Loss: 0.4318\n",
            "Epoch [23], Batch [135/157], test Loss: 0.3672\n",
            "Epoch [23], Batch [136/157], test Loss: 0.7809\n",
            "Epoch [23], Batch [137/157], test Loss: 0.6621\n",
            "Epoch [23], Batch [138/157], test Loss: 0.3582\n",
            "Epoch [23], Batch [139/157], test Loss: 0.3592\n",
            "Epoch [23], Batch [140/157], test Loss: 0.4221\n",
            "Epoch [23], Batch [141/157], test Loss: 0.6495\n",
            "Epoch [23], Batch [142/157], test Loss: 0.5614\n",
            "Epoch [23], Batch [143/157], test Loss: 0.5903\n",
            "Epoch [23], Batch [144/157], test Loss: 0.4202\n",
            "Epoch [23], Batch [145/157], test Loss: 0.4413\n",
            "Epoch [23], Batch [146/157], test Loss: 0.5326\n",
            "Epoch [23], Batch [147/157], test Loss: 0.4659\n",
            "Epoch [23], Batch [148/157], test Loss: 0.4555\n",
            "Epoch [23], Batch [149/157], test Loss: 0.3402\n",
            "Epoch [23], Batch [150/157], test Loss: 0.5324\n",
            "Epoch [23], Batch [151/157], test Loss: 0.3779\n",
            "Epoch [23], Batch [152/157], test Loss: 0.3941\n",
            "Epoch [23], Batch [153/157], test Loss: 0.4927\n",
            "Epoch [23], Batch [154/157], test Loss: 0.3346\n",
            "Epoch [23], Batch [155/157], test Loss: 0.3454\n",
            "Epoch [23], Batch [156/157], test Loss: 0.2490\n",
            "Epoch [23], Batch [157/157], test Loss: 0.5533\n",
            "Accuracy of test set: 84.70%\n",
            "Epoch [24/25] - Train Loss: 0.3800, Train Accuracy: 86.55% - Test Loss: 0.4300, Test Accuracy: 84.70%\n",
            "Epoch [24], Batch [10/938], Loss: 0.3095\n",
            "Epoch [24], Batch [20/938], Loss: 0.2629\n",
            "Epoch [24], Batch [30/938], Loss: 0.3177\n",
            "Epoch [24], Batch [40/938], Loss: 0.3950\n",
            "Epoch [24], Batch [50/938], Loss: 0.2092\n",
            "Epoch [24], Batch [60/938], Loss: 0.4043\n",
            "Epoch [24], Batch [70/938], Loss: 0.4103\n",
            "Epoch [24], Batch [80/938], Loss: 0.2157\n",
            "Epoch [24], Batch [90/938], Loss: 0.3076\n",
            "Epoch [24], Batch [100/938], Loss: 0.5875\n",
            "Epoch [24], Batch [110/938], Loss: 0.3571\n",
            "Epoch [24], Batch [120/938], Loss: 0.5110\n",
            "Epoch [24], Batch [130/938], Loss: 0.3782\n",
            "Epoch [24], Batch [140/938], Loss: 0.4647\n",
            "Epoch [24], Batch [150/938], Loss: 0.3792\n",
            "Epoch [24], Batch [160/938], Loss: 0.3745\n",
            "Epoch [24], Batch [170/938], Loss: 0.4252\n",
            "Epoch [24], Batch [180/938], Loss: 0.1903\n",
            "Epoch [24], Batch [190/938], Loss: 0.3584\n",
            "Epoch [24], Batch [200/938], Loss: 0.2682\n",
            "Epoch [24], Batch [210/938], Loss: 0.3572\n",
            "Epoch [24], Batch [220/938], Loss: 0.3404\n",
            "Epoch [24], Batch [230/938], Loss: 0.4098\n",
            "Epoch [24], Batch [240/938], Loss: 0.4752\n",
            "Epoch [24], Batch [250/938], Loss: 0.3854\n",
            "Epoch [24], Batch [260/938], Loss: 0.6664\n",
            "Epoch [24], Batch [270/938], Loss: 0.2713\n",
            "Epoch [24], Batch [280/938], Loss: 0.3192\n",
            "Epoch [24], Batch [290/938], Loss: 0.2685\n",
            "Epoch [24], Batch [300/938], Loss: 0.3766\n",
            "Epoch [24], Batch [310/938], Loss: 0.3411\n",
            "Epoch [24], Batch [320/938], Loss: 0.4135\n",
            "Epoch [24], Batch [330/938], Loss: 0.4283\n",
            "Epoch [24], Batch [340/938], Loss: 0.4794\n",
            "Epoch [24], Batch [350/938], Loss: 0.3999\n",
            "Epoch [24], Batch [360/938], Loss: 0.4404\n",
            "Epoch [24], Batch [370/938], Loss: 0.3727\n",
            "Epoch [24], Batch [380/938], Loss: 0.4651\n",
            "Epoch [24], Batch [390/938], Loss: 0.3650\n",
            "Epoch [24], Batch [400/938], Loss: 0.4384\n",
            "Epoch [24], Batch [410/938], Loss: 0.4539\n",
            "Epoch [24], Batch [420/938], Loss: 0.3717\n",
            "Epoch [24], Batch [430/938], Loss: 0.4814\n",
            "Epoch [24], Batch [440/938], Loss: 0.3069\n",
            "Epoch [24], Batch [450/938], Loss: 0.4743\n",
            "Epoch [24], Batch [460/938], Loss: 0.2046\n",
            "Epoch [24], Batch [470/938], Loss: 0.4416\n",
            "Epoch [24], Batch [480/938], Loss: 0.3220\n",
            "Epoch [24], Batch [490/938], Loss: 0.3598\n",
            "Epoch [24], Batch [500/938], Loss: 0.2668\n",
            "Epoch [24], Batch [510/938], Loss: 0.3582\n",
            "Epoch [24], Batch [520/938], Loss: 0.2729\n",
            "Epoch [24], Batch [530/938], Loss: 0.4205\n",
            "Epoch [24], Batch [540/938], Loss: 0.3051\n",
            "Epoch [24], Batch [550/938], Loss: 0.5320\n",
            "Epoch [24], Batch [560/938], Loss: 0.4380\n",
            "Epoch [24], Batch [570/938], Loss: 0.5146\n",
            "Epoch [24], Batch [580/938], Loss: 0.2806\n",
            "Epoch [24], Batch [590/938], Loss: 0.4413\n",
            "Epoch [24], Batch [600/938], Loss: 0.3407\n",
            "Epoch [24], Batch [610/938], Loss: 0.3537\n",
            "Epoch [24], Batch [620/938], Loss: 0.2651\n",
            "Epoch [24], Batch [630/938], Loss: 0.2701\n",
            "Epoch [24], Batch [640/938], Loss: 0.4135\n",
            "Epoch [24], Batch [650/938], Loss: 0.3804\n",
            "Epoch [24], Batch [660/938], Loss: 0.3265\n",
            "Epoch [24], Batch [670/938], Loss: 0.3877\n",
            "Epoch [24], Batch [680/938], Loss: 0.3634\n",
            "Epoch [24], Batch [690/938], Loss: 0.2202\n",
            "Epoch [24], Batch [700/938], Loss: 0.3536\n",
            "Epoch [24], Batch [710/938], Loss: 0.2423\n",
            "Epoch [24], Batch [720/938], Loss: 0.3517\n",
            "Epoch [24], Batch [730/938], Loss: 0.1753\n",
            "Epoch [24], Batch [740/938], Loss: 0.2645\n",
            "Epoch [24], Batch [750/938], Loss: 0.3269\n",
            "Epoch [24], Batch [760/938], Loss: 0.3905\n",
            "Epoch [24], Batch [770/938], Loss: 0.1924\n",
            "Epoch [24], Batch [780/938], Loss: 0.2889\n",
            "Epoch [24], Batch [790/938], Loss: 0.2882\n",
            "Epoch [24], Batch [800/938], Loss: 0.5591\n",
            "Epoch [24], Batch [810/938], Loss: 0.1757\n",
            "Epoch [24], Batch [820/938], Loss: 0.2204\n",
            "Epoch [24], Batch [830/938], Loss: 0.5252\n",
            "Epoch [24], Batch [840/938], Loss: 0.4241\n",
            "Epoch [24], Batch [850/938], Loss: 0.4653\n",
            "Epoch [24], Batch [860/938], Loss: 0.2476\n",
            "Epoch [24], Batch [870/938], Loss: 0.2322\n",
            "Epoch [24], Batch [880/938], Loss: 0.4761\n",
            "Epoch [24], Batch [890/938], Loss: 0.3310\n",
            "Epoch [24], Batch [900/938], Loss: 0.2370\n",
            "Epoch [24], Batch [910/938], Loss: 0.3180\n",
            "Epoch [24], Batch [920/938], Loss: 0.2715\n",
            "Epoch [24], Batch [930/938], Loss: 0.3018\n",
            "Epoch [24], Batch [938/938], Loss: 0.2578\n",
            "Accuracy of train set: 86.71%\n",
            "Epoch [24], Batch [1/157], test Loss: 0.6768\n",
            "Epoch [24], Batch [2/157], test Loss: 0.3149\n",
            "Epoch [24], Batch [3/157], test Loss: 0.4153\n",
            "Epoch [24], Batch [4/157], test Loss: 0.2205\n",
            "Epoch [24], Batch [5/157], test Loss: 0.2292\n",
            "Epoch [24], Batch [6/157], test Loss: 0.2683\n",
            "Epoch [24], Batch [7/157], test Loss: 0.6467\n",
            "Epoch [24], Batch [8/157], test Loss: 0.2838\n",
            "Epoch [24], Batch [9/157], test Loss: 0.5612\n",
            "Epoch [24], Batch [10/157], test Loss: 0.4130\n",
            "Epoch [24], Batch [11/157], test Loss: 0.2083\n",
            "Epoch [24], Batch [12/157], test Loss: 0.3869\n",
            "Epoch [24], Batch [13/157], test Loss: 0.5436\n",
            "Epoch [24], Batch [14/157], test Loss: 0.3132\n",
            "Epoch [24], Batch [15/157], test Loss: 0.3022\n",
            "Epoch [24], Batch [16/157], test Loss: 0.5343\n",
            "Epoch [24], Batch [17/157], test Loss: 0.3826\n",
            "Epoch [24], Batch [18/157], test Loss: 0.2977\n",
            "Epoch [24], Batch [19/157], test Loss: 0.4564\n",
            "Epoch [24], Batch [20/157], test Loss: 0.2961\n",
            "Epoch [24], Batch [21/157], test Loss: 0.3958\n",
            "Epoch [24], Batch [22/157], test Loss: 0.3477\n",
            "Epoch [24], Batch [23/157], test Loss: 0.5080\n",
            "Epoch [24], Batch [24/157], test Loss: 0.4495\n",
            "Epoch [24], Batch [25/157], test Loss: 0.5317\n",
            "Epoch [24], Batch [26/157], test Loss: 0.3328\n",
            "Epoch [24], Batch [27/157], test Loss: 0.3225\n",
            "Epoch [24], Batch [28/157], test Loss: 0.3936\n",
            "Epoch [24], Batch [29/157], test Loss: 0.3544\n",
            "Epoch [24], Batch [30/157], test Loss: 0.4655\n",
            "Epoch [24], Batch [31/157], test Loss: 0.4560\n",
            "Epoch [24], Batch [32/157], test Loss: 0.5350\n",
            "Epoch [24], Batch [33/157], test Loss: 0.5616\n",
            "Epoch [24], Batch [34/157], test Loss: 0.3441\n",
            "Epoch [24], Batch [35/157], test Loss: 0.4097\n",
            "Epoch [24], Batch [36/157], test Loss: 0.5282\n",
            "Epoch [24], Batch [37/157], test Loss: 0.3896\n",
            "Epoch [24], Batch [38/157], test Loss: 0.3615\n",
            "Epoch [24], Batch [39/157], test Loss: 0.5129\n",
            "Epoch [24], Batch [40/157], test Loss: 0.4644\n",
            "Epoch [24], Batch [41/157], test Loss: 0.5080\n",
            "Epoch [24], Batch [42/157], test Loss: 0.3033\n",
            "Epoch [24], Batch [43/157], test Loss: 0.3210\n",
            "Epoch [24], Batch [44/157], test Loss: 0.4306\n",
            "Epoch [24], Batch [45/157], test Loss: 0.5203\n",
            "Epoch [24], Batch [46/157], test Loss: 0.3817\n",
            "Epoch [24], Batch [47/157], test Loss: 0.5190\n",
            "Epoch [24], Batch [48/157], test Loss: 0.3866\n",
            "Epoch [24], Batch [49/157], test Loss: 0.3369\n",
            "Epoch [24], Batch [50/157], test Loss: 0.3947\n",
            "Epoch [24], Batch [51/157], test Loss: 0.3416\n",
            "Epoch [24], Batch [52/157], test Loss: 0.4476\n",
            "Epoch [24], Batch [53/157], test Loss: 0.2998\n",
            "Epoch [24], Batch [54/157], test Loss: 0.3604\n",
            "Epoch [24], Batch [55/157], test Loss: 0.3198\n",
            "Epoch [24], Batch [56/157], test Loss: 0.4690\n",
            "Epoch [24], Batch [57/157], test Loss: 0.4597\n",
            "Epoch [24], Batch [58/157], test Loss: 0.2134\n",
            "Epoch [24], Batch [59/157], test Loss: 0.4027\n",
            "Epoch [24], Batch [60/157], test Loss: 0.5872\n",
            "Epoch [24], Batch [61/157], test Loss: 0.4595\n",
            "Epoch [24], Batch [62/157], test Loss: 0.4858\n",
            "Epoch [24], Batch [63/157], test Loss: 0.5658\n",
            "Epoch [24], Batch [64/157], test Loss: 0.4008\n",
            "Epoch [24], Batch [65/157], test Loss: 0.4009\n",
            "Epoch [24], Batch [66/157], test Loss: 0.4920\n",
            "Epoch [24], Batch [67/157], test Loss: 0.3531\n",
            "Epoch [24], Batch [68/157], test Loss: 0.3852\n",
            "Epoch [24], Batch [69/157], test Loss: 0.6667\n",
            "Epoch [24], Batch [70/157], test Loss: 0.3115\n",
            "Epoch [24], Batch [71/157], test Loss: 0.4488\n",
            "Epoch [24], Batch [72/157], test Loss: 0.6503\n",
            "Epoch [24], Batch [73/157], test Loss: 0.3405\n",
            "Epoch [24], Batch [74/157], test Loss: 0.3374\n",
            "Epoch [24], Batch [75/157], test Loss: 0.3587\n",
            "Epoch [24], Batch [76/157], test Loss: 0.4844\n",
            "Epoch [24], Batch [77/157], test Loss: 0.4254\n",
            "Epoch [24], Batch [78/157], test Loss: 0.3804\n",
            "Epoch [24], Batch [79/157], test Loss: 0.3086\n",
            "Epoch [24], Batch [80/157], test Loss: 0.2259\n",
            "Epoch [24], Batch [81/157], test Loss: 0.3947\n",
            "Epoch [24], Batch [82/157], test Loss: 0.4393\n",
            "Epoch [24], Batch [83/157], test Loss: 0.4581\n",
            "Epoch [24], Batch [84/157], test Loss: 0.3133\n",
            "Epoch [24], Batch [85/157], test Loss: 0.3290\n",
            "Epoch [24], Batch [86/157], test Loss: 0.6452\n",
            "Epoch [24], Batch [87/157], test Loss: 0.2995\n",
            "Epoch [24], Batch [88/157], test Loss: 0.3408\n",
            "Epoch [24], Batch [89/157], test Loss: 0.3457\n",
            "Epoch [24], Batch [90/157], test Loss: 0.3418\n",
            "Epoch [24], Batch [91/157], test Loss: 0.3778\n",
            "Epoch [24], Batch [92/157], test Loss: 0.3074\n",
            "Epoch [24], Batch [93/157], test Loss: 0.4289\n",
            "Epoch [24], Batch [94/157], test Loss: 0.3289\n",
            "Epoch [24], Batch [95/157], test Loss: 0.5079\n",
            "Epoch [24], Batch [96/157], test Loss: 0.4234\n",
            "Epoch [24], Batch [97/157], test Loss: 0.5487\n",
            "Epoch [24], Batch [98/157], test Loss: 0.4718\n",
            "Epoch [24], Batch [99/157], test Loss: 0.2763\n",
            "Epoch [24], Batch [100/157], test Loss: 0.4643\n",
            "Epoch [24], Batch [101/157], test Loss: 0.5622\n",
            "Epoch [24], Batch [102/157], test Loss: 0.2938\n",
            "Epoch [24], Batch [103/157], test Loss: 0.5913\n",
            "Epoch [24], Batch [104/157], test Loss: 0.3279\n",
            "Epoch [24], Batch [105/157], test Loss: 0.4676\n",
            "Epoch [24], Batch [106/157], test Loss: 0.3889\n",
            "Epoch [24], Batch [107/157], test Loss: 0.6168\n",
            "Epoch [24], Batch [108/157], test Loss: 0.3914\n",
            "Epoch [24], Batch [109/157], test Loss: 0.5290\n",
            "Epoch [24], Batch [110/157], test Loss: 0.5001\n",
            "Epoch [24], Batch [111/157], test Loss: 0.4297\n",
            "Epoch [24], Batch [112/157], test Loss: 0.3051\n",
            "Epoch [24], Batch [113/157], test Loss: 0.4825\n",
            "Epoch [24], Batch [114/157], test Loss: 0.2721\n",
            "Epoch [24], Batch [115/157], test Loss: 0.2081\n",
            "Epoch [24], Batch [116/157], test Loss: 0.3898\n",
            "Epoch [24], Batch [117/157], test Loss: 0.4298\n",
            "Epoch [24], Batch [118/157], test Loss: 0.3765\n",
            "Epoch [24], Batch [119/157], test Loss: 0.3900\n",
            "Epoch [24], Batch [120/157], test Loss: 0.5996\n",
            "Epoch [24], Batch [121/157], test Loss: 0.6494\n",
            "Epoch [24], Batch [122/157], test Loss: 0.5265\n",
            "Epoch [24], Batch [123/157], test Loss: 0.4280\n",
            "Epoch [24], Batch [124/157], test Loss: 0.3613\n",
            "Epoch [24], Batch [125/157], test Loss: 0.4057\n",
            "Epoch [24], Batch [126/157], test Loss: 0.3454\n",
            "Epoch [24], Batch [127/157], test Loss: 0.2570\n",
            "Epoch [24], Batch [128/157], test Loss: 0.3341\n",
            "Epoch [24], Batch [129/157], test Loss: 0.5562\n",
            "Epoch [24], Batch [130/157], test Loss: 0.3821\n",
            "Epoch [24], Batch [131/157], test Loss: 0.3515\n",
            "Epoch [24], Batch [132/157], test Loss: 0.5252\n",
            "Epoch [24], Batch [133/157], test Loss: 0.3089\n",
            "Epoch [24], Batch [134/157], test Loss: 0.4137\n",
            "Epoch [24], Batch [135/157], test Loss: 0.3901\n",
            "Epoch [24], Batch [136/157], test Loss: 0.4994\n",
            "Epoch [24], Batch [137/157], test Loss: 0.4293\n",
            "Epoch [24], Batch [138/157], test Loss: 0.4356\n",
            "Epoch [24], Batch [139/157], test Loss: 0.2911\n",
            "Epoch [24], Batch [140/157], test Loss: 0.3380\n",
            "Epoch [24], Batch [141/157], test Loss: 0.2958\n",
            "Epoch [24], Batch [142/157], test Loss: 0.4542\n",
            "Epoch [24], Batch [143/157], test Loss: 0.7942\n",
            "Epoch [24], Batch [144/157], test Loss: 0.4642\n",
            "Epoch [24], Batch [145/157], test Loss: 0.3902\n",
            "Epoch [24], Batch [146/157], test Loss: 0.6135\n",
            "Epoch [24], Batch [147/157], test Loss: 0.3110\n",
            "Epoch [24], Batch [148/157], test Loss: 0.4863\n",
            "Epoch [24], Batch [149/157], test Loss: 0.3981\n",
            "Epoch [24], Batch [150/157], test Loss: 0.3840\n",
            "Epoch [24], Batch [151/157], test Loss: 0.4291\n",
            "Epoch [24], Batch [152/157], test Loss: 0.3696\n",
            "Epoch [24], Batch [153/157], test Loss: 0.4169\n",
            "Epoch [24], Batch [154/157], test Loss: 0.3185\n",
            "Epoch [24], Batch [155/157], test Loss: 0.5863\n",
            "Epoch [24], Batch [156/157], test Loss: 0.4148\n",
            "Epoch [24], Batch [157/157], test Loss: 0.7815\n",
            "Accuracy of test set: 85.18%\n",
            "Epoch [25/25] - Train Loss: 0.3739, Train Accuracy: 86.71% - Test Loss: 0.4180, Test Accuracy: 85.18%\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAFgCAYAAACmKdhBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAB6KUlEQVR4nO3deXxU1f3/8ddnJvs2IQuQBQibiiCgBnBBRa271rWudavW2m9bq622drOtXX7W7mqr1arUvXW3LnVX3AUUkUWUJUASlixk35Pz++NOQhKSECDJTJL38/GYx525987MZ65jDp8553yOOecQERERERGRPecLdQAiIiIiIiJDhRIsERERERGRPqIES0REREREpI8owRIREREREekjSrBERERERET6iBIsERERERGRPqIES2QIMLM7zOxnoY5DRERkT5nZYWa2KtRxiOwuJVgivWRmeWb2pRC873wz+3WnfTlm5swsAsA5d6Vz7le9eK2QfAYREekdM3vDzLaZWXSoY+kPZjbPzPK72P+GmV0O4Jx7yzm3dy9e6xdm9kB/xCmyJ5RgiUivtCZzIiLSP8wsBzgMcMCXB/i9h93f+OH4mWVgKMES2UNmFm1mfzGzwuDtL62/PJpZmpk9a2ZlZlZqZm+ZmS947IdmVmBmlWa2ysyO3oMY2nq5untPM7sfGAv818yqzOwHwfO/bGbLg+e/YWZT2r1uXjDOpUC1mV1nZo93eu9bzewvuxu7iIi0uQh4H5gPXNz+gJmNMbMnzKzIzErM7LZ2x75uZiuD7ckKMzsguN+Z2aR257VvK+aZWX7wb/xm4F4zGxFsP4qCvWjPmll2u+enmNm9wbZum5k9Fdy/zMxOaXdepJkVm9nM3bkInXu5umovzex44MfAOcE27ZPguZlm9kyw/VttZl9v9zq/MLPHzOwBM6sArjezGjNLbXfOgcHPH7k7sYsAKHMX2XM/AQ4CZuL96vg08FPgZ8D3gXwgPXjuQYAzs72BbwOznHOFwV8t/X0UT5fv6Zy70MwOAy53zr0CYGZ7AQ8DpwFvANfgJWD7Oucags8/DzgJKAaSgV+YWbJzriz46985wAl9FLuIyHB2EfAn4APgfTMb5ZzbYmZ+4FngNeBCoBnIBTCzrwC/wPs7vgiYCDT28v1GAynAOLwf3eOAe4Gz8dqke4Dbgq8NcD9QBUwNbg8J7r8P+Crw3+DjE4FNzrklvf/oXeuuvXTOrTGz3wKTnHNfbfeUh4HlQCawD/Cyma11zr0aPH4q8BW8ax0d/AxnA7cHj38VeMQ519trKLID9WCJ7LkLgBudc1udc0XAL/EaQPAauQxgnHOuMTiu3OE1jtHAvmYW6ZzLc86t6eE9rg32MJWZWRmwtIdzu3vPrpwDPOeceznYmPwBiGV7owlwi3Nuo3Ou1jm3CViA1zgBHA8UO+cW9xCPiIjshJnNxUt0/hP8m7oGOD94eDZewnCdc67aOVfnnHs7eOxy4Gbn3ELnWe2cW9/Lt20Bfu6cqw/+jS9xzj3unKtxzlUCvwGOCMaXgfdj2pXOuW3B9uXN4Os8AJxoZknBxxfiJWPdyWzfpgXbtbndnNvr9tLMxgRf54fBa7QE+Cfb22SA95xzTznnWpxztcC/8JIqgonseTuJXWSnlGCJ7LlMoH1jtj64D+D3wGrgJTNba2bXAzjnVgNX4/3quNXMHjGzTLr3B+dccusNmN7DuV2+Z29id861ABuBrHbnbOz0nLbGKLhVQyQisucuBl5yzhUHHz/E9mGCY4D1zrmmLp43Bi8Z2x1Fzrm61gdmFmdm/zCz9cEhdAuA5GDiMQYodc5t6/wizrlC4B3gTDNLxkvEHuzhfQvbt2nBdu3trk7cxfYyMxhjZbt96+m5TXsaL3mbABwDlDvnPuwhdpGdUoIlsucK8X51bDU2uA/nXKVz7vvOuQnAKcD3WudaOececs61/mLpgN/1RTA9vWfwfbqN3cwMrxEtaP+SnZ7zFDDdzKYBJ9NzIyoiIjthZrF4w9SOMLPNwTlR1wAzzGwGXlIw1rouyrARb1hgV2rwhv21Gt3peOe/798H9gbmOOeSgMNbQwy+T0owgepK649vX8HrJSro5rxd1kN72VWblmJmie32jaWHNi2YYP4HbzTKznreRHpFCZbIrok0s5h2twi88d4/NbN0M0sDbsAbLoGZnWxmk4KJSwXeUIdmM9vbzI4yrxhGHVAbPLbHunvP4OEtwIR2p/8HOCk4YTgSr3GtB97t7vWDjdFjeL+ufuic29AXcYuIDGOn4f2d3hdvPu9MYArwFt5coQ+BTcBNZhYfbH8ODT73n3jDyA80zyQza/3hbAlwvpn5g0UhjthJHIl47VGZmaUAP289EBwi/gLwd/OKYUSa2eHtnvsUcADwXbw5WX1iJ+3lFiDHgsWjnHMb8dqv/xe8RtOBy9j5D4H3AZfgVW5U2XfZY0qwRHbN83h/3FtvvwB+jTexeCnwKfBRcB/AZOAVvMnA7wF/d869gTee/Ca8whGbgZF41ZD6QnfvCfD/8JLBMjO71jm3Cu8Xx1uDsZwCnNKuwEV3/gXsh37pExHpCxcD9zrnNjjnNrfe8ApMXIDXg3QKMAnYgFfI6BwA59yjeHOlHgIq8RKdlODrfjf4vLLg6zy1kzj+gjcPtxivmuH/Oh2/EG+e72fAVryhewTjqAUeB8YDT/T6k+9cT+3lo8FtiZl9FLx/HpCD15v1JN4cs5d7egPn3Dt489E+cs7l9WHsMkxZ93PfRUS6ZmZj8RrY0c65ilDHIyIioWdmNwB7darqNyiY2WvAQ865f4Y6Fhn8VKZdRHZJcCjG9/DK2Cq5EhERgkMKL6Njxb5Bwcxm4Q1vPDXUscjQoCGCItJrZhaPN6/rGNqNzRcRkeEruJjvRuAF59yCUMezK8zsX3jD6q/uVH1QZLdpiKCIiIiIiEgfUQ+WiIiIiIhIHxlSc7DS0tJcTk5OqMMQEZE+tHjx4mLnXHqo49hdaptERIam7tqnIZVg5eTksGjRolCHISIifcjM1oc6hj2htklEZGjqrn3SEEEREREREZE+ogRLRERERESkjyjBEhERERER6SNDag6WiMhAamxsJD8/n7q6ulCHMiTExMSQnZ1NZGRkqEPpd/ruDC7D6bspIntOCZaIyG7Kz88nMTGRnJwczCzU4QxqzjlKSkrIz89n/PjxoQ6n3+m7M3gMt++miOw5DREUEdlNdXV1pKam6h/IfcDMSE1NHTY9OvruDB7D7bspIntOCZaIyB7QP5D7znC7lsPt8w5m+m8lIrtCCZaIiIiIiEgfUYIlIjJIlZSUMHPmTGbOnMno0aPJyspqe9zQ0NDjcxctWsRVV121S++Xk5NDcXHxnoQsYWKgvzsAH3/8MWbGiy++uLthi4gMCipyISIySKWmprJkyRIAfvGLX5CQkMC1117bdrypqYmIiK7/zOfm5pKbmzsQYUoYCsV35+GHH2bu3Lk8/PDDHHfccbsVd280Nzfj9/v77fVFRHZGPVhBLS2OrRV1VNY1hjoUEZHddskll/C9732PI488kh/+8Id8+OGHHHLIIey///4ccsghrFq1CoA33niDk08+GfD+gf21r32NefPmMWHCBG655ZZev9/69es5+uijmT59OkcffTQbNmwA4NFHH2XatGnMmDGDww8/HIDly5cze/ZsZs6cyfTp0/niiy/6+NPLnujP745zjscee4z58+fz0ksvdSgYcfPNN7PffvsxY8YMrr/+egBWr17Nl770JWbMmMEBBxzAmjVrOrwvwLe//W3mz58PeL2rN954I3PnzuXRRx/lrrvuYtasWcyYMYMzzzyTmpoaALZs2cLpp5/OjBkzmDFjBu+++y4/+9nP+Otf/9r2uj/5yU926f8BERk8WloclXWNbC6vo66xud/eRz1YQQVltRx28+v87sz9OGfW2FCHIyKDzC//u5wVhRV9+pr7Zibx81Om7vLzPv/8c1555RX8fj8VFRUsWLCAiIgIXnnlFX784x/z+OOP7/Cczz77jNdff53Kykr23ntvvvnNb/ZqzZ9vf/vbXHTRRVx88cXcc889XHXVVTz11FPceOONvPjii2RlZVFWVgbAHXfcwXe/+10uuOACGhoaaG7uv8ZtMBkO35133nmH8ePHM3HiRObNm8fzzz/PGWecwQsvvMBTTz3FBx98QFxcHKWlpQBccMEFXH/99Zx++unU1dXR0tLCxo0be4w9JiaGt99+G/CGQH79618H4Kc//Sl333033/nOd7jqqqs44ogjePLJJ2lubqaqqorMzEzOOOMMvvvd79LS0sIjjzzChx9+uMvXTkT6X31TM8VVDRRV1lNUWU9pdT1V9c1U1TVR3dBEVX0T1cGbd7+Zqvrt+2satrc79182m8Mmp/dLnEqwgkYHYjCDTeUqwyoig9tXvvKVtiFS5eXlXHzxxXzxxReYGY2NXffSn3TSSURHRxMdHc3IkSPZsmUL2dnZO32v9957jyeeeAKACy+8kB/84AcAHHrooVxyySWcffbZnHHGGQAcfPDB/OY3vyE/P58zzjiDyZMn98XHlT7UX9+dhx9+mHPPPReAc889l/vvv58zzjiDV155hUsvvZS4uDgAUlJSqKyspKCggNNPPx3wEqfeOOecc9ruL1u2jJ/+9KeUlZVRVVXVNiTxtdde47777gPA7/cTCAQIBAKkpqby8ccfs2XLFvbff39SU1N7e8lEZA81tzjKahooqqpvS5zablUd75fVdD/SLCbSR0J0BPHREcRHRZAQHUFaQhQ5afEkRPuJj/KOtZ4zMT2h3z6TEqygSL+P9IRoNpUpwRKRXbc7vQX9JT4+vu3+z372M4488kiefPJJ8vLymDdvXpfPiY6Obrvv9/tpamrarfduLWd9xx138MEHH/Dcc88xc+ZMlixZwvnnn8+cOXN47rnnOO644/jnP//JUUcdtVvvM5QM9e9Oc3Mzjz/+OM888wy/+c1v2hburaysxDm3Qwl051yX7xMREUFLS0vb487rUrWP/ZJLLuGpp55ixowZzJ8/nzfeeKPHz3355Zczf/58Nm/ezNe+9rUezxWRjpxzlNc2UlRZz7aaRirrGqmsa6KyrpGKuiYq2h43tR2rqN1+TnVD16MZYiP9jEyKJj0hmkkjEzh4YirpCdGkJ26/pcRHkRgdSXy0nwh/+Mx8UoLVTkYghsLy2lCHISLSZ8rLy8nKygJom6/Slw455BAeeeQRLrzwQh588EHmzp0LwJo1a5gzZw5z5szhv//9Lxs3bqS8vJwJEyZw1VVXsXbtWpYuXaoEK4z11XfnlVdeYcaMGR2qB1588cU89dRTHHvssdx4442cf/75bUMEU1JSyM7O5qmnnuK0006jvr6e5uZmxo0bx4oVK6ivr6euro5XX3217fvWWWVlJRkZGTQ2NvLggw+2fY6jjz6a22+/nauvvprm5maqq6tJSkri9NNP54YbbqCxsZGHHnpotz+ryFDS1NxCSbU3HG9rZR1bK+rZ2u5+UVV927ahqaXb14ny+0iMiSAxJoKk2EgSYyJIT0gI7oskKTaC5NhI0hNj2hKq9MRo4qMHb5oyeCPvBxmBWFYXVYU6DBGRPvODH/yAiy++mD/96U99ksxMnz4dn8/7lfDss8/mlltu4Wtf+xq///3vSU9P59577wXguuuu44svvsA5x9FHH82MGTO46aabeOCBB4iMjGT06NHccMMNexyP9J+++u48/PDDbcP9Wp155pncfvvtvPDCCyxZsoTc3FyioqI48cQT+e1vf8v999/PN77xDW644QYiIyN59NFHmTBhAmeffTbTp09n8uTJ7L///t2+569+9SvmzJnDuHHj2G+//aisrATgr3/9K1dccQV33303fr+f22+/nYMPPpioqCiOPPJIkpOTVYFQhoX6pma2lNdTWF7L5vI6Cstr2VRWx6byWjaV17Glwpvf1NJFh/KIuEjSE6MZmRjDnPHxpAeTopFJMYyIi/SSpmDylBgTQUzk8Pt/yrrrih+McnNz3aJFi3b7+b/873L+s3Ajy355nFZtF5GdWrlyJVOmTAl1GENKV9fUzBY75wZtTfmu2iZ9d8JLS0sLBxxwAI8++mi3cwP130wGg7rGZirqGqmobaKspoHC8jo2lXlJU2vyVFhWR3FV/Q7PTY6LJCMQS0YghlFJ0V6PUnAo3shEL4FKS4giOmL4JUzd6a59Ug9WO5mBWKobmqmsbyIpZufVs0RERGRwW7FiBSeffDKnn366Cq9I2HDOUVrdwPrSGraU11Fe20hFXaO3rW1qd9+b59R6v76boXqJ0RFkJMeQEYhlamYSo5NiyUiOITMQG9wfQ1zUMEgLWlqgshBKVkPGDIgd0S9vMwyuZO+NDniVijaV1ZE0WgmWiIjIULfvvvuydu3aUIchw1BLi2NTRR3rS6rZUFJDXkkNG0qrWV9Sw/qSGqrqdyw25PcZSTERBGIjSYqNJCnG63VKio1oe5wUG0kgeMsIeMlT4nDrOKirgJIvoHi1l0y13i9dA43eunhc8BhMPqZf3l4JVjuZyV6CVVhey96jE0McjYiIiIgMVs0tjpJqrxDE5vI68rfVsL60JphAVbNxW22H4hCRfmPMiDjGpsYxKyeFsSlxjEuNIzM5ti2hio/yD/5pLE31ULQKtiyDzcu8be02iE6EqASITvC27e9HJ0BUYrtj8d75znlJU8lqKP4imEythqot29/PfJA8DlInwfjDvG3qJMic2W8fUQlWOxmBWACVahcRERGRLjnnqKhtYktlHZvL69hSUcfWyvq2+1sq69ka3NfcqUpEXJSfcanxTB6ZyJemjGJcajzjUuMYm+IlUn5fp+SpoQaqiyAxE/yD8J/tlVu8BKp9MlX8ObQEe+ciYmDkFEjMgIZqqNoMpdVQXwUNwVtvxaV6idOkYyAtmESlToaU8RARvfPn96FB+F+q/4xMjMZnsFml2kVERESGlabmFkqrG9jabqHbrZV1wW1923ZrZR11jTvOdQrERjI6ySs1PnlkGqOTvGIRI5NiGJ0UQ2ZyLGkJUTvvgSrbAJ+/6N3WLYDmejA/JGXBiHFeb0znbcIo8IVwHaiGGq/naOtK2PLp9mSqumj7OUlZMGoa7HU8jJ4Go/aD1Ing66FoRksLNLZLuOorvUSsocrb55ohZYKXTMWl9P/n7CUlWO1E+H2MTIyhsFw9WCIiIiJDRU1DEwXbaskvq6VgWy2FZbUdkqaiyu7LkifFRDAyKYb0hGhmjklmVFI0o5Ji2m6tSdVulyNvaYb8hfD5/7ykausKb/+I8TDrMkjfG8rzYdt6KFsPq1/xenra80dD8thOiddYL/GKT/duMQHYk+GFLc1QvrHTvKYvoGQNVOR3jGXkPjD5uGAiNQ1GTd29BMjn84YCRg+uqTtKsDrJSI5hk3qwRGQQKCkp4eijjwZg8+bN+P1+0tPTAfjwww+Jiorq8flvvPEGUVFRHHLIITscmz9/PosWLeK2227r+8Al5Przu9Pq1FNPZevWrbz33nt9F7hIN8prG70EalsNBcEkqqCslvzgtrS6ocP5ET5rKz+elRzDzDHJpLcrSd66TUtolzg11kLBYvDVQ0IMJAS8uUC7o3YbrH7VS6hWv+w99kXA2IPh2N/AXsd5vTLdJUSNtVC20Uu4tuUFt8EELH8R1JXt+Bx/VDDZSoP4kZAwcvv9+HRISPfuxyZDRWG7OU2tBSLWer1praID3lC8nLlerGmTIH2Kd38wDmfsQ8P703chMxDLyk0VoQ5DRGSnUlNTWbJkCQC/+MUvSEhI4Nprr+3189944w0SEhJ6/EeyDE39/d0pKyvjo48+IiEhgXXr1jF+/Pi+CHsHTU1NRETonzJDnXOObTWNXvK0zUuaWhOp/G1eMlXZqeJeTKSPrORYskfEsV92IHjfu2Ulx3nTQjrPd9rxjb0kY/UrsOZVyHsbmjqNcopK8HqJEkZ5CUuHbfB+4miIS/OKMbT2Um143xveFpfqDZnb6ziYeJTXy9QbkbGQvpd360pduZeAVW+F6mKo2uoN12u9VW31hvNVb4Xmhq5fA7ykb8R4SJvsVdxLneTdT53sJWeDveBGP9FfpU4yAjG8+tkWnHODv0qLiAw7ixcv5nvf+x5VVVWkpaUxf/58MjIyuOWWW7jjjjuIiIhg33335aabbuKOO+7A7/fzwAMPcOutt3LYYYft9PX/9Kc/cc899wBw+eWXc/XVV1NdXc3ZZ59Nfn4+zc3N/OxnP+Occ87h+uuv55lnniEiIoJjjz2WP/zhD/398WUP9OV35/HHH+eUU05h1KhRPPLII/zoRz8CYPXq1Vx55ZUUFRXh9/t59NFHmThxIjfffDP3338/Pp+PE044gZtuuol58+bxhz/8gdzcXIqLi8nNzSUvL4/58+fz3HPPUVdXR3V1Nc888wynnnoq27Zto7GxkV//+teceuqpANx333384Q9/wMyYPn06f//735k+fTqff/45kZGRVFRUMH36dL744gsiI4dZGesw4pyjqKq+LVnyep1qOjyubWzu8JzE6AiyggnTnPEpZI+II2tEbFsilRLfi7lOXamrgHVver1Lq1+F8g3e/rS94MBLYeKRXtJRtdUbple11atYV7XVG9q35nWoL+/5PUbtB3Ov8ZKqrAN7noO0u2ICMLoXyZpzXjJWXRxMxoqgptQrOpE22RtuOMx7o3aHrlgnGcmx1DW2UF7bSHJcz0MkRETavHA9bP60b19z9H5wwk29Pt05x3e+8x2efvpp0tPT+fe//81PfvIT7rnnHm666SbWrVtHdHQ0ZWVlJCcnc+WVV+5Sz8XixYu59957+eCDD3DOMWfOHI444gjWrl1LZmYmzz33HADl5eWUlpby5JNP8tlnn2FmlJWV7c4VGB6G4Hfn4Ycf5uc//zmjRo3irLPOakuwLrjgAq6//npOP/106urqaGlp4YUXXuCpp57igw8+IC4ujtLS0p3G+95777F06VJSUlJoamriySefJCkpieLiYg466CC+/OUvs2LFCn7zm9/wzjvvkJaWRmlpKYmJicybN4/nnnuO0047jUceeYQzzzxTydUAaWpuYUNpDau3VvHF1irWbK1idVEVq7dWUdPQMYFKjoske0QsE9LjOXyv9LbEyUuq4gjE9tF/s5YW2Lw02Ev1Gmz8wKtwF5UIE46Aw66BiUd7c5p6q7E2mHi1Jl/BRCxhlJdUBbL7Jva+YOYNCYxN9ob4SZ9QgtVJRnCx4cKyOiVYIjKo1NfXs2zZMo45xls4sbm5mYyMDACmT5/OBRdcwGmnncZpp522W6//9ttvc/rppxMf7805OOOMM3jrrbc4/vjjufbaa/nhD3/IySefzGGHHUZTUxMxMTFcfvnlnHTSSZx88sl98hmlf/Tld2fLli2sXr2auXPnYmZERESwbNkyxo0bR0FBAaeffjoAMTFee/vKK69w6aWXEhcXB0BKys4nwh9zzDFt5znn+PGPf8yCBQvw+XwUFBSwZcsWXnvtNc466yzS0tI6vO7ll1/OzTffzGmnnca9997LXXfdtQtXSnqjrrGZdcXVHROprVWsK66moXl79b3RSTFMHpXA2bljGJ8Wz5gUb/he1ohYEqL78Z+ojbWw8llv7tOa17ZXusuYAYdcBZO+BGNmg383k7jIWC8h25WkTIYUJVidtCZYm8pr2TczKcTRiMigsQu9Bf3FOcfUqVO7LCrw3HPPsWDBAp555hl+9atfsXz58t16/a7stddeLF68mOeff54f/ehHHHvssdxwww18+OGHvPrqqzzyyCPcdtttvPbaa7v8nsPCEPvu/Pvf/2bbtm1t864qKip45JFH+MEPftDte3c1lCsiIoKWFu8f43V1Hee9tCb5AA8++CBFRUUsXryYyMhIcnJyqKur6/Z1Dz30UPLy8njzzTdpbm5m2rRpPX4e2VFjcwuby+vYVF5HYZlXRGJTuTecb11xNRtKa9qq8ZnB2JQ4Jo9MYN4+6UxKT2DyqEQmpseTGDPAPYe122Dh3fDBHV5SFZfq9U5NOtqb/5QwcmDjkSFLCVYnmcneYsMq1S4ig010dDRFRUW89957HHzwwTQ2NvL5558zZcoUNm7cyJFHHsncuXN56KGHqKqqIjExkYqK3hf1Ofzww7nkkku4/vrrcc7x5JNPcv/991NYWEhKSgpf/epXSUhIYP78+VRVVVFTU8OJJ57IQQcdxKRJGnoSzvryu/Pwww/zv//9j4MPPhiAdevWccwxx/DrX/+a7OxsnnrqKU477TTq6+tpbm7m2GOP5cYbb+T8889vGyKYkpJCTk4OixcvZvbs2Tz22GPdxl5eXs7IkSOJjIzk9ddfZ/369QAcffTRnH766VxzzTWkpqa2vS7ARRddxHnnncfPfvazPr6SQ0NZTQP5wVLmhWW1FJbXeUlUWS2FZXVsqayj8+8tyXGRZARi2TcziS/PzGLSyAQmpScwIT1+98uX95WKQnjvb7B4vrd+0qRj4NCrYNzc0K4dJUOWEqxO0hKiifAZm8pUql1EBhefz8djjz3GVVddRXl5OU1NTVx99dXstddefPWrX6W8vBznHNdccw3JycmccsopnHXWWTz99NNdFiqYP38+Tz31VNvj999/n0suuYTZs2cD3lCr/fffnxdffJHrrrsOn89HZGQkt99+O5WVlZx66qltPQl//vOfB/JSyC7qq+9OXl4eGzZs4KCDDmp77fHjx5OUlMQHH3zA/fffzze+8Q1uuOEGIiMjefTRRzn++ONZsmQJubm5REVFceKJJ/Lb3/6Wa6+9lrPPPpv777+fo446qtvYL7jgAk455RRyc3OZOXMm++yzDwBTp07lJz/5CUcccQR+v5/999+f+fPntz3npz/9Keedd17/XdRBoKm5hXXF1azYVMHKTZWs3FTByk0VbK2s73BeVIRXkS8jEMPcyWlkJseSGfAWzvVuMcRFheE/KYtWwTu3wNJ/g2uBaWfAod/15iiK9CPrbsjHHr+w2RjgPmA00ALc6Zz7a6dzDPgrcCJQA1zinPsoeOz44DE/8E/n3E7HUOTm5rpFixbtceyH3vQac8an8KdzZu7xa4nI0LVy5UqmTJkS6jCGlK6uqZktds7lhiikPdZV26TvTmg99thjPP3009x///29fs5g/29WXtPIys0VbUnUyk2VfL6lkvombxhmpN+YNDKRKRmJTBmdxJiUOC+pSo4hdXcr8oXKxoXwzl/gs2chIhYOuBAO/haMyAl1ZDLEdNc+9efPDU3A951zH5lZIrDYzF52zq1od84JwOTgbQ5wOzDHzPzA34BjgHxgoZk90+m5/SYjEEOhFhsWEREZcr7zne/wwgsv8Pzzz4c6lH7hnCN/Wy3LCsqDPVNeMlXQbmROanwUUzKSuOjgcUzJSGJKRhIT0xOIihjEw+Wcgy9e9hKr9e9ATDIc/gOY8w1vvSaRAdRvCZZzbhOwKXi/0sxWAllA+yTpVOA+53WjvW9myWaWAeQAq51zawHM7JHguQOTYCXHsjS/bCDeSkRERAbQrbfeGuoQ+kxTcwtri6tZXljOsoIKlheWs6Kwgoo6b9Fdv8+YkBbPgeNG8NWDxjElI5F9M5JIT4wOrx6pdW9B/ocQnRS8JXq3mNb7AW8b0UV15+ZGWPYEvPNX2LockrLguP8HB1wE0QkD/1lEGKA5WGaWA+wPfNDpUBawsd3j/OC+rvbP6ea1rwCuABg7dmyfxJsZiOHF5d1XIBIRaaW/E32nv4ashyt9dwaPcPhu1jU2s2pzJcsLvURqWWEFn22qaBviFx3hY5+MJE6ZkcnUzABTM5PYe3Ri7wtMOAclq73Fdbcsg1mXQeb+/fiJgu/5zl/hlV8AvbjGETHBhCtpewJWug7KN0L6PnDa7TDtrK4TMZEB1O8JlpklAI8DVzvnOpcc6qplcT3s33Gnc3cCd4I3zn0PQm2TEYihoamF0uoGUhOi++IlRWQIiomJoaSkhNTUVP1DeQ855ygpKWlbG2mo03dn8AjVd7O8tpH31hSz4ItiPlq/jS+2VtEcrH2eGBPB1MwkvnrQOKZlJTE1M8CEtHgi/Ls4xK+uHNa+CWtehdWvQfkGb78/2isMceIf4MCL+/iTBTXVw3+vhk8egqmnw8l/9nqj6iu9uOorob4i+Di4rS/v9LgS0ibDib+HycepIqCEjX5NsMwsEi+5etA590QXp+QDY9o9zgYKgahu9g+I0QGvVPum8jolWCLSrezsbPLz8ykqKgp1KENCTEwM2dnZoQ5jQOi7M7gMxHezqbmFT/LLWPB5MW99UcSSjWW0OEiIjiA3ZwRfmjKKqZlJTMsKkD0idvcS85ZmKPzY66Va8yrkLwLXDFGJMOEImPtdb12o6CR4/DL471Xe0L0T/+AtnttXqorg31+Fje/DvB/BET/0FswCrUUlQ0K/JVjBCoF3Ayudc3/q5rRngG8H51jNAcqdc5vMrAiYbGbjgQLgXOD8/oq1s8xk71eqwrJapmUFBuptRWSQiYyMbFtMVWRX6LsjAOtLqlnwRTFvf1HEu6tLqKxvwmcwPTuZbx85icP2SmfmmGQid7Vnqr3yAljzmpdQrX3DW2wXg8yZMPcab5Hd7Fng77To71cfhzf+Hyz4PWxaCmffByl98J3dshweOtdb6Pcr873eK5Ehpj97sA4FLgQ+NbMlwX0/BsYCOOfuAJ7HK9G+Gq9M+6XBY01m9m3gRbwy7fc453peOr4PZbTrwRIRkeFjZ0uEmFkAeACvLYsA/uCcu3fAA5VBqaKukXdXl/DWF0W89UUxG0prAMhKjuXkGRkcNjmdQyamkhwXnEPU3Agb3oHiz6G5wbs1BbfN9d7x5gZvuF1z4/Z9TfXe/srNULzKe62E0bD3iTDxKJhwJMSn9hyszw9H/RSyDoQnvgF3HgFn/BP2Onb3L8CqF+Dxy735U5c+D1kH7P5riYSx/qwi+DZdz6Vqf44DvtXNsefxErABlxofRZTfp1LtIiLDSC+XCPkWsMI5d4qZpQOrzOxB51xDCEKWQaCyrpHnP93EEx8VsGj9NppbHPFRfg6emMblh41n7qQ0xqfFbx/yV7kZPnoZvnjJ63Gq7zx9PcgfDRHRXs+Tv3Ub1XHfiBzY/6teL9XIfbcPw9sVe58A33gD/n0RPPQVr/T5vOu9BKy3nIN3b4GXf+71nJ37ECRl7nosIoNEGC67HXo+nzEqEM1m9WCJiAwns9n5EiEOSAwOg08ASvHWfRRp09zieHt1MY8vzufF5Zupb2phQlo83zxiIofvlc7+Y9sN+2tphvyFXkL1xUuw6RNvf1IWTDsDJh8LmQd4iVNEtJdE+SJ2L1naXSkT4PKX4dnvwYKboWARnHk3xKXs/LlN9fDsNbDkQW844Kl/h6i4/o9ZJISUYHUjIxDLpjIlWCIiw0hvlgi5DW/+cCGQCJzjnGsZmPAk3H2+pZLHF+fz5McFbK2sJxAbyVdysznzgGxmjkne3ktVXeLNifriJVj9ijcvyvwwZg4c/XMvqRo1dWCTqJ2JjIXT/g5jZsMLP4B/HO7Ny+ppmF9VEfznQtjw3o7FLESGMCVY3cgMxLBo/bZQhyEiIgOnN0uEHAcsAY4CJgIvm9lbnZch6Y81GiU8lVTV88wnhTzxUQGfFpTj9xlH7p3OmQdkc9SUkURHBIfSbVkOnz3nJVX5iwAHcWmw1/FeQjXxSIgdEdLPslNmkHspZEyH/1wM9xwHJ9wMB16yY+LUVsxiK5x1r9cbJzJMKMHqRkZyLFs+3URLi8Pn068tIiLDQHdLh7R3KXBTcA7xajNbB+wDfNj+pP5Yo1HCR0NTC699tpXHP8rn9c+20tTimJqZxM9O3pdTZ2aS1rrES3MjLHsKPrzL68XBvB6fedfD5GMgY//BuXZT1oFwxZvwxOXw7NXeEMeT/ri9lHtrMYuohGAxiwNDGq7IQFOC1Y3MQAyNzY7iqnpGJg2PhS9FRIa5hex8iZANwNHAW2Y2CtgbWDugUUrIrCuu5l/v5vHUkgLKahpJT4zm0kNzOPPAbPYZnbT9xMrNsHg+LLoXqjZ7xSaO/TVMP2forPMUnwoXPAZv3OTNy9q8FM6+H1Y+4xWzyJgB5z2sYhYyLCnB6kb7xYaVYImIDH3dLRFiZlcGj98B/AqYb2af4g0p/KFzrjhkQcuAWLy+lDsXrOWlFVuI9Pk4duoozjwwm8MmpRHRWqzCOdj4AXx4J6x4GlqaYNIxMPsWmPSlXau6N1j4/HDUTyA7F574Ovxttlceft/T4LTbVcxChi0lWN3ICHhJ1abyWmaMSQ5tMCIiMiC6WiIkmFi13i8E9mAhIBksmlscL6/YzJ0L1vLRhjICsZF8a94kLjpkHCMT2/3w2lADyx7zEqvNn0J0AGZ/A2ZdBqkTQ/cBBtJex8E3FsB/r4Zxh8Jh3x+cQx9F+ogSrG5kJns9WIWqJCgiIjJs1DY089hH+dz91lrySmoYkxLLL07Zl7NnjSEuqt0/m0rXwaK74aP7oa4MRk6Fk/8C08+GqPhQhR86I3LgoqdCHYVIWFCC1Y0RcZFER/jYpMWGRUREhrziqnrue28997+Xx7aaRmZkB/jb+Qdw3NRR24cBNjfB2tdh4T/h8xfBfDDlFJh9BYw7RCXIRQRQgtUtMyMjEMMmLTYsIiIyZK0tquKfb6/j8cX51De18KUpI7ni8InMyhnhrVvV3ARr3oDlT8HK/0JtKcSPhMOv80qWq4iDiHSiBKsHGYFYJVgiIiJDjHOOxeu38Y8Fa3ll5RYi/T7OPCCLy+ZOYNLIhO09Vcufgs+ehZoSr+T4XsfD1NNg8nEQERXqjyEiYUoJVg8ykmN4f01JqMMQERGRPtDc4nhxuVe4YsnGMpLjIvn2kZO46OAc0uP8kLcA3n+q66Rq0pe2r/MkItIDJVg9yAzEsqWynuYWh1+LDYuIiAxK1fVNPLpoI3e/s46NpbWMTYnjl1+eylcOGE1cwXvw+t+UVIlIn1GC1YOM5BiaWxxbK+vICOgPrIiIyGCytaKO+e/m8eAHGyivbeTAcSP4yYlTOGZkJf4P/gy3/FdJlYj0OSVYPdi+FpYSLBERkcFi1eZK7nprLU8vKaC5xXHc1NFcftgEDkxthDdugsfnQ0Q07H2ikioR6XNKsNpzDlxL22rrrUnVprI6GBvKwERERKQnzjneXl3MXW+tY8HnRcRG+jl/9li+Nnc84xIcvPc3eOCv0FzvLQJ8+A8gIT3UYYvIEKQEq1XZBvjbHDjpjzDzfMCbgwVoLSwREZEw1dDUwn8/KeSut9by2eZK0hOjue64vblgzliSo33w8f3wxv+Dqi0w5cvwpV9A6sRQhy0iQ5gSrFbx6dBYA+UFbbuSYiOIi/JTWKZS7SIiIuHm6SUF/Pb5lWypqGevUQncfNZ0Tp2ZSbTfB5//D17+ORSvgjEHwTkPwJjZoQ5ZRIYBJVitImMhLhUq8tt2mRmjAzHqwRIREQkjzS2O37+4ijveXMPMMcncfNYMDp+c5i0MnL8YXv4ZrH8HUifBOQ/CPieBqRqwiAwMJVjtJWV16MECb5igFhsWEREJD5V1jVz9yBJe/Wwr588Zyy9OmUpUhA9K18KrN8LyJ71RKSf9EQ64GPyRoQ5ZRIYZJVjtBcbAtnUddmUEYljwRVGIAhIREZFWG0pquPy+hawpquZXp07lwoNzoLoEXr4ZFt7tJVNH/BAO+Q5EJ4Y6XBEZppRgtRfIgry3O+zKSI5la2U9jc0tRPp9IQpMRERkeHtvTQn/9+BiWhzc97XZHDo+AO/eCm/eDA1VcMBFMO9HkDg61KGKyDCnBKu9pCyoL4e6CohJArweLOdgS0Ud2SPiQhygiIjI8PPA++v5xTPLyUmL558X5ZJT9THc8X0o+gwmHwvH/ApG7hPqMEVEACVYHQWyvW1FQYcEC2BzuRIsERGRgdTY3MKN/13B/e+v58i907nllEwS37waPv0PBMbCuQ/DPieGOkwRkQ6UYLWXlOVtywtg5BQAMpO9tbAKVehCRERkwJTVNPB/D37Eu2tKuPKwsfwg5S18d/0/aKqDw6+Dud+DKP3wKSLhRwlWe4FggtWuVHtrD9amMpVqFxERGQhfbKnk8vsWsamsjnuPbuHI1d+EhZ/CxKPghN9D2qRQhygi0i0lWO0lZgDWoVR7YkwkidERKtUuIiIyAF7/bCvfefhjMiKqeHffF0h751FvhMnZ98GUL2s9KxEJe0qw2vNHetWHKjquhTU6EEOherBERET6jXOOu95ay+9eWMH3Ut7lm00P4ltTDYd+Fw7/AUQnhDpEEZFe6bcEy8zuAU4GtjrnpnVx/DrggnZxTAHSnXOlZpYHVALNQJNzLre/4txBIBvK8zvsykiOZXOFerBERET6Q11jMz9+8lPWfPwmryU9wLjqzyHnMG+x4PS9Qx2eiMgu6c8erPnAbcB9XR10zv0e+D2AmZ0CXOOcK213ypHOueJ+jK9rSVmwZVmHXZmBGFYUVgx4KCIiIkNdfVMz37n7VY7Mv50/Rr8OkaPg5Lth2pkaDigig1K/JVjOuQVmltPL088DHu6vWHZJIBs+/x841/aHPSMQS3FVPfVNzURH+EMcoIiIyNDgnOPmh1/ihsLvkBVZis35P5h3fdtSKSIig5Ev1AGYWRxwPPB4u90OeMnMFpvZFTt5/hVmtsjMFhUVFe15QElZXgnYmu2daa2VBLeU1+/564uIiAgA/3r+TS5d/W3So+rwXfYSHP9bJVciMuiFPMECTgHe6TQ88FDn3AHACcC3zOzw7p7snLvTOZfrnMtNT0/f82i6KtWe7CVYheUqdCEiItIXXn7nfb704WWk+OuJ/tp/IXvgpluLiPSncEiwzqXT8EDnXGFwuxV4Epg9YNEkZXvbdqXaMwLeYsObVapdRERkj3269GOmvnQ+AV89EV/7L5a5f6hDEhHpMyFNsMwsABwBPN1uX7yZJbbeB44FlnX9Cv2grQdre4KVqR4sERGRPlGwZhkjnziDeGvAXfQMUdlKrkRkaOnPMu0PA/OANDPLB34ORAI45+4InnY68JJzrrrdU0cBT5pXYCICeMg597/+inMH8SPBF9mhVHtcVASB2Eg2lakHS0REZHdV5n9G9AOn4KORynOfJHv8AaEOSUSkz/VnFcHzenHOfLxy7u33rQVm9E9UveDzQVLGDosNZwRi2KQeLBERkd3SsGUVTfecgLU0kf/l/zB9n1mhDklEpF+Ewxys8BMY02EOFrQmWOrBEhER2VWuaBV1dx1PU3MTHx91P9MPPCTUIYmI9BslWF1JyupQRRAgIzlWCZaIiMiuKlpF7Z0nUN/YzHMH3MWXjpgX6ohERPqVEqyuBLKgohBamtt2ZQZiKK1uoK6xuYcnioiISJutK6n/5wlUNTTzj/G3cPGXjwt1RCIi/U4JVleSsqClCaq2tu0aHSzVrl4sERGRXtiygsZ7TqK8roVfp/2eay84hWABKxGRIU0JVlcCwbWw2pdqD3il2jeVqdCFiIhIj7Ysp/nek9hW57g69tf8/NJTiYn0hzoqEZEBoQSrK0nBtbDalWrPSFYPloiIyE5t/pSW+SdTWm98jZ9z49dOIzUhOtRRiYgMGCVYXemiByujtQdLpdpFRIYsMzvezFaZ2Wozu76L49eZ2ZLgbZmZNZtZSihiDUubluL+dQrbGnycW/9TfnzhyUwamRDqqEREBpQSrK7EjoCI2A6l2mMi/aTER1GoHiwRkSHJzPzA34ATgH2B88xs3/bnOOd+75yb6ZybCfwIeNM5VzrgwYajtW/i/nUyZU1RnFbzE75xxjEcMjEt1FGJiAw4JVhdMfN6sTqVah+dFKM5WCIiQ9dsYLVzbq1zrgF4BDi1h/PPAx4ekMjC3aJ74IEz2OZP5ZSqH3PKEYdwdu6YUEclIhISSrC6E8jaYbHhzGQtNiwiMoRlARvbPc4P7tuBmcUBxwOPd3P8CjNbZGaLioqK+jzQsNHcBC9cD89eQ0XmXOaV/pjp0/bj2mP3DnVkIiIhowSrO0nZHYpcAGQEtNiwiMgQ1lUNcdfNuacA73Q3PNA5d6dzLtc5l5uent5nAYaVunJ4+Bz44HZa5nyTr9ZcQ3TCCG46czo+n8qxi8jwFRHqAMJWIAuqtkBTA0REAZCRHEN5bSM1DU3ERenSiYgMMflA+3Ft2UBhN+eey3AeHli6Fh46F0rXwMl/YX7dPJYWruDW8/YnKSYy1NGJiISUerC6k5QFOKjc1LartZJgYZl6sUREhqCFwGQzG29mUXhJ1DOdTzKzAHAE8PQAxxce8t6Bu472foS88Ek2TT6XP760isP3Sufk6Rmhjk5EJOSUYHUnEBx236FUe+taWCp0ISIy1DjnmoBvAy8CK4H/OOeWm9mVZnZlu1NPB15yzlWHIs6Q+vgBuO9UiEuFr78G4w/nl8+soKnF8etTp2GmoYEiIhrn1p2k4FpY7QpdZAa02LCIyFDmnHseeL7Tvjs6PZ4PzB+4qMJASzO88nN491aYcCR8ZT7EJvPqyi38b/lmrjtub8amxoU6ShGRsKAEqzttPVjbC12MCngr0W/SEEERERku6ivh8a/D5y/ArK/D8TeBP4KahiZueHo5k0cm8PXDJoQ6ShGRsKEEqzvRiRAd6NCDFR3hJy0hWkMERURkeCjb4BWzKPoMTvwDzP5626G/vvoFBWW1/OcbBxMVoRkHIiKtlGD1JJDdYQ4WeIUuCjVEUEREhroNH8C/L/Cq6X71MZh4VNuhzzZXcPdb6zg7N5vZ41NCGKSISPjRT049CWR1sRZWDJvK1IMlIiJD2CePwL9O9kZzXP5Kh+SqpcXx4yc+JSk2kh+dMCWEQYqIhCclWD1J2jHBykyOZbN6sEREZKhatwCe/AaMmQOXvwrpe3U4/MjCjXy0oYwfnziFEfFRIQpSRCR8KcHqSSALakuhoaZtV0Yghsr6JirrGkMYmIiISD9582ZIzIALHoO4jsP/iirruemFlRw0IYUzD8gKUYAiIuFNCVZPWku1VxS27RodXGxYpdpFRGTI2fgh5L0Fh3wHImN2OPyb51ZQ19jCr0/bT2teiYh0QwlWT7oo1Z6Z7K2FVah5WCIiMtS89SeITYEDLt7h0NtfFPPUkkKunDeRSSMTQhCciMjgoASrJ0nBBKtdqfYM9WCJiMhQtHmZt9bVQd+E6I4JVF1jMz97ehnj0+L5v3kTQxSgiMjgoDLtPWlNsNqVah+VFIOZEiwRERli3v4TRCV2WOuq1d/fWMO64moeuGwOMZH+EAQnIjJ4qAerJ5ExEJ/eoZJgpN9HekK0SrWLiMjQUbIGlj8Jsy6D2BEdDq0pquKON9Zw2sxM5k5OC1GAIiKDhxKsnUnK2nGx4eRY9WCJiMjQ8fafwR8FB3+rw27nHD958lNiIn385KR9QxSciMjgogRrZwLZHeZgAWQGYigsVw+WiIgMAeX53sLCB1wECSM7HHriowLeX1vK9SdMIT0xOkQBiogMLkqwdqaLxYYzAt5iw865EAUlIiLSR969FXBwyFUddm+rbuA3z6/kwHEjOHfWmNDEJiIyCPVbgmVm95jZVjNb1s3xeWZWbmZLgrcb2h073sxWmdlqM7u+v2LslUAWNFRCXXnbrszkGGoamqmobQphYCIiInuoqggW/wumnwvJHZOom174jIraRn5z+jR8Pq15JSLSW/3ZgzUfOH4n57zlnJsZvN0IYGZ+4G/ACcC+wHlmFrqB312Uam9dbFjDBEVEZFD74HZoqoO5V3fY/eG6Uv69aCOXHTaefUYnhSY2EZFBqt8SLOfcAqB0N546G1jtnFvrnGsAHgFO7dPgdkUg29tWtF8Ly1tseJMSLBERGaxqy+DDu2DfUyFtctvuhqYWfvLkp2Qlx/Ldoyd3/3wREelSqOdgHWxmn5jZC2Y2NbgvC9jY7pz84L4umdkVZrbIzBYVFRX1fYRtPVjb52FlJgd7sMpUSVBERAaphf+E+go47Psddr+zppgvtlbx05OmEBel5TJFRHZVKBOsj4BxzrkZwK3AU8H9XQ307raahHPuTudcrnMuNz09ve+jTMwA83XowRqZGIPfZ2xWqXYRERmMGmrg/b/D5GMhY3qHQ0s3lmMGh+3VD22qiMgwELIEyzlX4ZyrCt5/Hog0szS8Hqv2M22zgcIQhOjxR3hJVrs5WH6fMTIxWnOwRERkcProX1BTskPvFcCnBWVMTE8gIVq9VyIiuyNkCZaZjTYzC96fHYylBFgITDaz8WYWBZwLPBOqOIHgYsOdS7XHsElDBEVEZLBpaoB3boFxc2HsQTscXppfzvSsQAgCExEZGvrt5ykzexiYB6SZWT7wcyASwDl3B3AW8E0zawJqgXOdt7BUk5l9G3gR8AP3OOeW91ecvRLIgk2fdNiVkRzL8oLybp4gIiISppY+ApWFcOptOxzaXF7H1sp6pmcrwRIR2V39lmA5587byfHbgB3/utM2ZPD5/ohrtyRlwaoXwDnwOt3IDMTwyootOOcIdsSJiIiEt+YmePvPkDETJh61w+Gl+WUA7JedPKBhiYgMJaGuIjg4BLK9dUJqStp2ZQRiqW9qYVtNYwgDExER2QUrnoLStXD4tW0/GLa3NL8cv8/YN0NrX4mI7C4lWL3RRan2jNbFhstU6EJERAYB5+CtP0Ha3rD3SV2esrSgnL1GJRIb5R/g4EREhg4lWL0RCCZY7RcbTm5dbFiFLkREZBD4/H+wdTkc9j3w7dj8O+f4NL9MBS5ERPaQEqzeSMr2tu1KtWcGe7A2qVS7iIiEO+dgwR8geSxMO6vLU/K31bKtppHpY5RgiYjsCSVYvRGfDv6oDqXa0xKiifCZerBERCT85b0FBYvg0Ku99R27sDTfq4w7PSt54OISERmClGD1hs8HSZkderB8PmNUUgybNAdLRETC3YI/QMJomHlBt6csLSgjyu9j79GJAxiYiMjQowSrt5KyO8zBAshMjqFQPVgiIhLO8hfBujfhkG9DZEy3py3dWM6UjESiIvRPAxGRPaG/or0VyOpQRRC8Uu2agyUiImHtrT9BTDIceGm3p7S0OJYVlLOfFhgWEdljSrB6KykLKgqhpbltV0ZyDJvL62hpcSEMTEREpBtblsOq5+Cgb0J0Qren5ZVUU1nfpPlXIiJ9QAlWbwWywDVD1Za2XZmBWBqbHSXVDSEMTERE2jOzk81M7RvA23+GqASYfUWPp7UVuFAFQRGRPaYGqLe6KNU+WqXaRUTC0bnAF2Z2s5lNCXUwIVO6FpY9Drlfg7iUHk9dml9OTKSPSend93KJiEjvKMHqrbbFhrfPw8oMeIsNF5ap0IWISLhwzn0V2B9YA9xrZu+Z2RVmNrzK4y1/ElwLHPytnZ76aUEZUzMDRPj1zwIRkT2lv6S9lRRMsNr1YGUkqwdLRCQcOecqgMeBR4AM4HTgIzP7TkgDG0hlGyEuFRJH93haU3MLywoqmK4CFyIifUIJVm/FjoDI+A6l2lPjo4jy+7TYsIhIGDGzU8zsSeA1IBKY7Zw7AZgBXLuT5x5vZqvMbLWZXd/NOfPMbImZLTezN/v8A/SVioLtPw72YE1RNbWNzUqwRET6SNfLucuOzHYo1W5mjA7EKMESEQkvXwH+7Jxb0H6nc67GzL7W3ZPMzA/8DTgGyAcWmtkzzrkV7c5JBv4OHO+c22BmI/vjA/SJikIIjNnpaUvzywDYTxUERUT6hHqwdkVSV2thxbCpTEMERUTCyM+BD1sfmFmsmeUAOOde7eF5s4HVzrm1zrkGvOGFp3Y653zgCefchuDrbe3LwPtUef72+cM9WJpfTkJ0BBPS4gcgKBGRoU8J1q4IZHUYIgiQmRyrHiwRkfDyKNDS7nFzcN/OZAEb2z3OD+5rby9ghJm9YWaLzeyirl4oWFRjkZktKioq2oXQ+0hDNdSV9WqI4NKCcqZlJeHzWf/HJSIyDCjB2hVJ2VC1FZq2r3uVEYhhc0UdzVpsWEQkXEQEe6AACN6P6sXzusowOv9xjwAOBE4CjgN+ZmZ77fAk5+50zuU653LT09N7H3lfqSj0tjtJsBqaWli5qYLp2cn9H5OIyDChBGtXBLIAB5WFbbsykmNpbnEUV9WHLi4REWmvyMy+3PrAzE4FinvxvHyg/aSlbKCwi3P+55yrds4VAwvwimeEl9bh7DsZIvj5lkoamlpU4EJEpA8pwdoVXZVqT/JKtRdqHpaISLi4EvixmW0ws43AD4Fv9OJ5C4HJZjbezKLwFix+ptM5TwOHmVmEmcUBc4CVfRh732jrwcrs8bSl+eUATFeBCxGRPqMqgrsikO1tK7paC6uO/UMRk4iIdOCcWwMcZGYJgDnnKnv5vCYz+zbwIuAH7nHOLTezK4PH73DOrTSz/wFL8eZ5/dM5t6x/PskeaG2ndjJE8NOCMpLjIhmTEjsAQYmIDA+9SrDMLB6odc61BMea7wO84Jxr7Nfowk1bD9b2SoKZAa9RUg+WiEj4MLOTgKlAjJk3tco5d+POnuecex54vtO+Ozo9/j3w+z4Ltj+U50N8OkRE93jaJxvL2S8rQOs1EhGRPdfbIYIL8BqpLOBV4FJgfn8FFbaiEyAmuUMPVnJcJDGRWmxYRCRcmNkdwDnAd/AKV3wFGBfSoAZaReFOhwfWNTbz+ZZKzb8SEeljvU2wzDlXA5wB3OqcOx3Yt//CCmOB7A5zsMyMnNR4vthaFcKgRESknUOccxcB25xzvwQOpmPxiqGvosCrfNuDlZsqaGpxWmBYRKSP9TrBMrODgQuA54L7huf8rS4WGz5g3Ag+Xr9NpdpFRMJD65CCGjPLBBqB8SGMZ+BVFPS6wMWMMerBEhHpS71NsK4GfgQ8GZzwOwF4vd+iCmeBLKjomGDNyhlBZX0Tqzb3ah61iIj0r/+aWTLePKmPgDzg4VAGNKDqq6CufKcl2pfml5OWEM3oYDVcERHpG73qhXLOvQm8CWBmPqDYOXdVfwYWtpKyoHYbNNRAVBwAueNSAFi0vpR9M5NCGZ2IyLAWbKNedc6VAY+b2bNAjHOuPLSRDaC2CoI9DxH8tKCM6dkqcCEi0td61YNlZg+ZWVKwmuAKYJWZXde/oYWpLkq1Z4+IZXRSDAvztoUoKBERAXDOtQB/bPe4flglV9Auwep+iGB1fROrt1apwIWISD/o7RDBfZ1zFcBpeOVrxwIX9vQEM7vHzLaaWZfrg5jZBWa2NHh718xmtDuWZ2afmtkSM1vUyxgHRhel2s2M3JwRLFxXinOahyUiEmIvmdmZNly7ZloLMfUwRHB5YQUtDiVYIiL9oLcJVqSZReIlWE8H17/aWSYxHzi+h+PrgCOcc9OBXwF3djp+pHNupnMut5cxDozWBqtdDxbArJwUNlfUUaD1sEREQu17wKNAvZlVmFmlmVWEOqgBU1HobRMzuj1laX4ZgCoIioj0g94mWP/AmyQcDywws3FAj42Vc24BUNrD8Xedc61j6t4Heh4sHi7aerA6JlgHjhsBwOL1GiYoIhJKzrlE55zPORflnEsKPh4+E2Qr8iF+ZI+LDC/NLyczEEN6Ys8LEYuIyK7rbZGLW4Bb2u1ab2ZH9mEclwEvtH9LvCEeDviHc65z71YbM7sCuAJg7NixfRhSNyKivYarUyXBfUYnkhAdwcK8Uk6d2XPlJhER6T9mdnhX+4M//A195QU7rSD4aUE5+2l4oIhIv+hVgmVmAeDnQGuj9SZwI7DHE4eDidplwNx2uw91zhWa2UjgZTP7rLuGMZh83QmQm5s7MBOgAjuuhRXh97H/2GQWqdCFiEiotS/CFAPMBhYDR4UmnAFWUQipE7s9XF7byLrias46cHAMHBERGWx6O0TwHqASODt4qwDu3dM3N7PpwD+BU51zJa37nXOFwe1W4Em8xjF8JGXtMEQQvHlYq7ZUUl7TGIKgREQEwDl3SrvbMcA0YEuo4xowFQXbh7N3YVmB99uoClyIiPSP3iZYE51zP3fOrQ3efglM2JM3NrOxwBPAhc65z9vtjzezxNb7wLFAl5UIQyaQ7TVgnSoG5uaMwDn4aIN6sUREwkg+XpI19NVVQH1FjyXal+Z7CdZ+WUqwRET6Q6+GCAK1ZjbXOfc2gJkdCvRYLs/MHgbmAWlmlo83xDASwDl3B3ADkAr8PVhJtylYMXAU8GRwXwTwkHPuf7v4ufpXUhY0VEFdOcQmt+2eOSaZCJ+xMK+UI/cZGbr4RESGMTO7le2Vbn3ATOCTkAU0kForCAa6H/63NL+McalxJMdFDVBQIiLDS28TrCuB+4JzsQC2ARf39ATn3Hk7OX45cHkX+9cCM3Z8RhhpX6q9XYIVFxXB1KyA5mGJiIRW+/UTm4CHnXPvhCqYAdVagKmHIYJL88vZf2zywMQjIjIM9baK4CfADDNLCj6uMLOrgaX9GFv4Sgr+MlheAKOmdjg0a9wI7n9/PfVNzURH+EMQnIjIsPcYUOecawYwM7+ZxTnnakIcV/9r7cHqZohgSVU9BWW1XHzIuAEMSkRkeOntHCzAS6ycc63rX32vH+IZHFqHXnQq1Q7ePKz6phaWFQyfNS1FRMLMq0Bsu8exwCshimVglRcA1u0iw0vbClwkD1xMIiLDzC4lWJ1Yn0Ux2CSOBvN3WUnwwHEpACzK63aNZRER6V8xzrmq1gfB+3EhjGfgVBRAwkiI6Hp+1af55ZjB1Mzhs+6yiMhA25MEa2DWnApHPr/362DFjglWemI049PiWah5WCIioVJtZge0PjCzA9lJYaYhYycl2pfmlzMhLZ7EmMgBDEpEZHjpcQ6WmVXSdSJldBx+Mfx0sdhwq9xxI3hl5RZaWhw+3/Dt6BMRCZGrgUfNLDghiQzgnNCFM4DKCyB9r24PL80vY+6ktAEMSERk+OkxwXLOJQ5UIINOUhYUftzloVk5KTy6OJ+1xVVMGqlLKCIykJxzC81sH2BvvB8EP3PODY8V4CsKYeKRXR7aUlHH1sp69tMCwyIi/WpPhggOb4EsryFzO3bw5eaMAFC5dhGREDCzbwHxzrllzrlPgQQz+79Qx9Xv6sqhobLbIYKtCwxPV4IlItKvlGDtrqRsaK6H6uIdDo1Piyc1PkrzsEREQuPrzrmy1gfOuW3A10MXzgDZSYn2pfll+H3GvhlKsERE+pMSrN3VttjwjvOwzIzcnBEsWq9KgiIiIeAzs7YJsGbmB7ouqzeUtFa2bV1KpJOl+eVMHplAbJTWaBQR6U9KsHZXoN1iw13IHZfC+pIatlbUDWBQIiICvAj8x8yONrOjgIeBF0IcU/9r/cGviyGCzjk+LSjX8EARkQGgBGt3JbUuNtxNgtU6D2u9hgmKiAywH+ItNvxN4FvAUoZD5duKQrxFhkfvcCh/Wy2l1Q1aYFhEZAAowdpd8Wngj+62VPvUzAAxkT4WasFhEZEB5ZxrAd4H1gK5wNHAypAGNRDKC7zkyr/jGlefFqjAhYjIQOmxTLv0wMybSNxNghUV4WPmmGRVEhQRGSBmthdwLnAeUAL8G8A513Xd8qGmoqCHAhflRPqNvUdr6RARkf6mHqw9EcjudoggeOthrdhUQXV90wAGJSIybH2G11t1inNurnPuVqA5xDENnIqCHkq0lzElI4noCBW4EBHpb0qw9kRSVrdFLgByc1JobnEs2Vg2cDGJiAxfZwKbgdfN7C4zOxpvoeGhzzmvPeqigmBLi1fgYr8sDQ8UERkISrD2RCALKjdBS9c/kB4wNhmfoXlYIiIDwDn3pHPuHGAf4A3gGmCUmd1uZseGNLj+VlcOjdVdDhFcX1pDZV0TM1TgQkRkQCjB2hNJWeCaoXJzl4cTYyLZZ3SS5mGJiAwg51y1c+5B59zJQDawBLg+tFH1s9bh6l0MEVyaXwbAfipwISIyIJRg7YlAz6XawSvX/tGGbTQ1twxQUCIi0so5V+qc+4dz7qhQx9KvKgq9bZcJVjkxkT4mj0wY4KBERIYnJVh7om2x4a4rCYI3D6umoZmVmyoHKCgRERl2WtuhwI4J1qf55UzNDBDhV5MvIjIQ9Nd2T7T+UthjJcHWBYc1D0tERPpJRQGYDxI6LjLc3OJYVqgCFyIiA0kJ1p6ICUBUQo+VBDMCsWQlx2oeloiI9J+KQi+58ndc3nJNURU1Dc1aYFhEZAApwdoTZsFS7Rt7PG1WzggW5pXinBugwEREZHeY2fFmtsrMVpvZDoUxzGyemZWb2ZLg7YZQxLmD8vwuhwcuzS8HYLoqCIqIDBglWHsqkNXjEEHw5mFtraxnY2ntAAUlIiK7ysz8wN+AE4B9gfPMbN8uTn3LOTczeLtxQIPsTkVhlyXal+aXER/lZ0JafAiCEhEZnpRg7amdLDYMMCsnBdB6WCIiYW42sNo5t9Y51wA8Apwa4ph2zjnvh76kHRcZXppfzrSsAD7f8FhvWUQkHCjB2lOBbKjeCk313Z4yeWQCSTERKnQhIhLesoD2Y77zg/s6O9jMPjGzF8xsalcvZGZXmNkiM1tUVFTUH7FuV7sNGmt2GCLY2NzCik0VzBiT3L/vLyIiHSjB2lNtlQQLuz3F5zMOHDeChSp0ISISzrrq5uk8efYjYJxzbgZwK/BUVy/knLvTOZfrnMtNT0/v2yg7a1sDq+MQwVWbK2loalEFQRGRAaYEa0/1YrFh8OZhrd5axbbqhgEISkREdkM+MKbd42ygw69nzrkK51xV8P7zQKSZpQ1ciF1obX86DRH8tKC1wIUSLBGRgaQEa0+1LTbcu3lYi9erF0tEJEwtBCab2XgziwLOBZ5pf4KZjTYzC96fjdeOlgx4pO21JVgde7C+2FJFbKSfsSlxIQhKRGT46rcEy8zuMbOtZrasm+NmZrcES+EuNbMD2h3rsUxuWGkbIpjf42nTswNE+X0s1DwsEZGw5JxrAr4NvAisBP7jnFtuZlea2ZXB084ClpnZJ8AtwLku1GtwlBeA+SGx4yLDeSXV5KTFE8wHRURkgETs/JTdNh+4Dbivm+MnAJODtznA7cCcdmVyj8EbrrHQzJ5xzq3ox1h3X1QcxI6A4tU9nhYT6We/7IAWHBYRCWPBYX/Pd9p3R7v7t+G1beGjogASM8Dn77A7r7iafTISQxSUiMjw1W89WM65BUBP3TWnAvc5z/tAspllMBjL5O5zMnz6H9i6ssfTcnNGsDS/jLrG5gEKTEREhryKgh2GBzY1t7ChtIacVK1/JSIy0EI5B6u7cri9LZMLDHAp3O586ZcQnQjPfd9bj6Qbs8al0NjsWJpfPoDBiYjIkFZesEOJ9vxttTS1OMZrgWERkQEXygSru3K4vSmTu/3AQJbC7U58qpdkrX8HPnm429MOHDcCQOthiYhI33DOK9Oe1DHBWldSDaAES0QkBEKZYHVXDnenZXLD0v4XQvZseOmnUNN1AjUiPopJIxM0D0tERPpG7TZoqt0xwSryEqwcJVgiIgMulAnWM8BFwWqCBwHlzrlN9KJMbljy+eDkP0NtGbz6y25Pm5UzgkV5pbS0hLbolIiIDAHlwQq2nYYI5pVUkxgTQWp8VAiCEhEZ3vqzTPvDwHvA3maWb2aXdSp1+zywFlgN3AX8H3RfJre/4uxTo6fBQd+ExfNh48IuT8kdl0JFXRNfbK0a2NhERGToqQgO8Ojcg1VczXiVaBcRCYl+K9PunDtvJ8cd8K1uju1QJnfQmHc9LHsCnr0GrngD/B0vceuCwwvzStl7tMrniojIHmhdg7GLBOuAsSNCEJCIiIRyiODQFJ0IJ/wOtnwKH965w+ExKbGMTIxmUZ4KXYiIyB4qLwBfBCSMbNtV39RMYVmt5l+JiISIEqz+MOUUmHwsvP6b7cM3gsyMWTkpLFShCxER2VMVhTssMryxtIYWBxOUYImIhIQSrP5gBifcDC1N8L8f7XA4N2cEBWW1bCqvDUFwIiIyZFQUdDE8sAZQBUERkVBRgtVfUsbD4dfCiqdg9SsdDuWO8+ZhqVy7iIjskYoCSMrssGtdsVdEaXyqEiwRkVBQgtWfDrkKUifDc9dC4/beqikZicRF+TUPS0REdl/rIsOBHXuwUuKjCMRFhigwEZHhTQlWf4qIhpP+CNvWwdt/3r7b7+OAsSM0D0tERHZfTQk01e0wRDCvuJqc1LgQBSUiIkqw+tuEI2C/s70Eq3h12+7cnBF8trmCirrGEAYnIiKDVkWBt+2iRLvmX4mIhI4SrIFw3G8gIhae+543pANvPawWBx9vKAttbCIiMjiVBxOsdkMEaxua2VxRpwqCIiIhpARrICSMhKN/BuvehGWPAzBzTDJ+n7FY87BERGR3dNGDlVdSDaiCoIhIKCnBGii5X4PM/eHFH0NdOfHREUzNTGLBF8W0tLhQRyciIoNNRXCR4fjtiwyvKw4mWKogKCISMkqwBorPDyf/GaqL4LVfA3DmAdks2VjGDc8swzklWSIisgvKCyAxE3zbm/K2BEs9WCIiIRMR6gCGlcz9YdblsPCfMPN8Ljp4JpvK67jjzTVER/j56UlTMLNQRykiIoNBFyXa84qrGZkYTUK0mncRkVBRD9ZAO+qnEJ8Oz16DuRZ+ePzeXHpoDne/vY4/vLQq1NGJiMhgUZG/Y4n2ElUQFBEJNSVYAy0mAMf9Fgo/hkX3YGbccPK+nD9nLH97fQ23vvpFqCMUEZFw17rIcFJmh93riqsZr/lXIiIhpQQrFKadCeOPgFd/BZVbMDN+feo0zjowmz++/Dl3LlgT6ghFRCScVRdDcwMEstt2VdY1UlzVwPh0JVgiIqGkBCsUzOCkP0FTLTx7DTTV4/MZvztzOqfMyOS3z3/Gv97NC3WUIiISriryvW27Hqy84hpAFQRFREJNCVaopE2CL/0CVj0Hdx8Lpevw+4w/nT2D46aO4ufPLOfhDzeEOkoREQlHFYXett0crLXFVQCM1xwsEZGQUoIVSgd/C859GLatg38cAZ89R6Tfxy3n7c+Re6fz4yc/5YmP8kMdpYiIhJvy4CLD7YYI5hXXYAbjUuNCFJSIiIASrNDb50T4xgJIGQ+PnA8v/oRoa+H2rx7IIRNTufbRT3h2aWGooxQRkXBSUQC+SIhLa9uVV1JNZiCWmEh/CAMTERElWOFgRA5c9pK3RtZ7t8H8k4mp2cxdF+WSOy6Fqx9ZwkvLN4c6ShERCRcVBd78q3aLDK8triYnTb1XIiKhpgQrXEREw0l/hDPvhs2fwj8OI27jAu65dBbTsgJ8+6GPeWPV1lBHKSIi4aC8YMc1sIqrVeBCRCQMKMEKN/udBVe8AQmj4P4zSHj39/zrkgPZa3QC37h/Me+uLg51hCIiEmoVBRDYnmBtq26gvLZRBS5ERMKAEqxwlL4XXP4qzDgP3ryJwOPn8MA5E8hJjeeyfy1iYV5pqCMUEZFQaWkJLjLcvoJgNaAKgiIi4UAJVriKioPT/g5fvg02vE/yfUfx7+MdGckxXHrvQpZsLAt1hCIiEgo1xdDS2CHBygsmWDlKsEREQk4JVjgzgwMuhMtfgah4kv9zOk/PWERqXAQX/vMDnl5SgHMu1FGKiMhAKg8u39FuiGBeSTV+nzFmhIpciIiEmhKswWD0ft68rCknk/j2r/hfxh3MTHd895ElfOuhjyitbgh1hCIiMlAqgmtgJWW27VpXXE32iFiiItSsi4iEmv4SDxYxSfCVf8EJNxOb9zr3NV7LrbNLeXnFFo798wJeXbkl1BGKiMhAqAiujZi0fZHhdaogKCISNpRgDSZmMOcb8LUXsYhoTln6bRbu+xgT4+u47F+L+MFjn1BZ1xjqKEVEpD+V54M/CuK9RYadc+QVV6vAhYhImOjXBMvMjjezVWa22syu7+L4dWa2JHhbZmbNZpYSPJZnZp8Gjy3qzzgHnewD4cp34PDrSF7zNI80XsVt077gscUbOf4vb/HempJQRygiIv2lotAbHmgGQFFVPdUNzUqwRETCRL8lWGbmB/4GnADsC5xnZvu2P8c593vn3Ezn3EzgR8Cbzrn2NciPDB7P7a84B63IGDjqp/CNBdiI8Zy8+ud8POEfjPEVcd5d73Pjf1dQ19gc6ihFRAaVnf0w2O68WcEfBc8ayPgAbw5W++GBRaogKCISTvqzB2s2sNo5t9Y51wA8Apzaw/nnAQ/3YzxD06ipcNlLcMLNBIo+4uHGq7lj4vvMf2cNJ93yFp+onLuISK/05ofBduf9DnhxYCMMKi/oUOAiryS4BpbmYImIhIX+TLCygI3tHucH9+3AzOKA44HH2+12wEtmttjMrujuTczsCjNbZGaLioqK+iDsQcjn9+ZmfesDbPzhHF9wC59k/o6sutWccfu7/OmlVTQ0tYQ6ShGRcNfbHwa/g9debR3I4ABvkeHKwg4l2tcV1xDpN7JGxA54OCIisqP+TLCsi33dLdp0CvBOp+GBhzrnDsD7JfFbZnZ4V090zt3pnMt1zuWmp6fvWcSDXSAbznsEzrqHxPrN/KvpB9yZ8Sz/eG0Fp//9HVZtrgx1hCIi4WynPwyaWRZwOnBHTy/Ubz/+VW+FlqYOiwyvK65ibEocfl9Xza6IiAy0/kyw8oEx7R5nA4XdnHsunYYHOucKg9utwJN4vyzKzpjBtDPhWx9iM8/j6JIH+Sjt54wpW8gpt77NP95cQ3OLFicWEelCb34Y/AvwQ+dcj5Nc++3Hv7Y1sNotMlxcowIXIiJhpD8TrIXAZDMbb2ZReEnUM51PMrMAcATwdLt98WaW2HofOBZY1o+xDj1xKXDq3+CiZ4iP9HFHyy+5N2U+f39hEWfe/i6fb1FvlohIJ735YTAXeMTM8oCzgL+b2WkDEh1486+gbYhgS4sjr0Ql2kVEwkm/JVjOuSbg23iTgFcC/3HOLTezK83synanng685JyrbrdvFPC2mX0CfAg855z7X3/FOqRNOAL+7z2Yew2HVL3MB4EfcUzxfVx6y9P8+eXPqW9SpUERkaCd/jDonBvvnMtxzuUAjwH/55x7asAi7NSDtbmijvqmFlUQFBEJIxH9+eLOueeB5zvtu6PT4/nA/E771gIz+jO2YSUyFr70C2zamcS89FO+tfYRroz8D68sOIBffnQSZ51zEQfkpIU6ShGRkHLONZlZ6w+DfuCe1h8Gg8d7nHc1ICoKICIG4lIBWFesCoIiIuGmXxMsCTOj94OLnoaSNfg/uo8jF93PcbW/pODev/F61unMOeMq4tLGhjpKEZGQ6c0Pg+32XzIQMXXQWqI9uMhwa4KlHiwRkfDRn3OwJFylToRjfknUdZ9Re/q91CXlcGThXUTdNoPif54Jn78ELRo6KCISdioKOxW4qCYm0sfopJgQBiUiIu0pwRrOIqKInXEGE7//Kp+e8Qb/iTodt/FDeOgrtPx5P3jjd9snVIuISOhVFHQq0V5NTmo8PpVoFxEJG0qwBID9pu/PGdfdxf0HvcC3Gq/mg6o0eOO3uL9Mg4fOhc9fVK+WiEgotTQHe7Ay23atK/ESLBERCR+agyVtYiL9fO+EaayYMZYfPn4MZYWfc/3IDzk+/xX8n78Ao/aD4/8fjD8s1KGKiAw/VVvBNbeVaG9qbmFjaQ3HTR0d4sBERKQ99WDJDvbNTOLJ/zuEC084gu+VnEpu9V95Z8bvcPXl8K+T4d9fhdJ1oQ5TRGR4aSvRng1AQVktjc1OFQRFRMKMEizpUoTfxxWHT+TFqw9nn6xULvhgDGf4/sIX067BrX4N/jYbXvkF1GvBYhGRAdGWYHlDBFVBUEQkPCnBkh7lpMXz0Nfn8KezZ1Ba7+OYRbP4atzfKcg+Ad7+M9x6IHz8ILS0hDpUEZGhrbXoUMDrwcprXQNLCZaISFhRgiU7ZWaccUA2r37vCP74lRkUNidz6KpzuCrhj2yLGg1P/x/88yjY8H6oQxURGbpaFxmOHQFAXkkNCdERpCVEhTgwERFpTwmW9FqE38eZB2bz8jWH85dzZrLMJrF/4Q+4Kfb71JYWwj3HwWNfg7KNoQ5VRGToaS3RHlxkeG1xNTlpcZipRLuISDhRgiW7LMLv47T9s3j5miP467n780rkERxQ9v+4P+pcmlc8i7ttFrz+W2ioDnWoIiJDR3lBWwVB8IYIqkS7iEj4UYIlu83vM06dmcWLVx/OzecdzP2x53N4zc28wYHw5u+8RGvpo+BcqEMVERn8KgrbFhluaGohf1sNEzT/SkQk7GgdLNljfp9xyoxMTtovgxeW7cVNr07gb1uP4rdVD7LXE5fj3rgJm3wMTDwKcg6FKP2DQERkl7Q0Q+WmtgRrQ2kNLU4VBEVEwpESLOkzPp9x0vQMTpg2mpdWTObqV3KZsvV5ztr2Prkf3kPkB7fjfJHY2IO8ZGviUTB6OvjUkSoi0qPKzd4iw8ES7Xkq0S4iEraUYEmf8/mM46dlcOy+o3ll5d7cvWgjl60qYH8+48vxn3FU0XLS834Jr/4S4lJhwjwv2ZpwZIf5BSIiElRR6G1bS7SXeAmWhgiKiIQfJVjSb3w+49ipozl26mi2Vc/ghWUH8MSSAq7PKyXNlXFe2hpOSVjFhLUL8C973HtS+j7be7fGHaLhhCIiABX53jY4RHBtcTXJcZEkx6lEu4hIuFGCJQNiRHwU588Zy/lzxrKpvJZnP9nE05+M5Za8AzE7j3PGVHBuymqm1i0mcuHd8P7fwXyQtheM3s8bSti6jU8N9ccRERlYrT1Y7YYIqoKgiEh4UoIlAy4jEMvXD5/A1w+fwOqtVTzzSSFPLyngkQ0BovyzOGbyD7goaxMHsoKIohWw/l349NHtL5CYCRmtCVfwlpyjuVwiMnSVF0Bk3PZFhourOWiCfmwSEQlHSrAkpCaNTOB7x+zFNV+azNL8cp5eUsh/lxby3GexxETOYvb44zksN43Ds31MbsnDt/VT2LQUNn8KX7zsTfoGiE6CUdO8ZCtjOoyZA6mT2hbkHFKcg8YaDZ8UGU4q8r3eKzNqG5opLK9TgQsRkTClBEvCgpkxY0wyM8Yk85OTpvD+2hJeWr6Zt1cX85vnV/IbIC0hikMmzmXu5NOYe3QamfHA1pWwOZhwbf4UPn4APgwucByXCmMPhrEHedvR0yFikM9X2JYHz/8A1rwKx/4a5lw5NJNIEemo3RpY60tVQVBEJJwpwZKw4/cZh05K49BJaQAUltXyzupi3l5dzDuri3nmE28uwoT0eOZOSmPupOM56OgLSIqJhJYWKPkCNrwfvL0Hnz3rvXBELGTnBhOugyB7NsQkhepj7pqmBnj3FljwB/D5IXsW/O96KPgITvkrRMWFOkIR6U/lBTDxSGB7ifbxmoMlIhKWlGBJ2MtMjuUruWP4Su4YnHN8trmSd1YX89YXxTy6KJ/73luP32fMyA4wN5iYzZzxVaIPvNh7gcrNHROut/4IrsUrojFqasdersSM8OsRWvcWPPd9KF4FU74Mx9/kxfn2H+G133i9eOfcDynjQx2piPSH5iao2txW4GJdcQ0AOWn6YUVEJBwpwZJBxcyYkpHElIwkLj9sAvVNzXy0vsxLuFYXc9vrq7nltdVERfiYOSaZ2TkpzBqfwoGTTiZh6mnei9RXQv6i7QnXxw/Ah3d6x6IDkJIDI3JgxHgvaWm9n5QF/gH8X6aqCF76KSx9BJLHwfmPwl7Hbj9++HWQMRMevwzunAdn3Q2TvjRw8YnIwKja7P0oFBwiuK64irSEaBJjIkMcmIiIdEUJlgxq0RF+Dp6YysETU7n2uL0pr2nk/XUlLFxXyod5pdz+5hpue301PoOpmQFmj09hVk4Ks3IOITU43IbmRm/+1sYPoWQ1bFsHW5bDZ89DS+P2N/NFQPJYL9kakdMx+UqbDBHRffOhWlrgo/nwyi+goQYOuxYO+37XwwAnHwNXvAH/vhAeOAuO/hnM/V749cKJyO4rL/C2wQQrr7iG8eq9EhEJW0qwZEgJxEVy3NTRHDd1NABV9U18vGEbH64r5cN1pTzw/nrufnsd4FUwnJWTwpzxKcwaP4Wsgw7o+GItzVBR4BWWKF3nbbet8+4XLIa6su3nRsR6wwwnzIMJR3gFNXz+Xf8Am5bCs9dAwSLIOQxO+iOk793zc1ImwGUvwTNXwas3evOyTrt98MwvE5GeVQQTrECwB6ukmiP3Tg9hQCIi0hMlWDKkJURHcNjkdA6b7P1jpL6pmU/zy/kwr5SF60p5dmkhD3+4AYCs5FimZwfYNyOJfTO92+jAGCx5LIw/fMcXr90WTL7WwsaFsPYNeOXn3rGYZO85E46A8fMgdWLPvUr1lfD6b+GDO7zqh6ffCdPP7n1PVFQ8nPlPyDrQG1b4z6PhnAchfa9eXikRCVsV23uwKusaKaqsVwVBEZEwpgRLhpXoCD+5OSnk5qTAPGhucXy2uYKF60pZuH4bywvKeWHZ5rbzU+Kjtidcwe2EtHgi/D5vwc/YEZC5P0w703tC5RZYt8BLtta9CSuf8fYnZQeTrSO8baLXw4ZzsOJpryJg5WbIvRSOvqFtMdFdYgYH/5+3Ftijl8BdR8Hpd8CUk/fkkolIqFUUQmQ8xARYX1gBqIKgiEg4U4Ilw5rfZ0zNDDA1M8Alh3pV+Krqm/hsUwUrNlWwotDbzn83j4amFgCiInzsMzqxQ+K19+hEb8J54iiY/hXv5pzXu9WabK16HpY86L1x+j5eslW6Bla/4g0pPOcBr4z8nhp/GHzjTW9e1r8v8OZwHfnj3RuyKCKhV57vDQ80Y11rifZ0JVgiIuFKCZZIJwnREdt7uYKamltYW1zdlnAtLyznxeWbeWThxrZzxqTEMmV0UluVw30zksgeMQHfrIkw6zKveMXmpV6ytfYN+Og+r3DG8TfBrK/3bYXCQDZc+gK8cB289QfYtMQbQrg7PWMiElpTT4e6coC2BGtcihIsEZFw1a8JlpkdD/wV8AP/dM7d1On4POBpYF1w1xPOuRt781yRgRTh97HXqET2GpXIaft7E82dc2yuqGNFYQUrN1WwclMlKzdV8PLKLTjnPS8hOoK9RycyJSMxmHiNY+/cbxF/6Hehqd4rvRwZ2z9BR8bAl2+FzAPg+eu8Uu7nPAijp/XP+4lI/5h2RtvdvOJqMgIxxEapR1pEJFz1W4JlZn7gb8AxQD6w0Myecc6t6HTqW865k3fzuSIhY2ZkBGLJCMRy9JRRbftrG5pZtcVLtj4LJl5PLynkgfc3BJ8H41LimBIcWjgxPYEJ6fFMSEvon3805V4Ko6bBfy70il+MyAFfpNdj5osEf6TXk+aP7GJ/8LE/ClInQc5cSJ8CPl/fxykiO7WupJoczb8SEQlr/dmDNRtY7ZxbC2BmjwCnAr1JkvbkuSIhFRvlZ+aYZGaOSW7b55yjoKy2rZdr5aYKPttcyf+Wb27r7QKvkuGE9HgmpicwsXU7MoGRidHYnqxtNWYWXPEmvHkTVBdBc5O3xldzI7Q0QWMt1Fd02t+4/XFTA9SXBz9gCow7xCsjnzMXRu6rhEtkgOQVV3PCfhmhDkNERHrQnwlWFrCx3eN8YE4X5x1sZp8AhcC1zrnlu/BczOwK4AqAsWPH9kHYIn3PzMgeEUf2iDiO2Xd7b1ddYzN5JdWs2VrNmqIq1hZVsaaomv8s2khNQ3PbeQnREcFervhgj1cCOWlx5KTGEx/dy/+NE0fByX/e/Q+xbT2sfwfy3vZunz3r7Y8dAeMO9ZKtnLkwcqoSLpF+UFbTwLaaRlUQFBEJc/2ZYHX1c7vr9PgjYJxzrsrMTgSeAib38rneTufuBO4EyM3N7fIckXAVE+lnn9FJ7DO646LAzjm2VNSzpqgqmHh5CdiH60p5aklhh3PTE6MZnxrPuNQ4ctLiyUmNJyctjnGp8ST0NvnqjRHjvNvM873HZRsg7x1Y3ynhiklul3Ad6g1PVAVDkT3WWuBCa2CJiIS3/kyw8oEx7R5n4/VStXHOVbS7/7yZ/d3M0nrzXJGhzMwYHYhhdCCGQyeldThW09DE2qJq1pfUkFdSTV6xd/+Nz4soWpzf4dy0hGjGB5Ot8WnBJCw1nrGpcSTFRO5ZkMljYeZYmHme97g830u48t7yEq5Vz3n7o5O8BZDHzIbs2V4p+tjkPXtvkWEoryRYol0JlohIWOvPBGshMNnMxgMFwLnA+e1PMLPRwBbnnDOz2YAPKAHKdvZckeEqLiqCaVkBpmUFdjhWXd+0PfEKJl95JTUs+LyIxzolXynxUYxNiSMnNY6xqfHkpMYxLtVLxlLjo3Z9zlcgG2ac490Aygu8IYUb3oONC2HB772qieCtA5Y9a3vSlbbXng0rbKqHyk1QsQnqyrx1xQJZu/96ImFoXVE1PoOxKXGhDkVERHrQbwmWc67JzL4NvIhXav0e59xyM7syePwO4Czgm2bWBNQC5zrnHNDlc/srVpGhIj46wlv8ODNph2M1DU3kFdewobS198u7vzBvG898UkhLuwG28VH+tqRrbGuvV0ocY0bEkZEcQ6S/F8lQIAumn+3dAOoroeAjyP/QS7g+exY+vt87FhOArNxgwjXL6+WKCXhrh9UUb0+eKguhcjNUBLeVm7z7taU7vn/qJBh/uHfLOQzi03Y8J5RaWrwCIxFRoY5E2unF8iKnAr8CWoAm4Grn3NsDEdu6khqyRsQSFaE5jiIi4cycGzrTlnJzc92iRYtCHYbIoFPf1Ez+tlo2lNSwvqQ6mHx5PWH5pbU0NLe0neszyAjEMiYlljEj4hiTEtfhfnpCND5fL3q/nIOS1bDxw+1J19YVeNMtDRJGQU2JV8WwA4OEkZCYAUmZkDgaEjMhKcO7H5UIBYtg3QJvyGJDpfe0UdO2J1zjDvESuIFSW+Z9ti3Lt9+2roCmOq+3bczs4G2O1xMoHZjZYudc7gC8jx/4nHZLhADntV8ixMwSgOrgyIvpwH+cc/v09Lp91TadcuvbJMdFcv9lXdZ8EhGRAdZd+9SvCw2LyOAQHeEPloZP2OFYc4tjU3ktG0tr2bithvzSGjZuq2VDaQ1vfl7E1sr6Tq/lI2tEa8LlbTOTY8kaEUtWcuz2BMwM0iZ7t/0v8J5cVwEFiyF/IWzLCyZSrclT8JYwylubqydj58DB3/LKzG9aAuve9BKuRffA+38H80Hm/tsTrjEHQVQfDLtqbvKSxi3LOiZU5e2KosYEvGRv5vkQGed93o/ugw/u8I4nZnZMuEZPVy/XwNnpEiHOuap258fTTQGmvuacY11xNWceoKGvIiLhTgmWiPTI79teYv5gUnc4Xtfo9X61T742ltawcVsNSzaWUV7bsQcq0u8t0JyZHENmcizZybFtCVhmciyZYw4nduKRfRR8hDfcMDsXDvs+NNZ5ydu6Bd7t3Vvh7T97Cypnz/JK2ZvfS8B8wW2H+/6O+1uPVW7xkqqiVdAcTDh9Ed7csrEHwciveUnVqKler1vn+W3Njd7zN364/bbiqeBniPaSwdaEa8xsL/GU/tCrJULM7HTg/wEjgZMGIrDiqgaq6ptUQVBEZBBQgiUieyQm0s+kkQlMGrlj7xdAZV0jhWV1FJbVkl9WS2HwVrCtlvfXlLC5oq7D/C+A1PgoMpJjGJ0Uw8ikGEYlxjAqKZpRge33R8RF9W4oYnuRMTD+MO/GT6C+Cja87/VwrX8XNi/zCnG4Zm+OVOt91wItze3utzvW0gxxqV7yNGFeMJHa10uuIqJ7F5c/0kuiMveHOd/w9lVsCg6dDN4+uAPevcU7ljwWRu3nvc/IKd5iz6mTvNeRPdGrJUKcc08CT5rZ4Xjzsb60wwv18RqNrRUElWCJiIQ/JVgi0q8SYyLZe3Qke49O7PJ4U3MLmyvq2pKwguDNS8Tq+HhDGSXVDTs8L9JvjEyMYWRSNKOTYhiV5N33ErDt95NiI7qviBidAJO/5N3CTVIG7HuqdwOv923TJ17Slb8Itq6Ez//nJXng9cKl7RVMuKZ4Cd/IKRAY2/sKjQ013ry3mmKoDm5rSrxbZCwkj/OSu+SxkDB6KC4ovUtLhDjnFpjZRDNLc84VdzrWp2s0tq6BNUEJlohI2FOCJSIhFeH3tQ1B7E5DUwtFVfVsqahja0Udm8vr2FLZ+rie1VureHt1MZV1TTs8NyrCx6ikaEYGe75ak7JRrdtgD1mPiVg4iIzx5paNbTdirbEOSr7wkq2tK2DLCq+3a9lj7Z4XDyP38Xq50veG5gaoKYXq4mAiVew9rimGxpqu39v82xO5Vv4oryBHa8KVPNZL5lrvJ44ejAtM92Z5kUnAmmCRiwOAKLzlRfrVuuJqInxGVnJsf7+ViIjsISVYIhL2oiJ8ZCXH7vQflzUNTWypqGdrRR1bgwlYUXC7paKeVZsreevzYirrd0zEoiN8pCdGMzJxexI2MjE6uC/G2yZFkxofjX9Xhyb2l8gYGL2fd2uvrsKbD7Z1xfbb5//bXhY/Mg7i0iA+1Stfn76Pt41L3b6NS9t+PyYAjbXeYtJlG6Bsvbct3+htP38RqrZ0jMEX6SVgo6fBOQ8MzPXYQ71cXuRM4CIza8RbXuQcNwDlePOKqxmbEkdEb5ZIEBGRkFKCJSJDRlxUBOPTIhi/k2FUNQ1NbK0I9oBVbt8WVdaztbKONUVVvLe2ZIcCHeAV/UiNjwomYDFtSVhqfBSpCdGkJkSRluA9To6LCk0yFpMEY2Z5t/ZqSiEiZvcqJkbFQfpe3q0rbQlYMPlqvfkHVwVE59zzwPOd9t3R7v7vgN8NdFzriqt3+r0WEZHwoARLRIaduKgIctIidlowoK6xOZh01VNUWdd2f2uFl4htqajj04JySqrqdyjUAd6aYSnxUaTGe4lXajDxSgve945FMSK4TYqJ3PXCHbsiLqX/XjsydnvZfelTLS2OvJJqDp0UZotli4hIl5RgiYh0IybSH1xIuecen5YWR1ltIyVV9RRXNVBSXU9JVYP3uNrbllQ1sKygnOKq+i7nioHXOzYiLpKU+ChGxEWRmhDcxkd5+4LJ2oj4SJLjogjERhIf5Q/vuWOyx7ZU1lHX2KIKgiIig4QSLBGRPeTzGSnBJGjyqJ2fX9/UTGl1A8WVDZTWNLCtuoGS6gZKq+sprW6ktLqebdWNrNpcybaaRrbVNNDdLJ8InxGIjSQQF0kgNpLk2OA2LoqkDo9bb1GkBJOzfu0tkz7TWkFwfKoSLBGRwUAJlojIAIuO8JMRiCUj0LuKcM0tjrKaBrbVNFBS5W3Laxspq2n0trXetrymkeKqBlYXVVFe00hFNz1l4A1fTI6L6tBjltJuuGL7x60JWWJMhJKyEGhLsNKVYImIDAZKsEREwpzfZ8ECGtFMGtn75zW3OCrrtidirYlZaXVD221bjbddX1LDxxvL2FbdQFNXE8oAM0iIjiApxusVS4ptfz+SpBhvX6DtfiQp8ZFMGtn1GmjSO3nF1URH+MhIigl1KCIi0gtKsEREhii/z0iO86oZ9pZzjsr6Jra1S8BKqrzErKKuiYraRirqGr1tbRMbSmu8Y7WNVDc07/B6Y1JieesHR/Xlxxp21hXXMC41Tr2HIiKDhBIsERFpY2Ze71NMJON2cc5PU3MLlXVNwWTMS8Ac/b5E1JB39ZcmU9HFkgEiIhKelGCJiEifiPD7GBGctyV9Z1pWINQhiIjILtCS8CIiIiIiIn1ECZaIiIiIiEgfUYIlIiIiIiLSR5RgiYiIiIiI9BElWCIiIiIiIn1ECZaIiIiIiEgfUYIlIiIiIiLSR5RgiYiIiIiI9BElWCIiIiIiIn1ECZaIiIiIiEgfUYIlIiIiIiLSR8w5F+oY+oyZFQHrgTSgOMThhDNdn+7p2nRP16Z7ujbd64trM845l94XwYRCu7YJ9F3pia5N93Rtuqdr0z1dm571W/s0pBKsVma2yDmXG+o4wpWuT/d0bbqna9M9XZvu6dp0pOvRPV2b7unadE/Xpnu6Nj3rz+ujIYIiIiIiIiJ9RAmWiIiIiIhIHxmqCdadoQ4gzOn6dE/Xpnu6Nt3Ttemerk1Huh7d07Xpnq5N93Rtuqdr07N+uz5Dcg6WiIiIiIhIKAzVHiwREREREZEBpwRLRERERESkjwy5BMvMjjezVWa22syuD3U84cTM8szsUzNbYmaLQh1PKJnZPWa21cyWtduXYmYvm9kXwe2IUMYYSt1cn1+YWUHw+7PEzE4MZYyhYGZjzOx1M1tpZsvN7LvB/cP+u9PDtRn23xtQ27Qzap+2U/vUPbVN3VP71L1QtE9Dag6WmfmBz4FjgHxgIXCec25FSAMLE2aWB+Q654b9onNmdjhQBdznnJsW3HczUOqcuyn4D6ARzrkfhjLOUOnm+vwCqHLO/SGUsYWSmWUAGc65j8wsEVgMnAZcwjD/7vRwbc5G3xu1TTuh9mk7tU/dU9vUPbVP3QtF+zTUerBmA6udc2udcw3AI8CpIY5JwpBzbgFQ2mn3qcC/gvf/hfc/37DUzfUZ9pxzm5xzHwXvVwIrgSz03enp2ojaJtkFap+6p7ape2qfuheK9mmoJVhZwMZ2j/NRA9+eA14ys8VmdkWogwlDo5xzm8D7nxEYGeJ4wtG3zWxpcJjGsBtm0J6Z5QD7Ax+g704Hna4N6Hujtmnn1D71TH9jejbc/8Z0oPapewPVPg21BMu62Dd0xkDuuUOdcwcAJwDfCna1i/TW7cBEYCawCfhjSKMJITNLAB4HrnbOVYQ6nnDSxbXR90ZtU2+ofZLdpb8x7ah96t5Atk9DLcHKB8a0e5wNFIYolrDjnCsMbrcCT+INW5HttgTH6baO190a4njCinNui3Ou2TnXAtzFMP3+mFkk3h/oB51zTwR367tD19dG3xtAbdNOqX3aKf2N6Yb+xmyn9ql7A90+DbUEayEw2czGm1kUcC7wTIhjCgtmFh+c2IeZxQPHAst6ftaw8wxwcfD+xcDTIYwl7LT+gQ46nWH4/TEzA+4GVjrn/tTu0LD/7nR3bfS9AdQ29UjtU68M+78x3dHfGI/ap+6Fon0aUlUEAYIlFv8C+IF7nHO/CW1E4cHMJuD9KggQATw0nK+NmT0MzAPSgC3Az4GngP8AY4ENwFecc8NyMm0312ceXje6A/KAb7SO6x4uzGwu8BbwKdAS3P1jvLHcw/q708O1OY9h/r0BtU09UfvUkdqn7qlt6p7ap+6Fon0acgmWiIiIiIhIqAy1IYIiIiIiIiIhowRLRERERESkjyjBEhERERER6SNKsERERERERPqIEiwREREREZE+ogRLJITMrNnMlrS7Xd+Hr51jZsNyPRAREdkzap9Edl9EqAMQGeZqnXMzQx2EiIhIJ2qfRHaTerBEwpCZ5ZnZ78zsw+BtUnD/ODN71cyWBrdjg/tHmdmTZvZJ8HZI8KX8ZnaXmS03s5fMLDZkH0pERAY9tU8iO6cESyS0YjsNwTin3bEK59xs4DbgL8F9twH3OeemAw8CtwT33wK86ZybARwALA/unwz8zTk3FSgDzuzXTyMiIkOF2ieR3WTOuVDHIDJsmVmVcy6hi/15wFHOubVmFglsds6lmlkxkOGcawzu3+ScSzOzIiDbOVff7jVygJedc5ODj38IRDrnfj0AH01ERAYxtU8iu089WCLhy3Vzv7tzulLf7n4zmncpIiJ7Tu2TSA+UYImEr3Pabd8L3n8XODd4/wLg7eD9V4FvApiZ38ySBipIEREZdtQ+ifRAvxaIhFasmS1p9/h/zrnWUrjRZvYB3g8h5wX3XQXcY2bXAUXApcH93wXuNLPL8H4J/Cawqb+DFxGRIUvtk8hu0hwskTAUHOOe65wrDnUsIiIirdQ+ieychgiKiIiIiIj0EfVgiYiIiIiI9BH1YImIiIiIiPQRJVgiIiIiIiJ9RAmWiIiIiIhIH1GCJSIiIiIi0keUYImIiIiIiPSR/w99OsdVJTWaSwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_model([train_loader, test_loader], num_epochs=25, learning_rate=0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb5783f",
      "metadata": {
        "id": "ceb5783f",
        "outputId": "61fbfce7-e998-484f-dcf5-1cf9a3299144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final test accuracy: 0.8518\n"
          ]
        }
      ],
      "source": [
        "print(f'Final test accuracy: {test_accuracies[-1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e128ed",
      "metadata": {
        "id": "a5e128ed"
      },
      "source": [
        "## Visualization of the labels and predictions\n",
        "\n",
        "In this section, you should visual one image from each class and show both the actual label and the predicted label for that image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0b79fd",
      "metadata": {
        "id": "6c0b79fd",
        "outputId": "d3fdadd4-6647-45e6-de3c-af6e7ca78361"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x720 with 10 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_predictions(model, dataloader, parameters):\n",
        "    \"\"\"\n",
        "    Visualizes one image from each class along with the actual and predicted labels.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store one image per class\n",
        "    class_images = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(x, parameters)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Store one image per class\n",
        "            for img, label, pred in zip(x, y, preds):\n",
        "                if label.item() not in class_images:\n",
        "                    class_images[label.item()] = (img, pred.item())\n",
        "                # Break the loop if we have one image for each class\n",
        "                if len(class_images) == 10:\n",
        "                    break\n",
        "\n",
        "    # Plotting the images\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(10):\n",
        "        img, pred = class_images[i]\n",
        "        img = img.cpu().numpy().squeeze()  # Move to CPU and remove the channel dimension\n",
        "        actual_label = i\n",
        "        predicted_label = pred\n",
        "\n",
        "        plt.subplot(2, 5, i + 1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f\"Actual: {actual_label}\\nPredicted: {predicted_label}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# Assuming test_loader is defined and parameters are initialized\n",
        "visualize_predictions(model, test_loader, parameters)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}